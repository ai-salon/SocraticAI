import logging
import os
from abc import ABC, abstractmethod
from collections import namedtuple
from typing import Optional, Dict, Any, List, Union
from anthropic import Anthropic
from tenacity import retry, stop_after_attempt, wait_exponential
from socraticai.config import ANTHROPIC_API_KEY, DEFAULT_LLM_MODEL

try:
    from google import genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

logger = logging.getLogger(__name__)
MODEL = DEFAULT_LLM_MODEL

# Model type mappings
ANTHROPIC_MODELS = {
    "claude-3-5-haiku-latest",
    "claude-sonnet-4-20250514"
}

GEMINI_MODELS = {
    "gemini-2.5-pro",
    "gemini-2.5-flash",
    "gemini-2.5-flash-lite-preview-06-17"
}

def get_all_models() -> List[str]:
    """Get all supported models."""
    return list(ANTHROPIC_MODELS) + list(GEMINI_MODELS)

class LLMResponse(ABC):
    """
    Abstract base class representing a response from a language model.

    Attributes:
        content (str): The text content generated by the language model
        metadata (Dict[str, Any]): Optional metadata associated with the response
    """
    def __init__(self, content: str, metadata: Optional[Dict[str, Any]] = None):
        self.content = content
        self.metadata = metadata or {}

class AnthropicLLMResponse(LLMResponse):
    """Anthropic-specific LLM response implementation."""
    pass

class GeminiLLMResponse(LLMResponse):
    """Gemini-specific LLM response implementation."""
    pass

class BaseLLMChain(ABC):
    """Abstract base class for LLM chains."""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        # self.model_name is initialized in subclasses
        self._setup_client()
    
    @abstractmethod
    def _setup_client(self):
        """Setup the specific client for this LLM provider."""
        pass
    
    @abstractmethod
    def generate(self, 
                prompt: str, 
                system_prompt: Optional[str] = '',
                temperature: float = 0.7,
                max_tokens: int = 4096,
                thinking_tokens: int = 0) -> LLMResponse:
        """Generate a response using the specific LLM provider."""
        pass

class AnthropicLLMChain(BaseLLMChain):
    """Anthropic Claude implementation of LLM chain."""
    
    def __init__(self, model_name: str, api_key: Optional[str] = None):
        self.model_name = model_name
        super().__init__(api_key=api_key)

    def _setup_client(self):
        self.api_key = self.api_key or ANTHROPIC_API_KEY
        if not self.api_key:
            raise ValueError("Anthropic API key must be provided or set in ANTHROPIC_API_KEY environment variable")
        self.client = Anthropic(api_key=self.api_key)
        
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def generate(self, 
                prompt: str, 
                system_prompt: Optional[str] = '',
                temperature: float = 0.7,
                max_tokens: int = 4096,
                thinking_tokens: int = 0) -> AnthropicLLMResponse:
        """
        Generate a response using Anthropic's Claude model.
        
        Args:
            prompt: The main prompt to send to the model
            system_prompt: Optional system prompt to set context
            temperature: Controls randomness in the output (0.0 to 1.0)
            max_tokens: Maximum number of tokens to generate
            thinking_tokens: Maximum number of tokens to use for thinking

        Returns:
            AnthropicLLMResponse object containing the generated content and metadata
        """
        max_tokens = max(max_tokens, thinking_tokens+1)
        try:
            messages = []
            
            messages.append({
                "role": "user",
                "content": prompt
            })
            kwargs = {
                "model": self.model_name,
                "max_tokens": max_tokens,
                "messages": messages,
                "temperature": temperature
            }
            if system_prompt:
                kwargs["system"] = system_prompt
            if thinking_tokens > 0:
                kwargs['temperature'] = 1
                kwargs["thinking"] = {
                    "type": "enabled",
                    "budget_tokens": thinking_tokens
                }

            response = self.client.messages.create(**kwargs)
            
            if thinking_tokens > 0:
                content = response.content[1].text
            else:
                content = response.content[0].text
            
            return AnthropicLLMResponse(
                content=content,
                metadata={
                    "model": response.model,
                    "usage": response.usage,
                    "provider": "anthropic"
                }
            )
            
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            raise

class GeminiLLMChain(BaseLLMChain):
    """Google Gemini implementation of LLM chain."""
    
    def __init__(self, model_name: str, api_key: Optional[str] = None):
        self.model_name = model_name
        super().__init__(api_key=api_key)

    def _setup_client(self):
        if not GENAI_AVAILABLE:
            raise ImportError("google-generativeai package is required for Gemini models. Install it with: pip install google-generativeai")
        
        self.api_key = self.api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("Google API key must be provided or set in GOOGLE_API_KEY environment variable")
        
        self.client = genai.Client(api_key=self.api_key)
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def generate(self, 
                prompt: str, 
                system_prompt: Optional[str] = '',
                temperature: float = 0.7,
                max_tokens: int = 4096,
                thinking_tokens: int = 0) -> GeminiLLMResponse:
        """
        Generate a response using Google's Gemini model.
        
        Args:
            prompt: The main prompt to send to the model
            system_prompt: Optional system prompt to set context
            temperature: Controls randomness in the output (0.0 to 1.0)
            max_tokens: Maximum number of tokens to generate
            thinking_tokens: Ignored for Gemini (not supported)

        Returns:
            GeminiLLMResponse object containing the generated content and metadata
        """
        try:
            # Prepare the content with system prompt if provided
            contents = []
            if system_prompt:
                contents.append(f"System: {system_prompt}\n\nUser: {prompt}")
            else:
                contents.append(prompt)
            
            # Create the config using the correct types
            config = genai.types.GenerateContentConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
            )
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=contents,
                config=config
            )
            
            # Check if response has content
            if not response or not hasattr(response, 'text') or response.text is None:
                logger.error(f"Gemini response was empty or None for model {self.model_name}")
                logger.error(f"Response object: {response}")
                if hasattr(response, 'candidates') and response.candidates:
                    logger.error(f"Candidates: {response.candidates}")
                    if response.candidates[0].finish_reason:
                        logger.error(f"Finish reason: {response.candidates[0].finish_reason}")
                raise RuntimeError(f"Gemini model {self.model_name} returned empty response")
            
            return GeminiLLMResponse(
                content=response.text,
                metadata={
                    "model": self.model_name,
                    "provider": "gemini",
                    "usage": {
                        "prompt_tokens": response.usage_metadata.prompt_token_count if hasattr(response, 'usage_metadata') and response.usage_metadata else None,
                        "completion_tokens": response.usage_metadata.candidates_token_count if hasattr(response, 'usage_metadata') and response.usage_metadata else None,
                    }
                }
            )
            
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            raise

# Unified LLMChain class that works with all providers
class LLMChain(BaseLLMChain):
    """
    Unified LLM chain that automatically routes to the appropriate provider based on model name.
    
    This class provides a single interface for working with multiple LLM providers.
    You can specify the model in the constructor and it will automatically use the right provider.
    
    Examples:
        # Use Anthropic Claude
        chain = LLMChain(model="claude-3-5-sonnet-20241022")
        
        # Use Google Gemini  
        chain = LLMChain(model="gemini-1.5-pro")
        
        # Use default model (from config)
        chain = LLMChain()
        
        # Temporarily override model for one generation
        chain = LLMChain(model="claude-3-5-sonnet-20241022")
        response = chain.generate("Hello", model="gemini-1.5-pro")
    """
    
    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):
        """
        Initialize the LLM chain with automatic provider routing.
        
        Args:
            model: Model name to use (defaults to DEFAULT_LLM_MODEL)
            api_key: API key for the provider (optional)
        """
        self.model = model or MODEL
        self.api_key = api_key
        self._chain = None
        self._setup_client()
    
    def _setup_client(self):
        """Setup the appropriate client based on the model."""
        # Use the internal factory method to create the appropriate chain
        self._chain = self._create_llm_chain(self.model, self.api_key)
    
    @staticmethod
    def _get_provider_from_model(model_name: str) -> str:
        """Determine the provider based on the model name."""
        if model_name in ANTHROPIC_MODELS:
            return "anthropic"
        elif model_name in GEMINI_MODELS:
            return "gemini"
        else:
            # Default to anthropic for unknown models
            logger.warning(f"Unknown model '{model_name}', defaulting to Anthropic")
            return "anthropic"
    
    @staticmethod
    def _create_llm_chain(model_name: str, api_key: Optional[str] = None) -> BaseLLMChain:
        """
        Factory method to create the appropriate LLM chain based on the model name.
        
        Args:
            model_name: The model to use
            api_key: API key for the provider
            
        Returns:
            Appropriate LLMChain instance
        """
        # No global MODEL manipulation needed here
        try:
            provider = LLMChain._get_provider_from_model(model_name)
            
            if provider == "anthropic":
                return AnthropicLLMChain(model_name=model_name, api_key=api_key)
            elif provider == "gemini":
                return GeminiLLMChain(model_name=model_name, api_key=api_key)
            else:
                raise ValueError(f"Unsupported provider: {provider}")
        except Exception as e: # This will catch errors during chain creation
            logger.error(f"Error creating LLM chain for model {model_name}: {str(e)}")
            raise
    
    def generate(self, 
                prompt: str, 
                system_prompt: Optional[str] = '',
                temperature: float = 0.7,
                max_tokens: int = 4096,
                thinking_tokens: int = 0,
                model: Optional[str] = None) -> LLMResponse:
        """
        Generate a response using the appropriate LLM provider.
        
        Args:
            prompt: The main prompt to send to the model
            system_prompt: Optional system prompt to set context
            temperature: Controls randomness in the output (0.0 to 1.0)
            max_tokens: Maximum number of tokens to generate
            thinking_tokens: Maximum number of tokens to use for thinking (Anthropic only)
            model: Optional model to use for this specific generation (overrides instance model)

        Returns:
            LLMResponse object containing the generated content and metadata
        """
        # If a specific model is requested for this generation, use a temporary chain
        if model and model != self.model:
            temp_chain = self._create_llm_chain(model, self.api_key)
            return temp_chain.generate(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                thinking_tokens=thinking_tokens
            )
        
        # Otherwise use the instance's chain
        return self._chain.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            thinking_tokens=thinking_tokens
        )
    
    def process_chain(self, 
                     initial_input: str,
                     processors: List[callable],
                     **kwargs) -> List[LLMResponse]:
        """
        Process a chain of operations on the input text.
        
        Args:
            initial_input: The starting text to process
            processors: List of processing functions to apply sequentially
            **kwargs: Additional arguments to pass to processors
            
        Returns:
            List of LLMResponse objects from each processing step
        """
        results = []
        current_input = initial_input
        
        for processor in processors:
            result = processor(current_input, **kwargs)
            results.append(result)
            current_input = result.content
            
        return results
    
    @property
    def provider(self) -> str:
        """Get the provider name for this chain."""
        return self._get_provider_from_model(self.model)
    
    def __repr__(self) -> str:
        return f"LLMChain(model='{self.model}', provider='{self.provider}')"

# Backwards compatibility functions (now just wrappers around LLMChain methods)
def get_provider_from_model(model_name: str) -> str:
    """Determine the provider based on the model name. (Backwards compatibility)"""
    return LLMChain._get_provider_from_model(model_name)

def create_llm_chain(model_name: Optional[str] = None, api_key: Optional[str] = None) -> BaseLLMChain:
    """
    Factory function to create the appropriate LLM chain based on the model name. (Backwards compatibility)
    
    Args:
        model_name: The model to use (defaults to DEFAULT_LLM_MODEL)
        api_key: API key for the provider
        
    Returns:
        Appropriate LLMChain instance
    """
    return LLMChain._create_llm_chain(model_name or MODEL, api_key)
