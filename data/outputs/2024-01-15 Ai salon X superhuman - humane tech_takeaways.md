These are the takeaways from the conversation: 2024-01-15 Ai salon X superhuman - humane tech.m4a

# Insights
- The path to positive change is complex - even technologies or movements fueled by good intentions can lead to harmful unintended consequences. Assessing humane AI requires considering both immediate impacts and long-term ripple effects across societies.

- Defining "humane" raises fundamental questions about what it means to be human and have dignity. Rather than a vague platitude, humanness could be unpacked by examining human needs for belonging, self-actualization, autonomy, etc.

- AI progress may displace many human jobs, causing economic precarity even if new roles arise long-term. Those building transformative AI should transparently grapple with these tradeoffs rather than tiptoeing around potential downsides.

- Establishing consent, transparency and control in data collection supports human dignity. But optimizing purely for individuals' stated preferences may not account for society's overall well-being. Defining "humane" requires balancing both.

- Lawyers navigating regulatory compliance could see job impacts from AI. But the empathy gap between blue-collar and white-collar disruption makes this loss less visible or pressing to those shaping the technology.

- Defining "humane" in relation to AI is challenging because humans have biological constraints on how fast we can adapt, yet AI systems can operate at speeds far beyond human comprehension. Ensuring AI progresses at a rate commensurate with human adaptive capabilities could be part of a definition of "humane" technology.

- People's stated preferences often differ from their true, underlying preferences. An AI system that could see through the distortion of societal norms to identify people's core needs could reveal more humane solutions, even if unintuitive.

- Building AI applications that require total transparency of personal data to function optimally (e.g. networked self-driving cars) could enable unprecedented surveillance and threaten civil liberties. Avoiding such applications may be more humane, despite efficiency trade-offs.

- Humane could mean AI communicating with each person to understand their personal dignity boundaries and not crossing them. But requiring people to make their boundaries explicit creates privacy issues and expansive trackability. There are inherent tensions between personalized humane treatment and other values like privacy.

I aimed to extract some of the more nuanced perspectives that highlight tensions or unintuitive insights related to defining or achieving "humane" AI. Please let me know if you would like me to modify or expand on any part of my response.

- The creators of AI systems imprint their own biases and values into the systems, which can lead to unfair outcomes for certain groups. We likely will never have enough tests to uncover all the potential biases.

- Surveillance technologies like recording devices are advancing whether we consent or not. However, there are privacy-preserving technologies like encryption that could allow us to still benefit from AI while maintaining control over our data.

- The challenges with governing powerful technologies are not unique to AI - humans have struggled with similar issues around things like nuclear weapons. There may not be a perfect solution, but we can aim for starting points like evaluating products for addictiveness or other clearly negative traits.

- The interface and user experience with AI systems matters greatly for how humane the systems feel. A conversational interface can help create a more human connection, building trust and understanding between humans and AI.

- Considering the appropriate scale and scope of AI systems is key for keeping them comprehensible and controllable by humans. As McLuhan noted, new technologies can operate at inhuman scales if we don't consciously constrain them.

- Strictly encoding laws and rights into AI systems risks brittleness or gaming of metrics. Some flexibility and human judgment is still needed in interpreting principles fairly. We cannot always simplify ethics down to unambiguous rules.

- Autonomous vehicles present a microcosm of the challenge of encoding ethics, with many open questions around prioritizing different types of harm. More transparency is still needed on how companies are solving real ethical trade-offs.

- Current autonomous vehicles systems still have very basic safety-focused programming, not sophisticated ethical reasoning. But they reveal telling priorities, like extra caution around small children.

- Implementing AI safety through funding startups rather than top-down regulation could lead to more innovative and adaptive solutions. Government grants empowering people to build companies targeting specific issues may be more effective than broad laws.

- Shifting the role of analysts from just data interpretation to driving implementation of their recommendations could increase business impact. Analysts diagnosing problems then managing solutions leverages their insights for organizational improvement.

- Creating win-win solutions benefiting users and companies alike is most sustainable. For example, privacy tools improving experience while still enabling value capture meet needs on both sides.

- Preparing society for AI change through retraining may be more important than debating if the tech itself is humane. Providing skills development opportunities counterbalances downsides like job losses.

- Implementing AI systems in the real world involves many challenges beyond just developing the core algorithm, like dealing with outliers, population shifts, interface design, and training users. Significant effort is required to translate lab performance into practical viability.

- There are incentives working against rapid AI adoption, like existing systems working well enough, high costs of transformation, and legacy capital investments. This suggests AI could diffuse into society more slowly than its capabilities might suggest.

- AI assistants may initially accelerate human productivity rather than replace jobs, until they hit skill bottlenecks requiring scarce human judgment. This creates opportunities to redesign roles around scarce human skills.

- Creative chaining of AI systems together can push towards closing the last gaps in capability compared to humans. But progress requires an artistic, exploratory mindset to discover new human+AI combinations.

- Abstraction and ease of use are key to unlocking the value of AI for non-experts. Product developers play a crucial role in whisping to language models and translating results into human-centric experiences.

- There is a lack of market incentive for companies developing AI systems to seek input from vulnerable and marginalized groups who may be impacted, even though their perspectives are critical. Nonprofits could play a role in aggregating those perspectives to inform system development.

- Questions used in prediction systems like parole algorithms can be extremely loaded and introduce all kinds of unintended biases. An evolving nonprofit "red team" service could help identify problematic questions or risk factors.

- Defensive and pro-social technologies like the nonprofit organization Polis could make democratic participation in technology design cheaper and easier by enabling crowdsourced input.

- Private sector attempts to influence regulation and policy in self-serving ways, like FTX in crypto trying to capture the regulatory process, should be viewed cautiously. Policy should aim for societal good.

- There are complex stakeholder ecosystems and incentive structures when developing AI systems that impact critical societal functions like parole, housing allocation, etc. Mapping those out thoughtfully is needed.

- [Insight 1] AI systems that answer questions without judgment could transform education and make learning more accessible, allowing people to ask "stupid" questions without fear. This could empower people to acquire new skills and knowledge more easily.

- [Insight 2] An AI moderator that summarizes key points and threads from a conversation in real-time could greatly enhance the value derived from group discussions. It may enable richer idea exchange and impact analysis.

- [Insight 3] Defining "humane" tech based on human scale and meaningful human interfaces risks overlooking use cases without direct human interaction. Judgment of "good" or "bad" requires examining indirect and systemic impacts too.

- [Insight 4] Reliance on a single "source of truth" AI risks narrowing perspectives and censorship. Diversity of opinion has inherent value for revealing nuance and multiple valid viewpoints, which an AI may fail to capture.



# Questions
- How can we better define "humane" in the context of AI systems - what principles or values does it encompass?

- What does it mean for an AI system to respect human dignity?

- How can we balance individual versus societal perspectives when evaluating the "humaneness" of an AI system?

- How do we reconcile good intentions in developing AI with unpredictable or unintended consequences?

- What are the core human needs we should optimize for with AI, and what tradeoffs emerge?

- How will advances in AI impact jobs and livelihoods - what is our responsibility around displacement?

- How can we build AI systems that adapt to human biological constraints and rates of change?

- How can AI balance progress and job displacement in a humane way?

- How can self-driving cars avoid becoming a mass surveillance system that threatens civil liberties?

- How can AI systems understand and respect the individual dignity and consent boundaries of each person?

- How can we ensure AI systems treat all demographics fairly when the models are too complex to fully audit for biases?

- If surveillance becomes ubiquitous, how can we preserve privacy and consent while still reaping societal benefits of data?

- Can decentralized, privacy-preserving technologies like blockchain provide an alternative to mass surveillance that protects liberty?

- How can we empower individuals with control and safety in a world where a few bad actors could create existential threats with technologies like AI?

- What clear ethical lines or metrics, like addictiveness, can we establish to guide development of humane AI systems?

- How can we implement more humane interfaces and experiences at scale across technologies to give individuals more autonomy and control?

- How can we translate principles like "fairness" into practical implementations in technological systems that avoid pitfalls like Goodhart's law?

- Can laws and regulations be clearly defined and iteratively updated to provide guidance for implementing ethical principles in technology?

- How are autonomous vehicle companies solving ethical dilemmas and implementing humane approaches to protecting different types of road users?

- Should we implement specific new laws for AI that are different from existing laws?

- What kinds of laws or incentives could nudge the development of AI towards more ideal outcomes?

- How can we create accountability systems and evolvable governance for AI as values change over time?

- How can we shift the focus from purely analyzing data to implementing solutions and creating value?

- How can technologies like AI tutors help mitigate issues like job losses from automation?

- How long will it take for AI technologies to be widely adopted and implemented across industries?

- What is the timeline and path for AI to reach advanced capabilities like artificial general intelligence (AGI)?

- What are the key bottlenecks and barriers that will slow AI implementation and adoption at scale (last 5-15%)?

- How can we design AI systems and human processes around them to be robust, avoid failures, and ensure successful adoption?

- How can we make AI systems intuitive and easy to use for new users without deep expertise?

- Should we have a governmental institution that helps mediate and create fair models and policies around AI systems?

- What can we as a society do to make it easier for startups to incorporate feedback from affected groups into their AI systems?

- How can we incentivize organizations to interact with resources that represent the viewpoints of marginalized communities when building AI systems?

- How do we get AI to help us create good election processes and government policies that people support?

- How can we create AI systems for participatory governance and aggregating human perspectives in a way that is transparent and resistant to censorship?

- What will society and business look like when AI makes us more efficient and productive across many industries - will there be new opportunities or will no one need to work anymore?

- Who gets to decide what the "truth" is in AI systems acting as a single source of truth, and does the diversity of human opinion itself have value?

- Can highly technical AI systems with little human interface like high frequency trading still be considered humane?

- What exactly constitutes a "humane" AI system on the scale of human life?



# Disagreements
- [Disagreement 1] There was disagreement over whether preserving status quo can be considered humane when AI brings major disruption. One viewpoint was that preserving status quo cannot be part of the definition of humane. The other viewpoint was that removing people's jobs without universal basic income would not be humane.

- [Disagreement 2] There was disagreement over whether the definition of humane should be based on people's stated preferences versus their true underlying preferences. One viewpoint was that stated preferences can be distorted by societal norms and AI could reveal people's true preferences. The other viewpoint was that basing the definition on people's true preferences pushes the goalposts on what is considered humane.

- One disagreement was around whether AI and related technologies are inevitable and uncontrollable. Some argued that individuals will inevitably develop dangerous AI applications, while others felt there are promising privacy-preserving technologies that could provide more choice and control.

- Another disagreement was on whether the challenges raised by AI are unique or just instances of general societal problems humans have always grappled with, like differing views on ethics and governance. Some felt AI uniquely empowers individuals in ways that defy existing social contracts, while others argued issues of consent, harm, and imposition of values occur in all technologies and social systems.

- One disagreement is around whether laws and rights can be clearly defined and encoded into technological systems. Speaker A argues this is possible and would address issues around implementing policies. Speaker I disagrees and says translating laws into practical implementation is more complex, using fair lending laws as an example.

- Another disagreement is whether autonomous vehicles have sufficiently accounted for ethical considerations in their programming yet. Speaker E suggests autonomous vehicle companies are already solving "ethical dilemmas by algorithm" like the trolley problem. Speaker A disagrees and says these companies are less than 10% of the way to full autonomy and mostly just prioritize protecting the driver, with some special priority for child pedestrians.

- There was disagreement over whether new, AI-specific laws and restrictions are needed, or if we should focus more on properly implementing existing laws. One speaker argued we may need new laws tailored for AI, while another speaker felt we should first try to properly apply current laws before creating new ones.

- Speakers disagreed on whether regulations and laws should be more rigid or flexible going forward. One speaker felt new laws often take a "30,000 foot view" and can be too rigid, while another speaker argued we need some accountability through auditors, funding requirements, etc.

- How feasible it is for startups to incorporate feedback from a diverse group of people who stand to be impacted by AI systems. Some argue that this is difficult or not feasible for startups, while others insist it is critical and never works to proceed without user input.

- Whether government regulation, oversight, or some kind of public entity is necessary to facilitate the responsible development of AI systems versus leaving it to markets and private companies alone. There are disagreements around the risks of regulatory capture and how to avoid that.

- How to incentivize organizations, especially private companies, to appropriately consider the voices and perspectives of vulnerable populations most likely to be impacted by AI systems. Some argue there needs to be requirements and oversight, while others focus more on creating shared public resources.

- [Disagreement 1] There was disagreement over whether having a single source of truth from AI systems is problematic, with one speaker raising concerns that it could enable censorship and lacks nuance compared to human perspectives. Another speaker argued that transparency is still important even if not sufficient.

- [Disagreement 2] There was disagreement over whether the concept of "humane" technology is inherently good or could also enable harm, with one speaker questioning whether high frequency trading with little human interface could still be considered humane. Another speaker argued that humane seems promising but does not automatically equate to good.

