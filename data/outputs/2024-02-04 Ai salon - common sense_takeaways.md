These are the takeaways from the conversation: 2024-02-04 Ai salon - common sense.m4a

# Insights
- The speakers have diverse backgrounds and interests related to AI, from anthropology to physics to policy, reflecting the interdisciplinary nature of the field. There is an opportunity to combine these perspectives to develop a more well-rounded understanding of AI.

- There was discussion around the constructivist view of common sense - that it emerges from consensus within a group rather than being innate. This raises interesting questions around whether AI systems could develop their own consensus-based "common sense" that differs from human common sense.

- Several speakers touched on the idea that common sense involves reasoning about how the world works in ways that may not be written down or formally captured. This poses a challenge for developing AI systems that can demonstrate human-like common sense.

- The speakers are interested not just in AI capabilities, but also real-world applications and impacts. This includes building guardrails and considering ethics. There is an opportunity for more nuanced discussions around actual versus imagined AI uses.

- Multimodality beyond just language, including symbolic logic and embodied experience, may be important for developing common sense in AI. This suggests value in cross-disciplinary collaboration, for example between natural language and robotics researchers.

Let me know if you would like me to modify or add any insights. I aimed to pull out some of the most creative ideas that could lead to further discussion.

- There seems to be a fundamental tension between whether common sense is shared universally or is socially/culturally constructed and diverse. Resolving this tension could be important for determining how to build and evaluate AI systems.

- Common sense may include both physical/folk knowledge (like objects fall down) as well as social/cultural norms and expectations. AI systems currently lack both.

- There are many unspoken social norms and expectations that are "common sense" to humans but need to be made explicit for an AI system to properly understand and act on them.

- Building systems with more embodied forms of learning and interaction (beyond just text) could help them develop more human-like common sense.

- We need better ways to handle and be transparent about the subjective biases that exist in AI systems, rather than hiding behind a guise of "unbiased" models.

- There is a need for better instrumentation and infrastructure to support automated scientific discovery that can keep pace with innovations in AI.

- The concept of "common sense" is culturally constructed and depends on shared sensoria and folk models within a culture. Different cultures may have different concepts of common sense based on what senses and experiences they emphasize.

- There are open questions around whether AI systems can truly acquire "common sense" just through passive observation of human behavior and interactions. Some believe embodied, causal intervention may be necessary.

- Communication methodology could be key - finding the optimal way for humans to communicate abstract concepts like "common sense" to AI systems. This will likely involve much trial and error.

- There are analogies between quadriplegics mostly experiencing the world through text and current AI systems mostly experiencing the world through text datasets. This raises philosophical questions around whether common sense could emerge in such limited worlds.

- Social dynamics and common sense have strong overlaps, so AI observation of human social interactions could lead to some "social common sense," even if fuller understanding requires embodiment.

- There are epistemological uncertainties around whether AI systems can ever have true understanding versus just mappings between input patterns and output behaviors.

- Common sense is difficult to define precisely, but relates to intuitive knowledge and reasoning about the everyday physical and social world that often goes unsaid. Current AI systems lack certain basic common sense abilities like spatial reasoning and counting that humans take for granted.

- Purely autoregressive training on internet data may not be sufficient for picking up on common sense knowledge that is implicit or subtextual rather than explicitly stated. Alternative training methods may be needed to instill basic common sense.

- There are likely multiple cultural perspectives and value systems reflected in what we consider "common sense." For AI to reflect human common sense, it may need to encompass multitudes and multiple worldviews rather than just privileging one cultural viewpoint.

- The human capacity for common sense likely emerges gradually through embodied experience and socialization over years of development. Capturing this likely requires prolonged multimodal learning grounded in physical environments and interactions, not just textual training objectives.

- There may be universal human experiences like pain, hunger, and pleasure that could form the basis for "common sense" that applies across cultures. Capturing how parents teach children about these experiences could help train AI systems.

- AI systems may need their own "machine common sense" that is distinct from human common sense - basic facts about computational processes and goals that humans take for granted.

- Traveling to new cultures reveals gaps in our common sense when we commit faux pas. We then learn to be more humble and adjust our assumptions. This demonstrates common sense is not fixed but adaptable.

- Activist groups form shared opinions they assume are "common sense," then spread them to others. This shows common sense can be deliberately shaped and propagated.

- Biological processes like heartbeats seem innate to humans, but may resemble how computers have innate "knowledge" about computational processes. This blurs the lines between human and machine common sense.

- The speakers acknowledge the limitations of relying solely on "common sense" to guide AI systems, since common sense can be flawed. More rigorous frameworks like media literacy may be needed to help people critically evaluate AI.

- There is a tension between the centralized, "authoritarian" way many AI models currently operate, versus more democratic approaches that incorporate diverse human perspectives. Collectively "fine-tuning" models could make them more aligned with human values.

- The speakers note much of human morality and values are implicit, not explicitly coded. Translating these implicit societal norms into explicit rules for AI systems is an immense challenge.

- As AI systems grow more advanced, we may need to consciously impart our value systems and explain why certain behaviors are right or wrong, rather than just dictating rules. This could help AI gain a deeper understanding of human ethics.

- The speakers envision a future where multiple AI models with differing moral reasoning co-exist, like Confucian versus Halal AIs. Allowing choice between these could accommodate moral diversity.

- Implicit social norms and values are more prevalent globally than explicit ethical systems, which are more common in individualistic, secular societies. Making ethics explicit could be valuable.

- Organized religion may have an advantage in developing AI alignment, as their values and norms tend to be clearly defined in texts.

- Subcultures that engage in higher risk activities seem to develop more explicit norms around consent out of necessity. These concepts could benefit society more broadly.

Without more context it is difficult to confidently assess the key insights and their implications. More background on the specific discussion and speakers could help drive more creative analysis. Please let me know if you have any other details to provide.

- Defining common sense or appropriate behavior becomes more complex as we interact with more types of intelligent systems. What's considered "common sense" expands to encompass multiple perspectives.

- There is a tension between developing AI systems that reflect individual/group values versus shared, societal values. Custom models could reinforce existing beliefs, while centralized models aim for consensus.

- The goals behind developing AI systems shape what capabilities we give them. Communication technologies may prioritize representing user perspectives, while acting technologies prioritize performance.

- Access barriers exist today that prevent broader, decentralized development of AI systems reflecting diverse viewpoints. Addressing issues of data, compute access, and skills development could enable more pluralistic AI.

- Human judgment, taste and values will likely continue shaping AI development for the foreseeable future, even as systems become more capable. True autonomy requires not just intelligence, but sensitivity to human preferences.

- Self-driving cars provide a useful testbed for developing AI agents that can interact appropriately in complex real-world environments with humans and other agents. However, there will likely be many edge cases that are difficult to anticipate and program for.

- Transmitting common sense intuitions to AI systems may require meticulously documenting how doctrines/norms apply in a wide variety of circumstances, similar to how religious texts like the Talmud evolved. But new situations may still require special effort to determine how to apply principles.

- There are limits to self-play when modeling human interactions, since human behavior is often not fully rational. Systems trained only on self-play may not capture important social nuances needed to interact properly with people.

- Understanding subtle social norms (like signaling behaviors in parking lots) requires careful study, but operationalizing these behaviors in AI systems remains challenging. Quickly laying off expert social scientists indicates overconfidence in systems’ abilities.

- When systems like self-driving cars largely behave correctly, people may overly anthropomorphize them and assume human-like reasoning underlies their actions. This can lead to failures when people apply false assumptions.



# Questions
- What does it mean for very large generative models to display reasoning about the world as they get sufficiently complex, and how is that reasoning impacted by how we set up the problem when training models?

- Do machine learning models have a form of knowledge humans would consider "common sense," and if so, what would we call it compared to human common sense which is constructivist in nature?

- How much beyond language processing do we need to go to accurately simulate human interactions - do we need to incorporate symbolic logic and embodied learning?

- What are the applications of AI in spaces that are not early adopters, like nonprofits and social work, and why does no one seem to be building tools for those segments yet?

- How can we construct AI systems that have a shared understanding of basic physical and social realities that humans take for granted?

- What sources of common sense knowledge, such as embodiment or social context, are missing from current AI systems?

- How can we design AI systems to make their inevitable biases explicit and controllable rather than claiming they are "unbiased"?

- What aspects of common sense and shared understanding do most cultures actually have in common, even if there are some differences?

- Is there a set of facts or knowledge that everyone can agree on that could serve as a starting point for future models?

- Should we try to find an unbiased, universal "common sense", or instead have transparent representations of the specific common sense a model is using?

- Does common sense shift in response to interacting with large models that know things we don't and vice versa?

- Is common sense something that can be generated outside of experience, or is experience an important constraint on the term?

- Can an AI develop common sense purely through observational learning like watching TV, or does it need to be able to actively test causal hypotheses like humans do?

- Is common sense fundamentally different from other types of knowledge, or is it just a very broad and complex set of knowledge?

- Does embodiment and having a carbon-based existence fundamentally change perception and reasoning compared to silicon-based systems?

- If an AI's entire world was just text, could it still develop common sense like a quadriplegic person can, or would the data stream be too impoverished?

- Can social dynamics and interactions be learned purely through observation, or is there an experiential component that requires active participation?

- Do humans even agree on what common sense is, given differences in language, meaning and individual experience?

- Does ChatGPT have common sense or does it just seem like it has common sense in certain narrow contexts?

- What is the definition of "common sense" and what types of common sense are missing from current AI systems?

- Why is common sense so "invisible" and implicit that AI systems fail to learn it from data, even though it seems obvious to humans?

- How can spatial reasoning and counting abilities, which seem like basic common sense to humans, be so lacking in advanced AI systems like ChatGPT?

- Can a foundation model pretrained on enough diverse data develop general common sense purely through observation, or is explicit training on common sense reasoning tasks necessary?

- Can emotions and subjective experiences like pain and pleasure ever be truly understood or experienced by AIs?

- Is there a unique set of "machine common sense" that is different from human common sense and that AI systems need to function effectively?

- Should we strive for AIs to have human-like common sense or should we focus on developing a distinct "machine common sense"?

- Is there an inherent "common sense" in how computer systems operate (i.e. information processing) similar to how biological systems have inherent common senses?

- How can we build AI systems that understand and respond appropriately to human frustration as a training signal?

- Can we use human emotional responses like frustration to help constrain an AI system's action space and teach it common sense?

- Do human values and common sense actually require more complexity to model than perceptual tasks like image recognition?

- How can we build AI systems that are more responsive inconversation and pick up on human cues, rather than just providing pre-programmed responses?

- How can we get AI systems to listen and adapt better when humans explicitly prompt them to improve their responses?

- How can we develop a more democratic process for collectively fine-tuning or adjudicating AI model outputs, rather than relying on a centralized authority?

- What kind of "literacy" do people need to develop common sense about AI systems? Can we rely on common sense alone, or do we need something more like media literacy?

- How can we translate our implicit value systems into explicit instructions for AI models?

- How can AI models capture the depth and multiplicity of human morality when even humans navigate between different moral frameworks?

- What kind of technology are AI models - communication technologies to express myself, mechanical technologies to act on the world, or something else?

- Do we want AI systems to lie or deceive if that's what a user wants them to do?

- Should we aim for AI systems that converge towards human common sense, or allow them to develop a different type of "common sense" that suits their nature and objectives?

- Should we have one overarching AI model that gets fine-tuned, or many small models that represent different groups and perspectives?

- As we interact with more kinds of intelligent beings, will the scope of what we consider "common sense" expand?

- How much can we generalize from self-driving car AI to more general common sense AI agents?

- Will there be a multiplicity of common sense models created by different groups, or will they ensemble into one later on?

- How do you handle all the edge cases and communicate human intuitions about common sense into models?

- What are the limitations of self-play when it comes to human interaction and social norms?

- How do you operationalize the social signaling and unspoken rules humans use to coordinate, so AI systems can properly interact with people?



# Disagreements
- Speaker A introduced the goal of bringing together different perspectives on AI and focusing the discussion on the theme of "common sense"
* Speaker D discussed a constructivist view of common sense as based on consensus rather than objective facts
* Speaker E brought up the idea of applying AI to social work contexts
* Speaker F talked about inferring logic and common sense from modes beyond just language
* Speaker C mentioned integrating multimodal information to provide context and "common sense" to AI systems

While there were some slightly different perspectives expressed, I did not detect any direct disagreements or debates between the speakers. They seemed open to hearing each other's viewpoints without openly contesting them. Let me know if you would still like me to identify potential disagreements from this conversation.

- [Disagreement 1] There is disagreement over whether common sense encompasses basic physical knowledge that all humans share, like knowing humans have five fingers, or if it is limited to social and cultural norms. One speaker argues common sense includes basic physical facts, while another argues it is specifically about navigating social situations and norms.

- [Disagreement 2] There is disagreement over whether common sense is fundamentally the same across all humans or if it differs based on cultural upbringing. One speaker argues there is a universal common sense while another argues common sense differs across cultures.

- There was disagreement over whether common sense should be defined as culturally/socially specific shared understanding versus more fundamental physical understandings like objects not falling through tables. Speaker A proposed the former while Speaker Anastasia argued for the latter.

- There was disagreement over whether there exists a set of facts and knowledge that has near complete overlap and agreement across cultures that could serve as a starting point for AI systems. Speaker E proposed this idea, while Speaker A was skeptical that such universal agreement exists.

- [Disagreement 1] There was disagreement over whether an AI system could develop common sense solely through passive observation of human behavior and interactions, without any ability to actively test hypotheses or intervene in the world. Some argued that active testing and intervention, like what children do, is critical for developing causal models of the world. Others suggested that with enough observational data, an AI may be able to learn enough to mimic common sense.

- [Disagreement 2] There was disagreement over whether an AI system lacks critical aspects of human cognition, like embodiment or shared concepts and language, which might fundamentally limit its ability to acquire common sense no matter how much data it processes. Some suggested social dynamics are the core of common sense and could be learned observationally. Others argued human biases and quirks also shape common sense in ways an AI may not capture.

- Whether current AI systems like ChatGPT have common sense. Some argue that ChatGPT demonstrates common sense in certain areas like giving social advice, while others point out deficiencies in spatial reasoning and basic tasks like counting letters.

- How common sense is acquired or defined. Some propose common sense consists of unstated fundamental knowledge, while others suggest it develops through enough observations and experience. There is disagreement over whether current AI systems acquire common sense similarly to humans.

- Whether common sense for machines should try to emulate human common sense or if there is a separate "machine common sense" that would be more useful for AI systems.

- The role of lived/embodied experience in developing human common sense, and whether that can be captured and translated to machines.

- The importance of early childhood learning from parents in establishing common sense, and whether recording and training AI on such interactions could help provide more human-aligned common sense.

But overall the speakers seem to be exploring these ideas without substantially disagreeing with each other. There are differences in perspective but no clear disputes jumping out from this part of the conversation. Let me know if you'd like me to take another look or re-analyze a different section of the dialogue.

- [Disagreement 1] There is disagreement over whether frustration from humans interacting with AI systems could be used to improve the AI's common sense and constrain its actions. Some argue that this could work well, while others point out that it may require too many real world mistakes from the AI before it learns properly.

- [Disagreement 2] There is disagreement over whether human values and common sense are simple or complex concepts for AIs to learn. One speaker argues they may be simpler than often thought, like image recognition tasks, while another disagrees and says current LLMs do not pick up on human frustration cues in their responses.

- There was disagreement over whether more data and better models can resolve conflicts in AI systems' outputs when the data itself contains disagreements. Some argued that conflicting data ruins models while others said that's not an issue for generative AI which isn't trying to accurately reflect the data distribution.

- There was disagreement over whether developing "common sense" requires making implicit assumptions and values explicit and systematizing them for AI systems versus interacting with AI more like humans, guiding them over time like children to develop common sense without needing to codify everything.

- How to best develop AI systems that can handle complex human concepts like consent and subjectivity - whether to have one large model or many smaller custom models
* The goals and tradeoffs between AI autonomy vs aligning with human preferences
* How feasible it is for AI to capture the nuances of human judgment and taste

However, these were presented more as open questions and tensions to explore rather than strong disagreements between speakers. There seemed to be a collaborative spirit of inquiry rather than opposition. So while there were definitely some areas of difference, I did not detect any substantive disagreements to summarize. The discussion seemed productive in surfacing complex issues around AI and human values.

