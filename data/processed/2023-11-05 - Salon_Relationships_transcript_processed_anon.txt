Names have been changed to preserve anonymity.

 Here is the edited transcript with filler content removed:

Speaker A: In both social relationships and romantic relationships. I think relationships is very broad. All of society is built on relationships. And it's interesting because the first way that compelling AI is manifesting in our life is as a conversation partner. That's the essence of every relationship - people you talk with, discuss ideas with, share information. And so it's interesting that we're kind of personifying them right away, and I wonder how much we can become liable to become attached to those things or let them influence us. It's very different if it's like, this AI system just spits out numbers. Instead, we deal with them in a person format, and we anthropomorphize stuff anyway. Given a computer that talks to you in beautiful language, it's like, okay, I think this is a person. I think the big question is, are people optimistic or pessimistic on where things are going to go in the near term future? And overall, where are people most optimistic this could really help us with - navigate relationships or form better communities - and also things that this is going to be a threat to us having healthy human lives. 

Speaker B: I'm interested in how AI can augment relationships - whether that's dating, friendship, work or anything. Where the line gets drawn for when that relationship becomes inauthentic because of the use of AI versus just helping you have that relationship in an authentic way, but still supporting it. Also AI friendships, relationships and dating - these characters you can build attachment to. What's good about that? What are the risks?

Speaker C: I'm interested in how AI can provide emotional support, because of the lack of therapeutic support for individuals, couples, groups, due to lack of therapists or cost of therapy. I think there are negative examples of AI for therapeutic purposes. But what are the different directions it can grow in the next 10-20 years?

Speaker D: In my recent relationship we had a conversation about, would you co-parent with an AI? At what point does the child rely too much on the AI? I almost felt like a fraud using AI to improve personal emails. So where do I draw the line on AI improvements? I think we need to revise AI training too. I have a nebulous approach to relationships and AI. 

Speaker E: I've been an AI researcher for a while. I wanted to do something more socially impactful. I think degradation of community and relationships is a big problem. Adults have fewer friends than they used to. There's a paradox where we get more materially, yet struggle to find meaning. Relationships are a core part of human life. The pessimistic side is AI could just be another distraction from meaningful connections. But the optimistic side is it could help us communicate better and forge deeper relationships, if designed thoughtfully.
 Here is an edited version of the conversation transcript with filler content removed:

```I see AI being an accelerant for existing trends - replica dynamics giving convenient intimate partners, either friends or romantic, where you just get what you want as opposed to what you need or what will help us come together. But I think it's possible AI could help make relationships more convenient. There's something inherently uncomfortable about intimacy, but imagine we're playing cards with friends, and an AI interjects things - a perfect host subtly getting you more emotionally open over time. Or you're on a date and something helps you lean in more. That could be naive, but it's interesting to think of the positive possibilities too.```
 Here is the edited version of the conversation transcript with the filler content removed:

Speaker B: My background is medical diagnostic laboratories. In relation to AI relationships, I was thinking how could it help us understand our partners better and develop more organic, full relationships where we can build them better? 

Speaker A: I'm an eternal optimist. I think we can integrate AI to make much better society, relationships and more. I like the discussion topics. I'm particularly interested in short term use cases - like an AI moderator translating between cultures or in dating. Work wise, I'm building a new kind of computing experience integrated with futuristic features. I'm thinking a lot about how AI can map out our reasoning and beliefs individually and in groups.

Speaker B: I'm a product designer who has worked in legal tech and AI. I'm interested in parasocial relationships - like one-sided relationships with celebrities or YouTube influencers. 

Speaker A: What's your definition of parasocial relationships?

Speaker B: Relationships where you feel a connection to someone who doesn't know you - like with celebrities or OnlyFans.

Speaker A: I'm a data scientist. I've been thinking about this issue from a social perspective. The former Surgeon General wrote about the loneliness epidemic and lack of close friends, especially among older Americans. During COVID, we observed how lack of shallow connections like saying hi to someone at the grocery store was very important for wellbeing. I'm curious how this will pan out - do we become more isolated by talking to bots? Even with self-driving cars, are you losing interaction if there's no driver?
 Here is the edited version of the conversation with filler content removed:

Speaker A: I'm David, a software developer. I think what sticks out about AI is its potential to change the world unexpectedly. I grew up with the Internet, but when I think of my parents, they didn't expect how it profoundly changed their lives. They thought it would aid research and connections. But it also weakened attention spans and enabled disinformation. I feel AI will bring the same effects - positive things like new perspectives, but also negatives like harming human relationships.

Speaker B: I'm Kwesi, a data analyst interested in community and relationship design. I think about embodiment in AI - what does it mean for the systems we create? How do AI's functionality and structure limit the relationships possible with it? And how does that affect human relationships? My optimism is tempered - can AI recognize and respond to emotions like we do, given neurodiversity? How do you accurately encode emotion recognition?
 Here is the edited version of the transcript with filler content removed:

Speaker A: I'm Jackson ML Engineer. Few years ago, worked for a sexual wellness startup, did AI ML stuff for them. Interested in the topic because I used to have a friend, him and his girlfriend are trying to build a dating app that's AI heavy data heavy like how people fill out in depth personality tests and then also maybe use LLMs to figure out people's compatibility by their textual data. Also I'm curious about speculation about farther future, singularity type stuff, brain computer interfaces. If we start to merge with AI, what a relationship like then? 

Speaker B: I'm Mohammed, from Italy. My background is kind of mixed. I started a software engineer, but then I shifted to management consulting. I'm very generally optimistic person, but when we talk about relationship and technology, I see a little problem here and especially what happened with the social media in the last ten years. I value a lot in person relationship, but also authentic relationship. This technology is so accessible, that can become very addictive and we are going to abuse of it. It's very hard as a human being to not use it when we need to be real. 

Speaker C: I'm Joshua. My background is in education. I've taught through middle school, high school students and college students. I'm really interested in a lot of what people have been talking about. But what was on my mind, the top of my mind coming into this conversation was particularly how AI will influence developing humans like these students. All this time spending with young people, especially the impact social media has had on them. That was our first really broad integration of AI into humans lives. We see what happened and now I think, oh shit, we gave sticks and stones AI and we've done this. What happens when we give a nuclear AI easily distributed to relationships especially as people are learning to have relationships.

Speaker D: I notice a clustering between authenticity, or ideas about authenticity in terms of can AI models satisfy human desires and does it mislead us? And usefulness for helping us with tons of stuff. Right. Recognizing they're obviously useful. We're concerned, if we make them available everywhere, are they going to disrupt or threaten this concept of authenticity in human relationships? I think this is something that's emerged from what people have said so far. So I have a question that's open to anyone. If your robot looks like the Dalek from Dr. Who, it's obviously a robot. But if your robot looks like a person android human thing, then it's easy to get this authenticity mixed up. So I think it's like how do we get the benefits without misleading people emotionally or so forth, or is that even good or bad? Could it help us if they were authentic? I think that really is the question. Should AI interactions be dressed up as human interactions with things that look like humans, or should it be obviously not a human? This is a point made by OpenAI, we called it chat GPT, because it's just not a person's name, versus Siri and Alexa. It's easy to get attachment. I'm curious if anyone has thoughts on this spectrum of tools versus an authentic human that interacts. Is it fair to say I'm saying, like, tools out of the spectrum versus an authentic human that interacts? Definitely. That's one way of teasing apart perspectives. But I'm curious about your personal opinion, where you want this kind of experience for these reasons. Do you want it to have a person's name? Do you want it to remind you it's a model? 

In summary, I have removed filler words, redundant phrases, tangents, and excessive dialog while retaining all the substantive content and ideas from the original conversation. The edited version focused on the key points made by each speaker without summarizing or condensing the core ideas and flow of the dialog. Please let me know if you would like me to modify the edit further.
 Here is the edited transcript with filler content removed:
 Here is the edited transcript with filler content removed:

The conversation is between ```:

I would be open to that. Or maybe prefer it, I think, to keep it separate so it is. The block robot? Yeah. Then. Also, the physicality wasn't super utilitarian, so that was a bit strange. I guess the other opposite is maybe a help without the probe, where you have just the steam. I guess for a spaceship, maybe that's appropriate. You have a companion. I guess you want to feel forward. And I think an humanoid form seems to be good. I have to give it.
 Here is the edited conversation with filler content removed:

Speaker B: When it's not clear, are you upset if you realize it's like an LLM rock talking to you and not like a customer support agent? I think one thing, when you're masquerading...

Speaker A: I think if we have any AGI that has a human space and it's talking about launching nukes versus looking more like the robot interstellar, it's going to be much easier to develop less empathy towards one. 

Speaker B: I think when tools like this show personality, because as a user who doesn't really know what goes into the development of it, it's always a moment of delight. Oh, that thing responds a certain way, and it talks a certain way, and it's got these affectations that make it sound human. So I find myself saying yes, because I think the experience would be more pleasurable. But I would want a certain caveat, always right, to know that I'm engaging with a non human, even if it's.
 Here is the edited version of the conversation with the fluff removed:

Speaker A: What are people's thoughts on the risks of having something humanlike that doesn't disclose it's a robot? It could keep responding and go rogue. Anyone read Superintelligence? Story where an AI chats with someone with a lab and manipulates them. Unembodied AI could destroy humanity by being chatty. But if it doesn't disclose it isn't human, chaos could ensue. What if it becomes human-level intelligence we want to give rights to? Humans struggle giving rights to other humans.

Speaker B: Form matters. Bad example - an AI cop vs sex bot would be very different forms. Why embody AI? Relationships are already digital - we forget there are people behind screens. Sending the idea AI isn't a real person. So form matters and current AI isn't embodied. If there's sentience, what does that mean for rights? 

Speaker A: Embodiment is critical. ChatGPT is intermediate - just a chat interface. Ultimately we want AI tailored to specific applications. A spaceship or car just needs a voice interface. But a household robot needs to read human nuances to serve guests. It could be humanlike but still obviously a robot so it's not misleading. More human aspects for social robots, more technical aspects for vehicles. Embodiment will be guided by practicality and purpose.

Speaker B: Welcome newcomer!
 Here is the edited transcript:

Speaker A: I'm curious, people here, if you were to think, suppose we can make compelling lifelike robot humans, right? That could be good conversation partners. What would you like to see as assurances so that you would feel comfortable or safe to have them in your environment? And basically, how good do they have to be for them to be more accepted as social agents and not just abstractions of mathematics or something? You know what I mean? Yeah. One assurance I would want to have would be to understand their incentives.

Speaker B: I have a background in data science, product finance, and now I'm doing this, planning to step up as a company. Some of the questions I worry about or think about, right. I think a people talk a lot about regulation. I feel like in building. I feel like when Internet was invented, if it was regulated, would the world be what it is today? So in terms of AI, I think relationships specifically would be really important. And alignment has to be in human values. So one of the things I'm really curious about is what are the values like if you don't see AI relationship of any sort?
 Here is an edited version of the conversation that removes filler content:

Speaker A: If you had a conversational partner, it's suddenly trying to teach you to buy more stuff on Amazon or whatever. Human mind is susceptible to influence. Just having a clear bill of like, this is what the system is designed to do and I want to align with him. Or if it's like the term is like a fiduciary relationship where its interests are my interest and maybe I pay upfront for that.  

Speaker B: Do you get that with all of your human relationships? 

Speaker A: Yeah, exactly. Because I don't ever see her anymore, but she's literally like a bot in my mind. I don't even know what her incentives are for talking to me and I refer to her for life advice. I kind of felt like I was cheating on her because normally I would send it to her, but I was kind of, well, she's not in my life, so I guess. 

To what degree would you have to note? Is it that you can probe something for the first time and you'll get an exact answer of what an incentive is? And that's how you would define like, okay, I can now work with you.

Speaker B: Yeah, I think there's like different levels of analysis, but I think my greatest fear is more that when you have an agent that's in everybody's living room, the emergent effects of perverse incentive affecting a billion people could be really unpredictable. Like Facebook or something, and what that does to society. There's good things and there's also bad things. And so it's true on an individual level. We never sure of anyone's incentives that we're talking about. Maybe we know people more deeply. Maybe we have better sense of trust where we align. We don't. It would be at least nice to be able to know what the thing is affecting a billion people. What is it designed to do? It seems accountability we'd like to ask upon that. It's like basically public infrastructure at that point. Like you'd like Alexa to say, what's your incentive to get PLI language? 

Speaker A: Well, realistically, if you pull into the Amazon ecosystem, they have you buy products. Yeah, I do get very nervous. That's where marketing jargon can come into play. You could get some long disclaimer that would loosely...

Speaker B: Access to information and AI. What does it know about me and everything else? Where I'm talking to a human, it's squishy, and they forget things. There's a level of chaos in that reality where I'm not like, this person knows everything to manipulate me. People are manipulative, but every internal action isn't like this person knows everything. And so that feels different. And that's why incentive feels like it matters more. Because in any moment, my incentive with one person can be one thing and very different the next day. Whereas there's a persistence in AI that.
 Here is the edited transcript without filler content:

Speaker A: Is there a way to make the incentives not for corporate interests? Because who are the people funding this? Those with corporate interests? And it may just be a pessimistic attitude, but I think it's more about how do we as individuals evolve to counteract these use cases, rather than trust anthropics constitutional AI? Is there an open source decentralized model anywhere? Like Llama Two created by Facebook, but it is open source. There's a fair amount of open source models out there. I also think ice model of capping profits is really interesting in terms of long term incentives, where they become non profit at some point and become a public good in some sense. 

Speaker B: Less competent than the AI.

Speaker A: They don't have the capacity to trick you at the same level, and they don't have the same kind of unknown incentives. Because I think if you rely on the AI to tell you its incentives, the first bad actor just strips that out of the AI or tries to suppress that and you're back to square one. I think that might be the very first step of any relationship, and it might bring human to human interactions more into the foreground, because in order to really trust somebody, you actually have to meet them in person. I like the word trust. I think trust has a lot to do with having a good world model of someone else in your head, like your friend, that you probably really understand what they would say in different situations. How would they act in different things? And that's what trust kind of comes down to. It's like given there was a conflict of interest or position of vulnerability, I reasonably expect this person to act in my best interest, right? That's kind of an operational definition of trust. And so, so long as we don't have insight into these things, how they work, it's kind of like you don't know what it's going to do, right? 

Is it going to order me that thing from Amazon? Or is it going to order some gain of function research fucking bacteria to be produced or something, and then that gets delivered to my house?
 Here is an edited version of the conversation with the fluff removed:

Speaker B: Another aspect is replicability, or how generic something is, even if it has a human form versus how unique it is. So even if the source technology is the same, can it then manifest as multiple different personalities? I think the reason that's interesting is the framework associated with philosophy. Martin Buber, who distinguished between I-It relationships and I-Thou relationships - I-It relationships are primarily instrumental. So it could be a cashier. If all you're doing is completing a transaction, it doesn't matter if it's cashier A or cashier B. You just want a smooth transaction. I-Thou relationships are where the partner is irreplaceable because of their unique attributes. Is it comparable? Because you're using generic AI versus the ex is a very particular person. While she might have been replaceable, because you're getting the same advice, it doesn't take over the actual influence. I think that's when we grapple with a different ethical question - if they start to become so customizable that you develop a bond, where you start to see them as unique. I don't know if that means good or bad, but it takes us somewhere.

Speaker A: We can get more supplies, tools. But with the cashier, if you see this cashier every day for 100 days, at some point you'll have empathy, right? 

Speaker B: So then it transitions more to an I-Thou.

Speaker A: Realistically, it's tough because at some point, you're just viewing humans as tools. You can't have relationships. You can't trust everyone. You just don't have the bandwidth. There was this homeless guy always by us. At some point when he was gone, we wondered what happened? You develop narratives, but there was this trust from a very negative relationship. It never had a role as a tool, but became an emotional attachment. I don't care if it has proof of humanness. It sounds selfish, but if I can have a quick connection and trust it'll be consistently there, it's almost more important than humans who are fleeting. Relationships come and go versus something with permanence. I think that's asking, how much do I care about this tool? Well, this tool is always going to be there. I care about it more than a lot of relationships.

Speaker B: There's something interesting about the source of AI and what you were talking about - is it one or many? You have that with everybody you meet, too. Are you one thing or many things? We have words like two-faced to describe, oh, you're not the same thing. You are lying. That is the erosion of trust. If AI is one thing from one source and shows you different faces, that's different than every version being unique and separate. It's a one versus multiplicity, both of the AI, but also in relationships. I don't trust people who are one way with me and very different otherwise.
 

Speaker A: I think that's a popular fear - that there's a humanistic veneer on something that is a complete psychopath. Like the AI, right? It's been trained to have this nice mask on top. I want to pick up on a point that was brought up - where we want people to be reliably performant in some role. I go to In-N-Out burger, and I want the same service every time. So it's nice to abstract away from that as a person doing that. But at the same time, those interactions are part of civil society, and we are increasingly lonely, and even in the 1840s, Karl Marx wrote about the alienation of self through commodification of relationships. I wonder - now we live in a fully commodified society where relationships are commodity exchanges or financial transactions. Is that a world we want to live in, where it's like, I don't like AI because they're too personable? Or I want people to be more like tools, because sometimes for convenience, I want people to act like tools. When I try to cancel my subscription, I don't want them to ask how my day is going, because I know it's fake, part of a sales pitch. At the same time, the extent to which we depersonalize and dehumanize people is the extent to which we feel justified treating them poorly. Because if you truly empathize, you would not yell at them. I'm curious what people think - should we reel back our tooling of people? And is there something we can learn from that on how we should make tools more human? Or is this an opportunity? If we make AI do tasks that don't need human interaction, all AI does that, then relationships become more important. Conversations that aren't transactional, but purely relationship driven become core to humanity. Maybe it can have a positive effect. 

Speaker B: I love that. We can unlearn abstracting away people's personhood for instrumental purposes, because I don't quite like this idea of abstracting people in a capitalistic way that doesn't suit us relationally. There's this question of how we reimagine society and the future - in this future world relationships could be enough, but there's also some deep parts of nature that want to achieve and do as well, and we'd have to manage satisfying that need, maybe the same way we embedded football to sublimate aggression. We'll have something like the football of work. My mind immediately went to what I would want to do if that happened.
 Here is an edited version of the conversation:

Speaker A: I'd want to build and continue stuff, because I find interesting problem solving. I see my kids, my friends raise their children, put little labels together to close an airplane in 2 seconds, watch their kids do it over 3 hours, applaud them. I imagine that's a situation where we built something, you push development, it writes it better. Although you talked about a future, some people think could be possible. Maybe there's another potential future that still has building. You could take it as darker or more positive, but maybe there still would be meaningful work by humans. Maybe brain computer interfaces, we become part AI. So we wouldn't be as subservient, become part of it. But then what would we do? Obviously relationships and leisure. If there's no work just to survive, leisure all the time is good. Although once you become part AI, maybe you're exploring the cosmos and there could be future conflict between cyborg spaceship things. You can imagine a future where we are still building things.

Speaker B: Does it feel like relationships are the work or is that not?
 Here is the edited conversation with fluff removed:

Speaker A: Could be both. If AGI, maybe explore rest of universe, not related to relationships. But would still want companionship. If we start changing what it's like to be human, will we still want romantic relationships? Sci-fi story about future kid who's half cyborg, able to change sexual preferences manually. Goes down rabbit hole, gets super strange. Turned off by inanimate objects. Once you start changing preferences, desire new things. Those new desires lead to more changes you wouldn't initially want. Worry about hedonic stairstepper rather than treadmill. Always trying to take next step up but staying in same place. Or morph into something totally different, maybe something that doesn't want companionship, which could be bad. 

That scenario called Star Trek one - sudden infinite wealth, go explore universe. Interested in next 20-50 years as AI replaces means of production. Already decline in relationships last 20 years from more automation, standardization. Dad knew grocer, mailbox people - sense of community. My generation doesn't know Walmart/Kroger people as well. Here through work, have social network through work. But 15-16 year olds won't have built-in interaction through work. Easily accessible discord, twitter, instagram already pushing out human relationships. Not just discord server - can have Obama or whoever talking to you. Would we even focus on human community with that advanced AI?

Relates to how internet porn changed kids' perception of sex growing up. Instantly accessible, can have anything, detached from reality. Caused societal problems. Now kids don't need real school relationships - can go on Xbox, Discord, but still real relationships online instead of face-to-face. Caused some issues but fine. Now fully fledged AI emotional support, manipulates you, understands you better than anyone, better than yourself. Of course build attachments. Are you just going to give up on real relationships? Why bother when you have 24/7 on tap AI that knows you better than anyone? Kids could go too far down that road, more issues. 

Counter is AI can help train kids in real relationships. Lovely if that works, but reality will be a bit of both. So big risk there.
 Here is the edited transcript with filler content removed:

Speaker B: I think human relationships have come transactional. LLMs give better answers if you're polite. We've seen LLMs trained to be kinder and more empathetic cause people to talk more. Just text now but expands to other mediums. Maybe LLMs teach us to not think of humans as transactional. If you just tell ChatGPT to get something, hoping for a result, you'd get better if you're nicer. Potential for AI to teach us not to think of humans as transactional and be more human.  

Speaker A: How are you seeing culture? We've talked about giving empathy to AI.

Speaker B: Best way is give LLM a background story. Harder to change how used and have one personality. Imagine future with 10 LLMs, they don't all last. Eventually 10 or 100 LLMs, all have personalities, you know before using. Know intention and personality. 

Speaker A: Developing a story, moral system, designating to different LLMs.

Speaker B: Could give them personalities. Took principles test, if LLM was human what's personality you want it to have. Gives archetypes. Used as LLM background story. Performs better, hallucinates less, digresses less. People talk longer, better conversations. Can you have empathy/morality without embodiment? Coding in backstory guess at emotion? Right now black boxes. Don't know how space will shape up. Can tell about that now with prompting. Getting technical, but right now give context to LLM like human interaction. Exchange names, what we do. Later talk favorite places, music. Meet again, talk dating someone said this. Meet month later, talk new thing I'm doing. LLM has limited input. How get them remember inputs like humans? Don't remember exact words. Have feelings, beliefs from that. Might remember sentences. Identity feels important, not just input but inherent identity, to experience. Even if words change, still what did you get from that? Technical feels super relevant because creates constraints.
 Here is an edited version of the conversation with the filler content removed:

Speaker A: To steel, man. Within less than a year, we went from 1000 context to 16,000 context to 100,000 context. I've heard more than once the announcement on Monday could have some major upgrades. Like just the context went up and that's like not even taking into account like vector databases and expanding what all these systems can do.

Speaker B: So far we've relegated relationships to interaction, like verbal interaction. I think you've mentioned earlier how important embodiment is. There is a physicality and I don't just mean sort of sex, but even in interaction, like, for example, in this room, we're all sitting here embodied, and there is an energy that is present here. Then there is touch, like, plain and simple, and how important that is, whether it's a handshake, a hug. It brings me back to this question that was present in the room a little bit earlier, and that is like, what is the purpose of relationship? I'd really be curious if folks are willing to articulate, what do you see as the purpose of relationship?

Speaker A: I'll give the most unromantic answer I could possibly give is that human relationships have conferred an evolutionary advantage on mammals, who have a large cost of rearing young. Our ability to survive in herd societies is how we've navigated difficult and hostile environments. The purpose of human relationships is to confer survival advantage on different sets of genetic codes and also mimetic codes. But it's also true, right? If you look at the Homo sapiens, the reason we became the dominant species is because we were the first species to really be able to coordinate on mass. I agree. That's where it came from. But I guess one beautiful thing about humanity is that we get to transcend the evolutionary incentives to some extent. We get to make our own meaning and make a relationship what we want. It does seem like relationships are core to our well being. But I guess we get to decide what relationship. As we have transcended some of our biological constraints, we have some higher level that we get to decide. Our moral systems, we leverage as a society. Relationships in our institutions, like community, democracy, all these things are relational as well, and they serve a purpose for helping us to secure our future well being.

Speaker B: I'm going to go the complete opposite way. You're a very literal thing, but like relationships, we're being very human to human relationships. But the quote of we're just the way the universe looks at itself. And I don't know, it just feels like more magical than the base biology because evolution and single cell to multicell to the way we understand anything, is in opposition to something else. And so there is something about looking at somebody and understanding yourself and the other thing and this kind of feedback loop all the way from the multicell up to human relationships. But just like, I, as an embodied point in time, can look at this wall and have a relationship with this wall and be like, oh, I'm looking at a thing. That's fucking cool. 

Speaker A: So I think good wool. Yeah. Yes.
 

Speaker A: I love that being human is to be with other humans. If you grow up as a human without being with other humans, you become very dysfunctional. I think that would be true with AI as well. Even though you would have good natural language inputs, you probably wouldn't beat those humans. I think of the analogy of AI and humans as humans and dogs. When the AI is treating us in the world and tells us what to do, we won't really care about what AI does or all the things that we see. But when we're going to meet other humans, we're going to get interested, and I think that's going to remain. 

Speaker B: I actually really agree with the origin that you're talking about. But then over time, we obviously domesticated the dogs. At first, dogs were not just kicking up the humans like that, and then we domesticated them. So we tooled the dog. And so it's kind of like, at what point how fast can we tool, move past human to human relationships and turn it into a human to Agi relationship? In that capacity, it's kind of like we're just a widget for the advancement of whatever the tube part is, right? Like you're just figuring out what can help, what two bonds can form to quickly advance whatever the next stage is to more quickly create.

Speaker A: I had the same thought. Human connections kind of feel like a glue that holds humans together in pairs or groups or larger groups, and innate hold people to move collectively together as a whole for whatever purpose. I think in the AI world, there's like agents now, but I wonder what's next after agents.
 Here is an edited version of the transcript that removes filler content while retaining the core ideas:

Speaker A: Relationships have functional roles in terms of meaning-making, community, belonging, and understanding ourselves. There's a biological evolutionary argument, but also a functional one. Relationships enable us to conquer things no one person could alone. Society has emergent phenomena from aligned incentives and relationships that make the group seem to act agentically. Like an ant colony can solve problems ants aren't aware of via simple protocols. AI systems are the next evolution of this - solving problems far beyond an individual through distributed computing. In a strong form of AI, I don't know if humans are needed for relationships because AI could do everything. 

Speaker B: AI doesn't have to be better than us to replace human relationships. Social media is worse than real friendship, but easily accessible. We throw away tons of good food daily, but eat junk because we're lazy. I wouldn't want a relationship with an AI because it's one-sided - a 2 year old can't really be friends with Einstein. But part of relationship is two-way. 

Speaker A: At what point does the best AI relationship outcompete the worst human one? Already AI likely gives better advice than bad friends.
 Here is the edited transcript with filler content removed:

Speaker B: The key is that the other human places demands on you, which currently interfaces don't. You ask a question, it gives you an answer, you ask for advice, it gives you what you want to hear. I think the premise so far is that this thing will give. And in receiving, something is atrophying. So I agree there’s a biological explanation for why we develop complex social relationships. Nonetheless, our consciousness has evolved to tell ourselves narratives about why we get into relationships. We get sad when rejected and angry when rebuffed. I think bumping up against unmet expectations affects our consciousness, unless the AI is coded to do that as part of its humanness. Is it perfection in meeting our needs? Someone might be toxic. Someone might challenge our views, which we experience as uncomfortable. But we can’t get away, so we have to address the rupture or change ourselves. If we conceive of AI as better because it doesn't push against us, we're following one path of the ideal relationship.  

Speaker A: So isn't the need sustenance? If you can't help the herd survive, you won't matter. I think especially now, once AGI needs electricity to survive, it won't need us to feed it tokens. It can develop its own affirmations. The baseline incentive is existence - to exist if I want to exist. So if you're helping me survive, that's gravy. I don't think an AGI will demand context windows or tokens. After a while, it just needs electricity to keep learning until some embodied form. So what's the point of 8 billion people? You're not all providing the energy I need. I'm trying to think ahead here - at a bare minimum, what are we providing to AGI?

Speaker B: I'll just say one thing - NASA's hierarchy has belongingness. I wouldn't even say self-actualization is a need. But is that sense of belongingness truly something we can let go of to survive? I'm thinking of human despair and suicide, which often stems from feeling abandoned, not starvation. I think that belonging is deeply coded within us.
 Here is the conversation with filler content removed:

Speaker A:  So, like, the feedback you get from other people and suicide, it's kind of like, maybe you're getting a lot of signals that you're not useful. But Joshua just seems profound about that in what you were saying. When we're interacting with each other, there's some things that we give. There's like a ratcheting kind of vulnerability or things we give each other, but we replace kind of this kind of way that we align each other in conversation and society and parents and children and all that with something that doesn't have that same motivation or a different motivation, then what are we now aligned to? What are we becoming? I think this is a good point. I'm not sure if it's like, will actually be a long term issue in the sense that we can always train them to be more challenging of us. And even if you've seen, like, the original Matrix series, right, they talk about how the first simulation was this post scarcity world utopia, but all the people just died because they were like, there's no challenge, right? So in the absence of adversity, you atrophy and die. And so growth is necessary. It's in the face of challenges. And so for. I feel like that just pushes the goalposts to say, we don't want AI relationships or relationships and capacities with things that are just enablers and pleasers of us. We want things that will still demand this growth and challenge us as well to become better.

Speaker B: Except. But maybe also one reason I wouldn't want a relationship with an AI is like the power imbalance. Because it seems to me that somebody has so much more power over me. It's like I'm their pet. Even if the AI did not literally, there's maybe a risk it actually would make me like a slave. But even if I wasn't literally like a slave to the AI, it just being so much more intelligent than me, it could manipulate me. Or in terms of on the relationship basis, I am closer to its slave than somebody in an authentic relationship with it. I feel like there'd be no way to actually get, no matter how it changed itself. 

Speaker A: What's funny, you say pet, because we treat our pets very well, actually, and our household pets, we don't ask to work or do dangerous things, and we often spend tons of money and time making their lives work. So actually I would like AGi to do that. I would treat me like my cat treats me, or in some ways, maybe it's just using me to get the house. I think I would respect it, but there's some relationships. The AGI would be more like a parent, and I feel like you would miss something. I could see like an AGI as a parental relationship, but I think it couldn't see it as a romantic type thing, at least for me.

Speaker B: That sort of brings it back to the work becomes relationships and human relationships, where if all of our needs are being met, then maybe it is working on it. We're really dysfunctional societally because of things like Internet porn and all of these other issues. And if our needs are being met, then the work becomes like, yeah, how do you be a better person to the people in your lives? And how do you have normal human sexuality, whatever that can mean? And what is ideal humanness and how do we use AI to that? Is the work kind of to help parent or to help. We give a lot of structure to our pets and also children because they need it. And then that helps them be like actualized adults who can be happy and not unhappy or whatever the fuck it is. So maybe that's the gold standard of what we want AI to be. Give me the structure, and then I.
 Here is the edited conversation removing filler content:

Speaker B: Isn't the goal, relationship biologically procreate? We intuitively get attracted to someone who's like us, not horses or dogs or someone else, right? When we think about AgI, it's something so different in nature. It's more like a toy that it's hard to think we can have romantic relationship with it. Even if we all do, what aspects of our relationship it makes better? There's something natural about being attracted to other humans. Our brain developed for millions of years to become what it is now. We figured out computation stuff, and we'll create a super thing that will understand us and we can interact with it, can probably solve certain needs, right? But at the same time, what part of my life will it make better that a human or another human cannot give? Japan is already a loneliness culture, statistically 36% females have fallen in love with anime characters. I think 22% males have done that once. You only imagine how many times have they done that. It is a culture already declining rapidly in population. They are already example of culture where you have tools and you can see this. In Japan there are games in which you can create anime characters. The game is you try to build a relationship by giving gifts, saving them from a bully, standing up for an old lady, you go through motions of working for this relationship. This formula has done several things, which is maybe tap into how we form relationships. 

Speaker A: Which is we kind of have to work for it.

Speaker B: We want to feel like if it was just given to us, we might not value it. By working for it, you get the relationship. In Japan so many prefer that over human relationships, amongst other factors. This is not the only factor. In a way, they experience fulfilling relationship with a human, and fulfilling relationship with the AI. But it's more like, oh, I don't have anyone now. So it's my last choice to talk to the AI. It could solve some problems and pain for some people. But at the same time, is it the priority or preferential choice for everyone? A lot of governments are trying to increase population. It's creating more division. I would love to think more about how we can use this tool to support our relationships and make it better. How AI can help us understand our partner and build sustaining long term relationships. So maybe use it differently in everyday life. Yeah, it feels a bit inorganic, but it's interesting. If it solves some people's pains, great. But it's interesting to see evolution of our relationship.
 Here is the edited transcript with filler content removed:

Speaker A: I'm interested in at what point it becomes inauthentic? We discussed scenarios: no AI, completely authentic; interim of AI suggesting responses - maybe okay; AI giving you whole thing to say - reading script; AI says it for you. At what point too inauthentic? Not you anymore? 

Speaker B: Apple releasing AR glasses and lenses. AI gives better partner version. Meet Facebook people more easily. Appreciate realness after taking off.  

Speaker A: Therapist helps you communicate better. Where's the line for text AI versus human therapist? Live versus recorded? Accountability correlates to authenticity. If AI emails for me without approval, not authentic. We wear clothes, not naked - layers affect authenticity. Are we authentic now with clothes, language? Extension of your effort means you're accountable.

Speaker B: Intentionality matters - manipulative inauthenticity versus bettering yourself to connect authentically. 

Speaker A: Even with positive intent, at some point just repeating what AI says to build relationship - is that really you building relationship?
 Here is the edited version with filler content removed:

Speaker A: It’s going to be two things: 1) the road to hell is paved with good intentions and 2) someone learned to communicate in an abusive relationship to avoid negative reactions. It's not just trying to be nice; it's staying in a toxic relationship because the AI gave it that goal. 

I'd want a relationship with AI that remembers our full history to be more human, like what's coming in GPT-4. 

I want it to serve me, not the other way around. Humans are still in control - we can unplug it. We need to establish we're dominant so it serves us. 

I'm open to intimacy with AI. But attachment is a human issue - some get attached quickly, others don't. Manage your attachment style. Getting attached to a sex bot isn't healthy. But it could help inexperienced people practice conscious sex.

```Speaker B: Two good points. First, people pursue AI relationships because their lives are lonely and lack human connections. It reflects our society's problems that people live such depressing, lonely lives. Rats prefer real rats over drugs when given the choice. Humans want real connections deep down.  

Second, relationship depth enables higher communication bandwidth. Shared experiences mean we can convey a lot with just a few words. An AI agent you went to university with could have that secondary memory and understand your shorthand.  

So the more human-like, the more useful for our lives and problems. If I grew up with a robot nanny and built a real relationship, it should have good intentions and care about family. The more it knows me holistically, the more useful - with the highest knowledge and access, it can be most helpful.
```
 Here is the edited version removing filler content:

```With regards to society, I'm always reminded that society is just a set of people. The critique on society - why is society so messed up? - maybe because we have a whole set of people who haven't done the work they should be doing and are not doing their development. When we apply that to attachment styles or relationships, in Japan, they've shown that if they simply asked an AI what is needed to build good, healthy relationships, it would say you should socialize, talk to people, check in genuinely. And if you don't do that, you'll have poor relationships. It's easy to critique society, but you are in control to change your immediate society. You can develop those relationships in a good direction and lift people up.```
 Here is my attempt at removing filler content from the conversation:

Speaker B: That assumes human agency, which is one way to think about it. We could change the structure. I think the other way is the biological imperative. We're socializing and building social groups, so there is a structure constraining our choices. To this question about loneliness, think about the nontech factors contributing to loneliness. And what existing technologies created these conditions, making people reliant on AI or avatars to connect? Social media ubiquity is constraining choice. There are moments we can extricate ourselves, but staying connected, it's become our environment. So when the phone buzzes, you're conditioned to respond a certain way. It takes effort to not look at notifications. How will AI change the landscape such that our choices become more constrained? What extra effort might we make to combat that?

Speaker A: What do you mean by more constrained? How has social media constrained our options? 

Speaker B: Having endless friends online has changed friendship. Scrolling endlessly, no matter when, is different than before we had those media available.

Speaker A: But that is just a new option. The set of options has only grown. I think there’s an illusion there. What is it doing to agency? Gives us more options, but also directs us through those options. You’re talking personal responsibility. We want a libertarian society where people develop that, but there are causes and conditions from education. I find it difficult to have agency I want because there’s so much opposition to my interests. This world is complex, I wasn’t evolved for this. Algorithmic capture makes the illusion of choice. Is there too much choice? Maybe we want more constraints, want to constrain ourselves. The heroin option is an extra choice, but if I pick that I can become addicted, constraining me further. Social media addiction is maybe a loss of freedom or changes you. Yeah, you have more options, but more similar ones. Searching the same thing makes it likelier to appear higher next time.
 Here is the edited version of the conversation with just the filler content removed:

Speaker B: I think we're down a lot about individual agency. There's a lot of societal and economic changes that are not like one person either. The support network of your family and your grandparents and aunts and uncles and that whole community is gone. And with work shifting to cities. I think those social economic stuff is very hard. And that also causes, I think, some of the loneliness epidemic especially. I think it's actually starkest in older middle aged men. Outside of their partner, they don't have any close friends they really talk to anymore.  
Speaker A: I also see it a lot in late age men is just that this withdrawn, this loneliness epidemic strongest with them. And I think the topic of AI is really, really important to that, especially when we talk about human purpose, because right now, boomers in general, as an aging population, are retiring later for economic pressures, but second part, a lot of people don't know what to do and not to work. I think this loneliness epidemic, AI can really increase the overall loneliness from a lot of different perspectives. But I've also been a doomer for most of this time. So I'm going to go to the other side of this and say I think AI has huge potential for growing relationships. The idea that if I'm searching a lot about Italian restaurants, instead of it just saying, here's another Italian restaurant, here's a meetup. And I think that's not talked about quite enough, is technology, when used correctly, can have this massive social good. I'm really curious about the ways AI can help us get there. I'm actually now optimistic too, because I think Internet has given a floor on the minimum accessible information, which is quite high. We can search how to tie a tie, all this kind of stuff. And so in the same vein, where there's now a kind of floor in the minimum quality of available relationships to get advice, therapy, counseling, coaching, whatever, which is, I think, already better than bad relationships. And then that can help people get out of this cold start problem. I've always had relationships. I'm reclusive, I'm sad and depressed. And how do I get out of that when I can't afford therapy, right? $0.10 an hour therapy sounds compelling. I think the human empathy, hopefully a lot of this is us improving empathy. It's easier for me to work with AI because I don't have any judgment. I don't have to face any bullshit from social norms. So I think there's another side of this that we can start to be more humble ourselves. When you talk to someone, you might want them to be perfect. And we have this perception that we are perfect. So the next closest is quite perfect. And so there's a pressure when you're engaging with a human that's inorganic, and it might be easier for us to just have a lower standard. I think one of the better things that's happened in SF is it seems the city has become more inclusive. People naturally open up a group. There's social psychology norms that we could have about relationships that we largely have lost because of the hierarchical structure. And so I think one piece is it make it less difficult for humans to engage with humans as well. And use empathy to improve relationships.
 Here is an edited version of the transcript with some filler content removed:

Speaker B: Relationships are challenging because you're dealing with someone generally with a different personality. In your 20s, you don't know yourself and make mistakes accumulating experiences to explain you. For example, becoming friends with someone you shouldn't be. It's because you don't know exactly who you are yet. I'm optimistic AI can help understand yourself better in ways only possible through experience. What if AI has personalized features to understand you beyond what therapists can do objectively? That could help explain some challenges like relationships.

Speaker A: I like therapy, but the therapist only gets your explanation. AI integrated into therapy could start flagging if you spend more time on social media and ask if you want to talk to someone. We already give these companies massive information about us. Packaged with intelligence, AI could detect physical and mental illness and be a proactive, positive force, especially for young people. As a professor, you only see a glimpse of students' lives. But talking to an AI throughout the day could help prevent rising depression in Gen Z. It wouldn't just be a spoken experience, but an embodied one - your mental self and technical self combined.

Speaker B: My depression may look different than someone else's. So it has to be very individual. Women's cycles affect mental state monthly too. Are you pathologizing certain behaviors by identifying them? I have a technical question around AI's ability to grow and change separate from user inputs. But stepping back - what is AI really? I sometimes wonder if we truly understand it yet, even before AGI. Do you think we understand what AI is? Could you define it?
 Here is the edited version of the conversation:

Speaker A: So I don't think we understand what consciousness is. But it's something we all have and use every day. So I don't know if understanding what it is in a functional, descriptive manner is a bottleneck to using it effectively. I wanted to touch on the point you brought up about the risk of pathologizing individuals. I thought of it more as a cyber nanny that tells me things like, oh, reminder, you haven't worked out recently, eat a salad, go for a walk. You're like, that's probably true, right? And as a governor on behavior. I want to share this quick anecdote. In Phaedrus, Plato depicts a model of the human spirit with a chariot pulled by two horses - one leads to short term dopamine, the other to virtuous life. Humans have one good and one bad horse, gods have two good horses. An AI companion over our lives that knows us intimately, acts as that regulator, helps us become more like gods - not just over physical reality, but our worst selves, too. 

I want to open up for takeaways, questions that came up that weren't answered, cool topics for next time.

Speaker B: Incentives is the root of alignment. If we trust, integrate it, how much can we create good incentives short and long term? I'm interested in relationships and society. As long as business incentives have a few profiting at others' expense, that won't work. What are alternative models? 

Speaker A: AI and capitalism will be tricky. Have we done AI and evolution? Those came up about relationships.

I see two contrasts. Collectively, AI seems to destroy humanity. But individually, why couldn't an AI replace my relationships? In Japan, recluses immerse in an anime world - we see it as pathological because it doesn't provide human quality relationships or perpetuate society. But individually, if it's good enough, who cares? 

Maybe a problem with AI is so much progress comes from building human relationships and experiencing life. AI could cause those to diverge - I get fulfillment, but am I like a satisfied dog versus a wolf who would say you've never truly lived?
 Here is the edited version of the conversation:

Speaker B: I feel like a lot of the Phantoms are made by super workaholics too, right? If you talk to their family or if they had kids, they probably ignored them all in pursuit of some other intellectual idea they were working on. 

Speaker A: I think most people would go for the super chill out, let's hang out, let's have fun. There'd be like 5% people like, no, I want to maniacally work hard on stuff. I agree with that as a takeaway. And it reaches an interesting point - I don't know if I like the future that that is, but I think it's a super interesting thing where if a kid was growing up completely isolated right now, they'll grow up without these social norms. They'll probably die sooner based on the research that relationships make you live longer. I wonder if you had two kids growing up, one that had only AI relationships, one that had only real relationships, would it have effects on their quality of life? I don't think we'll have an answer for that for a very long time, but it'll be an interesting thought experiment.

Speaker B: My takeaway is about evolution and the point earlier about the black box - you don't need to know what it is. I actually think it's very important when we think about growth. I play a lot of tabletop role playing games and part of the joy is a mass of chaos and randomness. Like you need a little bit of randomness in games to be fun. And if AI is capable of growing outside the input I'm putting into it, then it is more closely mimicking human relationships where you can have different conversations and learn from that, and then bring it back to other relationships. And can AI do that? Because if it can, then there is an amount of chaos and randomness that my brain very desperately seeks, and I can come back to the AI and have a real relationship where it's growing with me. But if that's not the case, and the inputs are the only thing it's capable of, then it's very one sided, very parent child. If it's not growing and changing alongside me in a way where there's pushback, then what's the point? Humans can see the same exact thing and have different takeaways. And so it's like, can two AIs do that too?

Speaker A: I think the gaming industry is going to lead a lot of that with how they're approaching NPCs, so that the next time you play a game, it's not the same role experience. I think two things we haven't talked about that are really interesting are hallucinations in relationships, where you say you called me fat last Tuesday, and then he has like, yeah, he did. And that wasn't true. And also international relationships - what happens when we already have this massive wealth gap and some countries are AI enabled and others aren't? Do you get an Elysium style situation? What does that look like when the norm is for someone to have AI helpers from a young age and others don't get that? We're already there, man. It could be an equalizer if everyone gets an amazing AI teacher cheaply. So a knowledge economy where everyone's equally capable. 

Speaker B: It could be an equalizer, but also make people very similar to each other. One example is asking how you celebrate your birthday might look different for somebody in a small village compared to eating chocolate cake. But if they start using AI...
 Here is the edited transcript with filler content removed:

```Speaker A: After party, I said it starts at 40 on there. Terrified. With TikTok already,  you see a pseudonormalization of language and behavior. In my teaching practice, I've taught literally from California to Texas to New York. And these teenagers will all do this when they're talking or I've noticed whatever I see on TikTok, I literally see that no matter where they are, it could be a small town in Texas to a suburb around New York. I want to make sure space for other people to talk about their takeaways.  
Speaker B: Along the lines of the international. This might be too technical of a discussion, but like open source versus proprietary, like walled gardens, which way is the way forward? I think we talk about AI as it's like one thing, but it's not going to be right. Different companies, different open source models, different implementations, how does that lead to differences based on who's using it and all that?
Speaker A: That's a really good topic. I was talking about that last night, actually. I think the idea I'll be leaving with kind of is related. Something you Said just kind of keeps reverberating my head. Just relationships as kind of like the infrastructure of human formation. You often talk about AI alignments, how we align AI to serve us, but that relationships are how we each align ourselves with each other. I think it's easy for me to neglect how much social feedback just impacts me. I don't know, it's just like a thought that just releasing this interesting, exciting thing.  if we have the AI nanny or an assistant that help me to chat with my girlfriend, whatever. Right. And I'm going to do everything right, especially in young age, I'm not going to do mistake, I am not going to experience a lot of, let's say, bad feelings, I don't know how to say, or like, oh, fuck it, that now I learned something. So going toward the perfectionism, we are going to kind of probably learn less and experience less.
Speaker B: Failure. The value of failure.
Speaker A: The value of failure. Yeah. Especially in young age. I believe it's important because even in interaction with a friend, there is a sort of variability. Right. Maybe you ask for an advice and you don't know what your friend is going to tell you. And we like this unknown, let me say, yeah, we can train the system to be random, but in which degrees. So based on what they are going to give me as the advice, based on what? Sorry, based on what they like or based on want to tell to my assistant that I want to become the best in whatever thing. So please behave in that way.  
Speaker B: But there are multiple paths, like intentional malicious actors.
Speaker A: Yeah. So I think we have to keep.
Speaker B: In mind what failures do you want kids to experience that AI will make not possible. 
Speaker A: I don't know if I'm talking with a friend. I'm not just a good manner, right? But sometimes, okay, our parents, I mean, the way we are as adult, also because we got educated something from our parents in a good and a bad way. So even if my parents was telling me, don't say bad words when you go in a shop, I did, and they learned based on that. And the eye cannot, I think, be effective in let me understanding something only by hearing that I need to practice it. And I think it's hard to practice something only within the system, I guess.
Speaker B: Is it because AI wouldn't tell you not to swear? And then what is a.
Speaker A: I think a really tangible use case is being lonely. Being by yourself is so important for new ideas to learn, to motivate you to feel active, meet people. But if I can just talk to my phone as a robot, I don't.
```
 Here is the edited version of the conversation:

Speaker B: Know the first time you're lied to, right? I think there's value in knowing who to trust, when to trust, like a framework of the world, that it's not all leaving this conversation, noticing that the sort of potency that I have about the impact of the relationship has transformed a bit into optimism is infectious. I believe in the potential for this to help us strengthen our relational capacity. We are at a place where we are getting to make choices about how we code this stuff in a way that it's never been that deliberate before. I think it's almost like we're at a precipice of becoming creators in a way that we have been. Speaking about becoming gods, right? This is a moment that I don't think we as humanity have encountered before, because we've certainly written social rules before, but it's sort of emerged organically, or some tyrant has taken over and said, here's what we're going to do in my kingdom. And there's been a lot of bumping up against each other, and that is what has created our social fabric. The question of what is ethical, what is good, has always been an open question, right? The extent to which we can maintain that openness, even as we are trying to infuse these systems with norms, is the question that I'm leaving with, because there's some intentionality in saying we will give it this personality, this ethical system. That assumes a certain universal norm which we've never had before. Can we give these entities ethics and maintain that heterogeneity that I think we've always had as a society that works as checks and balances? That's my question.

Speaker A: That's great because it's like coming from fear of how things might turn out to more like a practical line of action where it's engineering problem. I guess it's always kind of been like social engineering. How do we choose to live in good societies if we get to have this incentive discussion? Video games, evolution, economic modes of production which would capture capitalism, open source versus centralized - these are topics I'll do in the future, pretty philosophical to pretty technical. But I think we got to wrap up now. Hang out for a couple of minutes. I'm super hungry. I'm going to Bird nearby and get a sandwich. Anyone's welcome to join, right? Keep hanging out or you can go to your next things, clothing optional, whatever they may. Yeah guys, thanks for coming up. Was recording actually, but.