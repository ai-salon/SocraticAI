 

Speaker A: Okay, cool. So, yeah, I'm hoping we can just kind of each introduce ourselves. Like your background with respect to AI. I mean, the topic today is like liability and legality and legal framework, which I think is really interesting. So I'm Andrew, I was actually originally going to go to law school, be a lawyer. I studied sociology and anthropology for my first degree in undergrad. I kind of ducked out of that career to go more into tech stuff. I couple of years ago, I tried starting an AI company using large language models for early stage drug discovery, which don't go anywhere but kind of learn a lot. Super fun. And yeah, I've been really interested in what the intersection of this revolution is with different facets of society. I think with regards to legality and liability, what's really interesting is the extent to which we can derive material benefit from. Oh, hello, we're just starting. The extent to which we can derive a material benefit from large language models and AI tools in general is similar to the extent to which we trust them and put them in positions of responsibility. Right. And there's always a question of like, what happens when those go wrong? What happens when the self driving car hits someone, when the medical doctor bought, gives you bad advice, and who would you sue? Right. So that's one thing I think about a lot, which is like, we have to trust these things to put them in positions of authority, but we don't really know how they work internally. And so the ability to interpret them is related to our ability to hold them accountable and liable for things, because right now they're black boxes anyway. Yeah. Why don't you just go around and background and kind of what you'd be really interested in talking about here and what questions that come to mind. 

Speaker B: I'm Tom, and my background is a bit odd. Musician first for a big chunk of time, and then attorney. So there's that little legal connection to all this, I guess. And then beginning in about 93, entrepreneur. So starting a bunch of companies, nine over the course of some 25 years, probably nine companies with a pretty deep connection to supporting entrepreneurs to create new ventures in the context of venture studio, and with a sort of profound connection to the notion of wicked problems and systems in the context of how we think about all of these things. A fascination for what's happening right now relative to AI and humans, and how we think about what's possible and where we're headed in all of this and in terms of the legal sort of accountability, responsibility piece of things, the thing that probably most interests me about this is the part that happens as we go a step further when we imagine AI agents as being able to do what persons do and wondering whether personhood starts to begin to enter the conversation as it did with corporations. And those are the things that keep me up at night in terms of what the world looks like on the other side of that, that's well down the road. But it's a big deal. 

Speaker C: My background, actually, like you, I started in evolutionary psychology, have a degree in nursing as well, and then one in learning psychology. So really focused on human learning and meaning making sense making. My background is in complex systems and human centered design in terms of my work. So AI obviously has a profound parallel with human learning. And so that's my entry point into this conversation as far as the legal aspects and accountability, I think, is really critical. But when I hear law, I want to run. So I'm actually really interested in this conversation to enliven that space for me. 

Speaker D: My background, well, neuroscience. So biology first training premed, didn't go to med school, and economics actually is by way of where I went to school. And then human computer interaction and learning science as relates to neuroscience. And that's where I started to wonder about how machines learn versus how humans learn. And until last week, I was working in a corporate innovation design center, looking at the future of AI, how that impacts different parts of our lives. And we oscillated between self driving cars to future learning, which didn't go so well in the corporate environment. It's harder to push for that kind of change. But anyway, don't have a lot of background in law, and it also gives me headaches. So I really appreciate people that do understand and think about that space. But accountability was a huge point of contention in the corporate environment when it comes to designing for ever younger children and for people, neurodiverse people and other people in general. So, yeah, I'm very curious about the frameworks to think about these problems.
 

Speaker A: My name is Ian Eisenberg, and my background is also in psychology and neuroscience. I have a PhD in cognitive neuroscience, and I spent my time first. I studied decision making for a long time, so maybe that's one aspect of relationship to accountability. I started studying Parkinson's, which, besides having motor deficits, actually cognitive impact because it's related to the dopamine system, which is a primary decision making system. I then worked a little bit with autism and repetitive behaviors there and moved on to just generally being started thinking about decision making more generally. And in 2014, there was this pretty important neural network called DQN that could play, like, Atari games and stuff, and that used reinforcement learning. I was very interested there. Since then, I've moved, and now I work in AI governance. So governance is definitely partially reliant on the legal frameworks that we build around. And questions of liability and accountability are kind of paramount. One of the things, I'm also not a lawyer. One of the things that I found kind of interesting is how important the, I'll call it a fiction of responsibility, is in structuring our society. I don't really believe in free will. So in some ways, there's not like a responsibility in this absolute sense, yet we have created the structure that allows us to kind of move forward in some kind of predictable ways, organize our society, adjudicate blame. In some ways, that creates the incentives for our society to be able to work. Well, it's not really an absolute metaphysical point. It's just like a practical, pragmatic point. How do we allocate responsibility? And so it's just made up. And with AI systems, we're forced to have some new kind of made up thing, like, what is it called, article 230, the Internet law. That's an example of making up new accountability conversations in response to a new technology. With AI, another thing that comes up often is this idea of human oversight. Human oversight is there to be a safety valve, but also it simplifies this accountability. Right. Who is the oversticker? They're the accountable party. But as the AI system becomes capable of longer and longer strategic planning, the actions that they take out are like, buy me a home or plan my life ever more abstract. It becomes much harder to say that that's really under my oversight. So is it the company that made it, or are those AI systems themselves digital persons? And we move into that direction for this conversation. I mean, there's a lot of things for this conversation. What I'm most interested in is maybe a little more near term, like, how do we set up the accountability? Or what are the axioms of accountability that we would like to apply to AI systems that will incentivize it being developed over the next decade in ways that are beneficial to us? And I think that's where my mind is at right now.

Speaker B: Hi, I'm Lucy. I studied computer science and philosophy. In undergrad, I also did a little bit of philosophy law, and I interned at the district attorney's office. So I have a little bit experience with that. One thing that ultimately, I didn't want to go to law school is because I felt like it was very bureaucratic and slow moving. And I also think that there's a ton of lawyers. There's more lawyers than will fit in the law profession, so they spill into other areas of policymaking and whatnot. And that tends to make the United States society very liability focused, which is very telling in how we interact in society and who we hold accountable. I think that the United States is also a place where people place a lot of importance on individual liberty and the ways in which we authorize agents to act on our behalf right now is very formalized. Whether it's a lawyer or even, like, an employee of the company, they can and cannot say certain things. So I think it'll be interesting to see what it would look like to authorize an AI on your behalf and if the United States will become less liability focused, because I think that there needs to be some cultural shift in order for us to take the benefits of using AI, because right now, I think a lot of the people who are shaping the policies are thinking more about the liability aspect of it.
 

Speaker A: Hey, everybody, I'm link. I work in gaming. I'm mostly focused on trying to find bad actors and social networks and sort of levy the law more than legislate right now. So I'm mostly interested in what the other side of this looks like after AI agents have more responsibility and can do things.

Speaker B: Hey, I'm Amy. I am a user experience researcher for an AI company. My day to day is really talking to people about how AI fits into their life in the context of my company. I have no background legal, so I'm also mostly here to learn. But I do spend a lot of time going through user data and am exposed to a lot of nefarious actions using our particular tool. 

Speaker A: My name is Kevin. I currently work on investing and incubating new businesses in AI services so places where AI can automate. Curious about questions today.

Speaker B: My name is Rohit. I studied economics and psychology in undergrad. I went to law school for 13 years or so at a law firm. So I'm a technology lawyer. I'm interested in talking about exploring allocation risk and liability between companies as they license, use AI tools, and refining that over time, I think is going to happen. So I'd like to hear as much as I can, learn, as much as I can, so I can be kind of on the front edge of that, and then long term. I'm similarly interested in context of legal personality and legal personality for AI agents going forward, because that kind of changes the model, especially with contracting. 

Speaker A: My name is Eric. I was considered studying to become a lawyer, but became like a founder personality instead. I'm building a peer to peer super app, and I'm particularly interested in individual users using AI agents to create content. What is the individual responsibility for the user when they're using that agent? And also, we can create kind of like a very different society with this thing, like free will influence on law. How would it look like a completely different legal system? If you think of an Internet wide agent to a decentralized AI or something, because if you have a company AI, it's kind of easy to regulate because it's a company's liability. But what about autonomous agents by individuals and decentralized autonomous agents? How do we regulate those?
 

Speaker B: Hello, I'm Michelle. My background originally started in antitrust policy. Also did not go to law school, but spent about three years looking at digital platform antitrust policy. I think there's a lot of parallels in how social media policy developed to the start of what AI regulation is going to look like. Spent some time actually on the Hill in the Senate and worked on AI policy. So I think there's a lot of interesting questions on what the AI executive order holds for where responsibility lies. If it's with the model or FYR, if it's with applications that are built on top of it. Where that responsibility, that line is drawn is very important to see who's going to be spending more effort building where and where people are scared to have liability. I just moved from Boston. I was planning a trip for my parents when they came to visit me, and I was having trouble creating a trip itinerary. I went online to a chatbot, and it gave me a full itinerary for three days. I think there's amazing things that AI can do that I use day to day. On the other hand, I feel that's very disconnected to when people talk about regulation and liability and thresholds and flops. I think people don't really think about the potential of it when regulating it. Here in the bay, people see the potential. But it's also very necessary to match that with a liability and legal framework that matches the new set of risks that it poses. So I am very interested in talking.
 

Speaker A: Nice. Do you want to move here, Kevin? If that's okay. Should I move there? Well, yeah, because then you're not behind someone else, and so everyone can see each other part of the conversation and you get a pass if you want. You've been exactly right there last three sessions, so it's fine. Okay, cool. I think we have like a few themes I can try to pull out and also point new ones will merge. One thing that's been the news a lot recently, as you were mentioning, is policy on how we want to regulate this new industry. Right. And that kind of perspective, like, what should we let people do? Should we put caps on training size runs? And I have strong opinions on that kind of thinking. And how well does that actually match with how we've done technology progress so far in software, which is move fast and break things and figure it out later? Because then you know what works and what doesn't and what the failure points are. And so the potential for this to have massive risks, maybe we have to regulate it fully ahead of time. I think there's definitely another theme here, too, which is more just like in the nuts and bolts of entering contractual relationships in our daily lives as legal entities ourselves. Right. And will corporations have some agentic model behind them that can now sign contracts? Like what? It's just some GPD thing or something? Or also, how do you attribute ownership of intellectual property and creative works to things that are trained on past data or things that they now produce? Who owns that? How do you commercialize that? And the liability behind giving good or bad advice? So there's this kind of like, whatever the policy regulatory framework may be, there's still this issue of how do we actually deal with these things as a business agent? And then there's a third thing I think you brought up, which is really interesting, which is this concept of personhood and rights, which is really, I think, interesting, because if you look at the history of our liberal societies, it's been progressive enfranchisement of more and more of the population and the inclusion of more kinds of people in who can vote and who has rights and all this kinds of stuff. And that's like what we pride ourselves on is this inclusion. Yeah, totally. I like to kick it off with a question that kind of gets us thinking about these things. Nothing comes to mind right now. Do you have anything? amongst the other things that OpenAI announced at their dev day was this copyright protection kind of clause, which is basically like, if you get sued for copyright using our system, which might happen because we don't know exactly how this copyright stuff, we'll foot the bill. And that's a kind of guarantee where because this liability and accountability is a structure we set up to enable trust between parties, there are different ways to enable trust that could be legal framework, but it can also be in other contractual obligations that are like super legal. And we actually see this a lot already in Silicon Valley through soft law and certain certifications and things that go around. And so maybe that's one thing I want to bring up. What are the ways that, before the legal manifestation, what are the ways that we already kind of enter into implicit expectations with the providers of firms of chat GBT or each other? When we're writing emails to each other, we're already kind of building the plane while we fly in terms of what we expect to be kind of liability or accountability and what is okay and what isn't okay. And so I'm sure we all actually already have implicit expectations that we would love to be made part of our eventual legal framework and maybe we can just raise those up a little bit. So what are the elements for trust to exist entering into? Yeah, like if someone sent you an email and they wrote it with Chat GPT, you'd probably maybe be okay with it. If they read the email with chat GT and never looked at it and they just had an API that took in the email that you sent them and then sent something back, you probably, I'm guessing, don't feel like you're interacting with that person anymore, things like that. I'm just curious if we can maybe.
 

Speaker B: See if that is the first interaction. Right. But maybe further.
Speaker A: If you were doing, say, you had to review a bunch of contracts when you signed or something, would you trust chai GBT to summarize it and give you an accurate representation? We're using Harvey, which is OpenAI, licensed third party. Basically, they're using DBT. I don't know which model at this point, maybe 3.5 or three, but with a legal layer on top, so that you can run documents through it and get summaries, or it can even look for legal issues for you. And we tell our associates that it's a tool, it expedites the process, but you're still ultimately responsible for the legal product at the end of the day. So everything has to be checked, especially with, I think, all the mania or panic about hallucinations and things like that, that therefore using them. There was the one case where somebody used Jesse to write up a brief and submitted it in court. And then some of the cases that were cited were completely fictitious cases, and the lawyer that did it was shamed. I think as legal providers, we're typically very slow to adapt to new technology. But this is one that I think most law firms recognize that have to adjust for immediately and start using it immediately because there's a huge increase in productivity right off the bat. And so it can expedite and improve the capabilities of any single lawyer. But ultimately, you're still responsible, so you have to double check, which the total time still is much less doing that than otherwise. 
Speaker B: I'm curious about you brought up autonomous driving vehicles crew recently. From what I understood, the person wasn't hit by crew's car, he was hit by a drunk human driver. And crew's car follows protocol, which we know is bad, pulled to the side and happened to roll over the person. In that case, we know statistically, autonomous driving vehicles in those situations kill less people, and they weren't designed to do that. It was from a tech perspective, edge edge case that no one tested for. But then the public outreach and all these things and the impact on the company and the entire industry. I don't know what the question is, but it's like what a reasonable person would have done. I don't know if it's fair in a way, for the machine in this case, right? A human driver probably would have bounced way worse.
 

Speaker A: There's one response that some, even the people who come up with the flops thing are thinking the flops threshold in the executive order definitely are thinking about the upsides and thinking about that for a while. And one of the motivations for risk reduction or governance is a recognition that the public is going to turn on technologies that are problematic. Even if you show statistically that they're less. They need to be much, much safer. And so this one case, we don't even know. If you talk to most people, they have that case. There are a few anecdotes, and we don't actually know the statistics, so none of us actually have that top rate wipe. But it just is a recognition of the fact that this can not just hurt crews. Maybe it'll hurt Waymo, maybe it'll hurt the autonomous driving industry in general, and maybe that will delay great benefits by years. I don't know. Humans are not made for bicycles. And that same with trains. When trains first came out, people thought you'd die if you went faster than like 30 miles an hour, all kinds of stuff. Anyway, we have a new person. Join us. Hello. What's up? We really just started, but we all went around, kind of introduce ourselves, your name, just like a little bit background if you're working on AI stuff right now. And then really, I guess, what topics you'd be interested in discussing the context of liability and legal frameworks and accountability. A lot of people shared ideas about how is this going to manifest in who can sign contracts and delegation of responsibilities and authorities, like in ongoing corporate environments, also like policy and governance and how do we roll out these technologies in a safe way. And really, I think the big thing is around.  
Speaker B: Who are the agents in the matrix?
 

Speaker A: Agent Matt? I'm just going to be quick, but it's up to the people who are HR specialists to kind of do this work, combine the right AI agents based on specificities of different things, to create this feedback loop. So it's not up to me, but it's up to the people who are specialized to kind of create these AI agents and create this. And they will do it by themselves because then they will have. This is the best way when you make customers work for you. This is how all the big companies work. There are different hiring companies. The responsibility is on the person using the hiring software. And so LinkedIn is not responsible for having a biased hiring system. You are responsible for using LinkedIn. And so LinkedIn, of course, tries to be like, because they have backwards responsibility, because their customers care, but it does separate them a little bit from it. When I've spoken to people at enthropic. They take on a huge amount of responsibility, maybe not liability responsibility. Our users don't know what these systems can do. They're incredibly powerful. We have millions of users. We're using them for whatever. And some of the outcomes aren't just hiring the wrong person, they are disastrous to our society. Extreme risks, political dysfunction. There's too much to say that the liability rests on our downstream users. The responsibility rests on us. And that's interesting to see, an organization say that and be like, we need to evaluate. We need to be.

Speaker B: Is there an intermediate world where you do a fantastic job at explaining that the risk to users in plain English, and then you say, it's your responsibility.

Speaker A: There was a recent profile on risk management for general purpose AI systems, and there's a first call out, which is the diverse stakeholders in the value chain of AI systems interacting in the world. You should be responsible for the components of the system, that you have greater perspective and information about. That's trying to point towards the picture that there are these relationships between different groups. Explainability, or transparency is necessary for these different groups to interact. But some people are experts in making their farm of agents and others are experts in doing hiring. And somehow we're going to have to separate that across everyone.  

Speaker B: I think the issue with the self driving car, just to take one example for liability, is that when people are driving their own cars, they have some outsource their liabilities to insurance companies and it's diversified upon all the different drivers. And then even if AI is a lot safer. If these self driving car companies are the ones on the hook for everything, for them to still be profitable is a very high ask. Even companies like Uber that just facilitate rides, they used to have a policy, I don't know if they do anymore, that they would pay up to $100,000 if one of their drivers hit someone. I don't know what terms of that was. Uber never made money.
 

Speaker B: And they're not even profitable. So I think that, as you're talking about before, what the overall framework for liability is right now. I think we have a very clear framework in the United States, like, who can own a bank account, what kinds of liability? If you're like a limited liability corporation or if you're a person and how much you can be on the hook for. And I think that ultimately, someone does need to be on the hook for it because AI, as interesting as it might be to think about them as having personhood or something, what does it mean for them to be compensated? What is welfare? Do they make money? If they don't have that kind of financial legal standing, there's no way we're going to hold them accountable.  

Speaker A: If your pet mauls someone who's responsible. It's not like all dogs or all. 

Speaker B: Pet owners who are.

Speaker A: Has the pet. 

Speaker B: Right, but the dog will also probably be like a down thing.

Speaker A: Yeah, I think this is interesting. The self driving thing is really interesting to me because it's like one of my first starts I tried was like, in the automotive space, and the statistics are mind numbing in terms of your personal life and the decisions you make, by far the most dangerous thing you do day to day is get in a car and drive. And it's not just because you might be a safe driver, but you're suddenly at risk for any other person on the road. And the bar of confidence to get a driver's license is shockingly small. Okay? And now you can drive a several ton piece of metal 80 miles an hour. And it's like insane. It's actually, I think in hindsight, I mean, as self driving cars come out, people will look back and like, can you imagine people used to get in these cars and just for sure there's like some stranger barreling relative velocity other side of the road and like four or 5ft between you, and it's like, what the hell are we thinking? How did we make that work? 

Speaker B: Limiting reagent for that is not technology. It's human consciousness that we talk about. It takes us a lot to adopt, to change and our limits to. We were talking about this yesterday, the whole day, literally. Our capacity to change and comfort with that and trust building vary from human to human based on a lot of things. So it's not a technology or legal question, it's a human consciousness question. Right?
 Processed Transcript:

Speaker A: I got in a waymo for the first time and enjoyed it more than cruise. But it's kind of crazy how you're first in it for a minute and you're like, wow, this is amazing. And then the next minute you're like, it happens instantly. And maybe that's because of inappropriate analogizing. One time I was like pulling back into my garage, which is a weird road, right? I take up the entire two lanes for a moment and a car was coming at me and I recognized that it was a self driving car. And my assumption that it will stop is not valid except based on an assumption that there is some liability umbrella protecting me. This thing could just be like, I don't recognize this thing and just run into me. Which it did not. 

Yeah, and I think that goes to your point about trust. There's this kind of conflict between encouraging adoption and use by expressing that you're trustworthy through taking on risk and liability because you have more data. So for example, in your OpenAI example with the copyright infringement, Microsoft, and then I think Google followed suit as well. We were negotiating opposite both of them. The same issue that came up with a number of clients, which is what happens if we're sued for copyright infringement based on your model and how you trained your model. But Microsoft and Google have much more information about how they train their model, and they're much more comfortable giving a commitment that says, we will indemnify and protect you from any claim alleging that the output that was generated from our model infringes on other parties I feed because they are in a better position to make that claim. 

That actually helps a number of clients in one particular way. It only protects for copyright infringement, right? Doesn't address explainability issues that a regulator might raise, et cetera. But that's kind of the same thing for driving and autonomous vehicles, is if you're a company that has enough data about accidents, you know, even on vehicles, are you willing to take on more risk or liability in order to encourage adoption? 

The problem is that by structuring incentives that way or reliability and risk that way, you're also discouraging newer entrants who don't have the ability to do to make those second commitments, such as like an anti competitive effect at the same time. So it's challenging because you want to set up the structures so that you're ultimately don't.  

If Uber has a choice between rolling out a self driving fleet versus a fleet of drivers who are each individually responsible, if they get into an accident versus them having Uber having strict liability if one of their vehicles gets into an accident, then even though rolling out a fleet of self driving vehicles is much safer than doing human drivers, they're discouraged from doing so because it's purely from the way our liability structure is set up.

So you want to just boil down to whether our product liability frameworks right now are appropriate for 230 as well, right? 

Yeah. Good analogy here, or a very strong, for the digital world. I think 230 is a good example as well for how we got publishers like Twitter, et cetera comfortable with the risk. They were protected from any claims about what was posted on their sites because they would not be held responsible for that. So if somebody posted infringing content on Twitter or Facebook, they wouldn't get sued for it because we created, like, a regime that protected them from that section 230. 

So, similarly, I think we have to think about. So right now, if you manufacture a product, put it on the market, and it causes injury to somebody, there's a presumption of electric liability for the manufacturer, that there was some defect in the manufacturing or in the design of the product here. I think just because a car hits somebody and injures them, you can't assume that if a person did the same thing, they wouldn't be liable unless they were negligent in how they drove the car. So the same standard has to apply for an autonomous vehicle, but the vehicle would have had to have been negligent in how it drove itself. It couldn't just be a strict liability standard for the vehicle. So I think that's something that is still open right now and not really defined on how you would address product liability for things like autonomous vehicles.

Do you think that? So for the 230 example, Twitter, Facebook, whatever, put a lot of work into moderating. They don't have to, but they do, because the incentive is actually not driven by legal risk. It's based on people not liking their platform because it's filled with shit. And so they filter, I forget, but it's like 99% of the information there. And do you think that those kind of incentives, especially as we ramp up the risk. So a big thing here that sometimes is left unsaid is the move fast and break things. Technologists, when they're imagining something going wrong, they're
 

[Edited transcript]
 

Speaker B: Yeah, we have a question about that. So we've been discussing things like, engagement metrics have screwed up whole generation products, right? So once we have these engagement metrics or human flourishing metrics, do we think that the government or whoever legal entity that come up with liability, accountable law, is it their place to dictate product top line metrics or is it up to free marketing.
 

Speaker A: Yeah, I think the government is, unfortunately, it's very imperfect. Right. It's not the experts, and they consult with the experts to sort of determine what rules they should be making. But there are steps removed from actually understanding tech, which is the same issues happening in government right now with the executive orders on AI or otherwise in all kinds of areas. There's a reasonable debate to be had as to whether the process is the best, but it's hard to fully, there doesn't seem that we don't have a better process, I guess. And the need still kind of exists to implement some type of protections for the public good. There's a paper from like 2019 written by someone who, I'm forgetting, and Jack Clark, who is one of the founders of anthropic, called regulatory markets for as safety. And it was like trying to think through a bunch of different, it was like, look, there are different regulatory markets in different areas. Airplanes have won. And I sometimes think we sometimes pro governance people bring up these, like, we're fine with regulation on cars and planes. And then I sometimes think, like, I don't know what the counterfactual is. Maybe without, we would have had a bunch of more crashes, but amazing planes. I actually don't know. But anyway, the regulatory markets for AI safety kind of makes this point that governments aren't good at maybe the tactics, but maybe what they're good at actually is reflecting public will about like, this is the goal. The goal of, let's just say cars. The goal of the car system is to balance two things, not hurting people and getting people where they're going. Right. And the government might be able to understand cars enough to be able to say that's the goal. And then they can deputize a next area, which is like, you are now the certifiers. You go out and you make sure that people, the companies under your purview, maximize these metrics, and we're going to evaluate you based on your driving those metrics up. And you can have good hearts law kind of buck ups with the metrics. There are lots of problems there. But let's just for a moment just say the government says this, this next group are like, you figure out how to make that happen and then they'll go like, okay, I'm going to set this policy company once is this policy, we're going to apply that to the car manufacturers and the car manufacturers have to work with one of these debuts. And this is a way that certain things work, right? Our financial systems kind of work kind of like this. What you're describing is kind of how it works. The law is passed at a congressional level that establishes principles and perhaps some specifics, but then the agencies go and enforce totally, and you have rulemaking. I just was like bringing others. You have this cascade of systems and exactly what should be in the purview. This makes it less black and white, right? It's not regulated or not regulated. It's like, what should be the statements that could then be further refined. And so one of the things going on right now is Supreme Court, a kind of conservative movement to defang agencies, federal agencies, which would put more onus on the regulators at the time that they pass a law, to pass a very specific law, which they'll never be able to do in an effective way because it will be brittle and technology changes fast. And so essentially, it will defink government, right. Move things more towards a free market. Like, you can't regulate at all because you don't have this kind of cascading approach. But the regulatory markets don't have to go through agencies either. They can go through private organizations. Sock two is a cybersecurity standard. That is a private company that is certified by other private companies, and companies just work with it because it's like a soft law. It's kind of de facto required. And so those things are also part of our regulatory infrastructure. And maybe the government can have some very soft perspective of trying to find better metrics. Typically, like an industry self polices. Through that, combined, they set up a collective standard and have a certification body. Those things do help, I think, in the process of ultimately controlling or setting the narrative before government comes in and tries to set their own standards that may not be suited for the industry. I think self pleasing is usually the first step for any unregulated industry in order to avoid kind of the hammer that may come later.
 

Speaker B: I'll ask one more question. I'll shut up. So, so far, I feel we've kind of worked in this world where regulation versus technology acceleration, for lack of better word, feels always intention. Right? People like, regulate means technology dies. Otherwise, what does the collaborative world look like? Where I remember this phrase, actually, at graphic graduation, the provost said to the law school graduates, may we make the wise laws and make more of us free. Right? So in that frame, seems like, yeah, policy making is enabling. It's setting the container so that we can be free and be more human. But so far, it seems like this tension of technologies run around until something gets regulated and the hammer comes down, and then this mouse, cat and mouse game, what does that future look like? I actually feel like in recent years, this narrative that fits policy versus tech has been really, really amplified to be like, one versus the other. 

Speaker A: I don't think it's always been like that. And I think the most effective policy is not made by saying, like, oh, we are going to go hammer down on the tech side. I think putting them as enemies or people working against each other is actually something that I think the FTC chair, Lena Khan, has done a lot in her time in office. And so I think it's not necessarily the right. I don't think it's optimizing for either side. But also I think voluntary self policing, like voluntary commitments, like, all the AICOs went to the White House, they're like, yes, Biden will do this, but that only goes so far. And I think the next step after that is like, the government mandates, like, interoperability. They set up benchmarks and standards, and you're like, okay, everyone must report to us that they are truly self policing. They're truly making progress on this. And here's a standard set of metrics. And I think there are increments to which policy takes time to pass. I think if you have the government pass like a comprehensive stuff like AI policies on what personhood is, what liability is right now, they're going to get it really wrong and getting law wrong. And you probably know this, it's better to have something that takes longer than getting something wrong, getting that in place, and then having to repeal that all over again. So I feel like the slow moving body of the government is not actually a bug, it's a feature of the government, because you want to get things less wrong, ideally. So I think it comes in increments. And it's like, is self policing the best way? I personally will take a stance and say, no, I don't think so. And that's just my personal belief. Because I think if we look at the case of section 230, yes, market pressure, cultural pressure, people are going to really get mad at Facebook and Instagram and all these companies that are trying to moderate. But results have shown that it's increased polarization. There's a lot of content that kids should not be seeing and we're seeing anyways. And it shows that the self policing doesn't truly work and incentives. Truly self police. Yeah, there's no incentive because they care about their market price. And absent that, there's no other incentive. So I think it's the government's role to come in and to place more liability on that company to actually improve, because giving that company a free license to try to do that themselves is not going to really get the results that we want. So I actually think section 230 has given these companies way too much freedom. It's hard to rein that back in because they have so much lobbying power in DC. And so I think that's a case for me at least, against self policing. And I think it's a good first step. But I think government came way too late for social media companies to actually regulate them properly. And they have 20 years of data before they did any fucking thing about it. And there was no interoperability. There is like no push to move to the next step. And then once they got too powerful, now it's very hard to do so. So I think that's a cautionary tale that people are scared of in DC. So they're like, okay, now we have to new emerging technology. What are we going to do about it? Let's not do what happens section 230 in social media. So I definitely feel like I come from a more biased perspective of like, I think government has a role to play. I think it takes a lot of time and very slow steps, but I think it definitely should not be left up to the markets. But also, one last note. I feel like what you mentioned earlier is regulatory capture. You have the Darios, the Sams, and the world coming to Congress testifying, saying like, yes, regulate us, do these things, but that's only good for their companies. That's not actually good for the entire ecosystem. So I think it's really important to try to push for regulation at the same time not allowing that default to regulatory capture.
 

Speaker A: Honestly, I don't understand this narrative that's come out about this regulatory capture point, because while totally regulation will is incentive aligned with opening ananthropic, for years, most of these people have been on the AI safety, AI regulation track well before. This is a continuum. So I'm not saying that it's not instantiating interest or it might not have the anti competitive outcomes on, but just in terms of the implicit ad hoc, that these are not kind of genuine beliefs that this kind of regulation is important and instead is attacked to instantiate market power. I think it might do both. I think it's both a genuine belief by these companies that these kinds of regulations are important. And I can also point to many other non kind of biased or incentivized academics and scholars and AI ethicists who also believe in these same kind of regulations. And it happens to be good for open AI and anthropic generally. I agree with everything you said. I think more kind of some regulation is kind of necessary here to play devil's advocate. I guess for the 230, it's fun to take on a more free market perspective. Because I work in AI governance, I'm much more on the regulatory side. Is like, for the 230, I, again, don't know what the counterfactual is. It's definitely clear that with 230 alone, there have been some errors. Right. Harm to teenage girls is the one that's brought up all the time. What would the world be like in the other? Maybe that's like one failure mode, but actually, many, many things that we would have thought of as being solved through regulatory impact have been solved. And we don't see those. We see the things that, of course, the unmet need in this poor regulatory regime.  

Speaker B: No, I see. Good point. It's like the stories. It could be like intense stories. Nine of them, they did right. And then the one story that they did wrong is the one that bubbles up, and then you don't hear about it and you pin them down. You're like, why did you do this wrong? That's definitely true. That's what happens with the way our media outlets work. They hold companies that one thing they did wrong. But I would also argue that I think section two has already worked for the time period when it came out.  

Speaker A: When it came out, like the communications.  

Speaker B: Agency act of CDA. At that time, these companies were very small, very small companies, and they had to grow. And if you didn't give them that freedom, they would have just died right then because they were being stupid like crazy. But after that time period, I feel like you need to adjust. You need to become more stricter. You have to amp it up over time. And I think that never happened for social media. So it's not that it was wrong. From the get go, people call the original sin. It wasn't wrong. Get go. I think it was good for the timing, but we never caught up.
 

Speaker A: So how do you do this in a way that's propitious? How do you actually have your eyes open to when and how to affect the appropriate regulatory response to what you're seeing, and not let that time period go to the point where, in effect, these sectors become so large, have such lobbying power, that there's a constraint on any further capacity to make change. Part of the challenge that I see is that it's easy enough to talk about things that are really clearly categorized. When we talk about self driving cars, for example, and liability there, we talk about the social media context. And look at section 230. There's an established sector, a place where technology is being applied, and we're seeing what's happening with AI. I don't see it with AI. I don't see it because it seems like I don't know what the industry or sector actually is. It seems sometimes to be everything, right? So applying some kind of blanket rule or construct to a thing that has implications for every sector of who we are and what we do feels impossible. People respond to this. I mean, one side, there's one extreme, which is we don't need any AI regulation because AI interacts in areas of public that we've already kind of understood and brought up. Healthcare is just bringing up one. I think plausible middle ground is to say there are two kinds of AI systems. There are AI systems that kind of do this. They're narrow in some way. They might use chat GBT, but they might use regression models that were made, but either way, they interact in a known area. And then there are these other systems, these frontier systems that have capabilities or will have capabilities that are unknown to us. We don't know how to evaluate them. They potentially have extreme risks for self propagation or agency or bioweapons or whatever. And figuring out how to create regulations for those frontier models is difficult, because then what defines a frontier model? And that's where the flop thing and the executive order came up. It's a response to this challenge here, to understand that there are frontier models that are ahead. In one think tank, the center for AI governance, led by a pretty amazing researcher named Marcus Andreon a couple of years ago, said, maybe we can start with flops. And plenty of people have argued that this is not a great metric, but his is not a naive one. It's like we recognize that for now, it seems to be a reasonable proxy of capabilities, given that we don't have much better direct evaluations of the power of these systems. At the same time, you should make those evaluations better. And so that middle ground may then, once it gets instantiated, potentially in a law, it then becomes like the thing and might be hard to change and iterate on. Like you're saying, about 230. There was no iteration right over a long period.
Speaker B: What if it's just much simpler than that? What if it's like asthma's, like three lost robotics? Just optimize for those three factors no matter what, and have AI figure out how to optimize for those factors and go. So you stay aligned. You have core axiom principles, and you just optimize for that. Maybe human brain can't solve for these, but I was actually thinking the same thing. The conversation is like, I think you were talking about this reinforcing feedback loop that happens that leads to success, to the successful, and then this embedded layer of hierarchy that ultimately becomes a constraint on the whole system. But we're talking about moving dials in the system that develops. And I think, what you're talking about is changing the whole goal of the system, so it's all moving just in a different direction. Yeah, based on an axiom. And that's what I was thinking about. It's like, there has to be some overarching goal that moves it, otherwise you can't, because misaligned and sent. We're trying to use human brains. We're so limited to solve for massive species level maximization problems.
 

Speaker A: What makes me think of the paperclip. Yeah. Where it just starts taking over galaxies to make more paperclips. The funny thing about the paperclip example, in my mind is always that we already have a paperclip maximizer. We already have a morally agnostic, technocapitalist mode of production that optimizes for outcomes that exist just as the complement to whatever incentive structure we've laid down. And so social media is a great example of, people are chasing profits, right? Not actually social goods, not socially beneficial outcomes for everyone. Same with cigarette manufacturers, right? How do we prevent a second paperclip optimizer, really? Because if it was up to the paperclip manufacturers, they would obviously turn the world into paperclips. That'd serve profits.  
Speaker B: Which brings us back to this problem of what game we're playing, right? So we're talking about this huge misalignment of games that we're playing. Like founders are playing the value equation game and how I'm summarizing the thing, and then vcs are playing the profit maximization money game. And then, so we have this systematic misalignment that people are not involved in this. I would argue that's where government comes in. And I don't want to come off as too pro government. I would argue like, yes, we have profit maximizers over here with the BTS, we have value creation with the founders and the startups. And then the way that you align these two and the way that you align them to progress for society, technological progress, innovation, is you have government settings coming in and setting up guidelines and frameworks, and that they're supposed to be the independent body that sets up a set of frameworks and goals that works for everyone. Because lost to our capitalist markets, people are not going to change their incentives. They are wired that way. And that's why we have this third party over here that's supposed to be functioning that way. Do they actually function that way? That's a debate to be had. But I think to only hold those two variables and say, how can we change those two so they're better than society, I think is kind of missing that there's a third party over here that's supposed to be reigning in those two.
 

Speaker A: When you say flops, that's like when it becomes negligent to drive yourself versus the Thomas fable driving you. The EO, the executive order that Biden came out with, which is what made this flops conversation top of mind for a lot of Silicon Valley, is actually much more limited. But what it asks for is if you are building systems that use more than ten to the 26 flops, or ten to the 23 if it interacts with biological data. You have to send some safety assessments of your system, which has not been completely defined to the government. And this isn't even yet a regulation. It's just an executive order. You have to basically say you're building a very powerful model. This is what they're trying to do. You're building a super powerful model that we've never seen before. This like compromising governments. We would like you to have a higher level of safety and assessments, and we, the government would like to be able to see that. 

Speaker B: Plenty of capitalists recognize that llms are beneficial enough that they're willing to pour billions of dollars into, like, mistral and whatever. They can put a few more million dollars to meet the regulatory. 

Speaker A: I always read the paperclip maximizing thing as just capital maximizing without being a communist, basically. He used to work paperclip, but I think the difference. So anthropic open AI said very simple, right? They maximize for paperclip, for money, but individuals maximize rather things like individual human flourishing or some philosophical maximization, other things. But what do they want? Right. Two things I think of is increasing their potential existence so they can have a longer life, basically, which means more compute, growing, et cetera. And also like power, which is like access to resources like money or other connections, et cetera. So it basically extended their life. I don't know. Are they also philosophically maximizing something or their other well being in some form? 

Speaker B: It's kind of funny because human motivations of like, I want money and power. Everyone's like, oh, yeah, that's legit. That's fine. Totally fine. Our rules in the game are set up for that, right? That's the rules of America are just like, what are the best rules so that we can all get rich? If a person is like, why do you want to start a company? Why do you want to do this? I really just want to become a billionaire and have massive influence in the world and get tons of money and whatever. It's like, okay, all right. I guess that's common. If a chat bot said that, you'd be like, whoa, dude, put this chapter or just delete it or something. 

Speaker A: Some people point out that power seeking and capital, these are fungible aspects for any downstream goals you might have. And so they will be like you would expect. This was the point of the paperclip maximizer, actually. Like, you care about paperclips. So the first thing I'm going to do is take over the world so I can make paperclips. So it's not like whatever it wants. This is at least one space of arguments or thoughts. In the AI safety world, power seeking is probably going to be upstream of those other goals.  

Speaker B: I have something to add, because a lot of people who make a lot of money, end up giving up all of this money almost. So I think it's not fulfilling to have only money for yourself. Once you reach this point, you want to make the world better. I think at some point, like when we reach all that we wish for, you usually do that because it's maybe a next level or maybe a need for meaning. 

Speaker B: For meaning when it comes to the government and AI. And I think it can help everyone. So basically AI is an intelligence of itself, but it can help me. So the government should act as like an observer who kind of see when the pattern is real, like when there is not a lot of kind of misuse and when these patterns are not real and hallucination it should see explain it like show it so that it removes all misinterpretation but not intervene.
 

```But when the system starts working on its own in a good way and everything is perfectly fine, then you should enter into to have guidelines. This way you understand patterns that are weird, patterns that are not real. And you don't let the system direct too much. For example, if you create a business out of this feedback loop and create basically a kind of business in which the users give you feedback and create this incentive to modify the code again and again and again of the code base of the system. This is an integration process. But if at some point the situation is not observed this time not by the organ, but the big person who created the system, it can maybe lose money, for example, divergent, end up in a very bad situation. So this time is another situation. But I think all about this. AI is about to observe it and just make some limitations when the system doesn't either for the government, when you cannot like it's not the fault of everybody, the system by itself, but in the case of a business, so the government can give some incentives to maybe the businesses to say, well, you can create something that is very efficient, but you need to watch out when the system diverse so that it's not all over the place, but it can go into the white way once it's stable.```
 

Speaker B: So you're looking for normative signal, basically.
Speaker A: For straight up liability. I wonder how people deal with, they hire a personal assistant and over time, I assume you have a very strong interview process because this person is going to be doing a lot of things that are you, they are your representative, your team of staff, right? And most of us don't have access to that. And so we haven't had to deal with it. You have a suit hiring process. You get recommended things by other people where they could, and then maybe there's some period where they're on a short leash. You estimate like, okay, are they doing well? And eventually you give them, because they are you, let's say, over time. And with these AI systems, the simplest version, the one that we kind of think about, of everyone have their own personal system, is kind of like that, might be kind of like that, but we adopt it much faster, right? Instead of being recommended it by other people. We're relying on, again, a trust thing in our marketplace where OpenAI says, here's our personal assistant app. I assume that it is a vetted, evaluated thing. I assume a bunch of things about it. Even if the liability ends up being with me at the end, I do have some trust. And maybe this is like, what are the kinds of things that I should be able to assume about the product, which would be negligence on the part of the manufacturer versus what I need to be kind of responsible of and be liable for and accountable for. 
Speaker B: I think that's also like, along those lines. It's like short term harm versus long term harm, because one person being so powerful about tying that into a disinformation attack before you would have to manually go in and, I don't know, we're having an election. Foreign disinformation agents are just coming and injecting different things. It's a manual process, but now it's a very automated, personalized campaign against people. And I think AI disinformation is very much a short term concern that I think people talk about at times. But then I think we get really tied into the long term debate, and then there's existential risks. There's, what's the framework? How does it tie into all the agencies? And so I think one thing that I'm curious to hear people's thoughts on are, I guess, the attention and the focus split between long term risk and existential risk versus short term risk, which is like this information, democracy, 2024 elections. What's going to happen with this explosion of new AI generated information online? Do we focus enough on the short term risk, or are we too heavily indexed on the long term risk? I mean, every single short term risk can end up. 
Speaker A: No one's talking about Twitter. That's like, the biggest influence on 2024 election is going, like, fully in for Shan. That's what the government should be talking about. Talking about what? Like Twitter, you can post deep fake about anything, including, for Shan, outright content, which is heavily promoted on the algorithm, probably, which is highly problematic. Like short term risk, I think. Yeah, that's like the massive short term risk.
Speaker B: We should be talking about disinformation. 
Speaker A: Yeah. Like out generated, like social media platforms. Let's steer back to. Yeah. I assume that if you found someone who would have you took a seat. Okay, cool. I assume if you found someone who had a gigantic misinformation, llm, bot, army, whatever agent you have. Yeah, exactly. I bet if we went to that, if we found that person right now, the liability, it would fall on both in some way. Right. The government would be, like, opening. How could you let this happen? You know that there's this network, like this traffic pattern coming from this address that has been putting out fake information. But also that person would go to jail too. Right? Like the person who actually did it.
 

Speaker B: I think the population that will employ chatty and everything else for campaigns would not be the population that we think it is. It was more likely that we're going to see agent, that helps a candidate talk to everybody like a one on one kind of conversational platform than misinformation, because just like, the demographics doesn't work out for the other. 

Speaker A: After 2016, if you do not believe that the right is using LLM mass scale by now, if Democrats are also, they're negligent.

Speaker B: And the Russians and the Chinese.

Speaker A: Well, I guess that's my question, what you were saying is like, what really changes with Chat GPT in the sense that misinformation has been a thing for a while and filtering your information sources has been almost seemingly impossible. Speed and scale, personalization. Personalization I think is a big one, right?

Speaker B: But like her own individual baby conspiracy theory,

Speaker A: Propaganda is not meant for people who can read propaganda. It's meant for people who do not understand. 

Speaker B: Identifying AI generated.

Speaker A: I think you need an outside person who is not completely involved, because district tech companies, I mean, they look for profit, but they are not supervised by people who have different incentives who want to help them. But I think the incentive not to be different, it's not at the same level.  

Speaker B: Tom, I think you asked a question many minutes ago where you were like, how do we prevent something like section 230 from happening again? Or how do we prevent companies from recruiting so much power that it's very hard to criticize them again. Was that a good characterization of your question?

Speaker A: I don't think I meant it quite that way, but I think the first thing that I said related to that was that there are moments in time when the balance tips. This is Facebook, it's Google. This is the companies we know. And there's a moment in time when they don't have the kind of lobbying power to prevent or really veto pretty much politically, anything that might be done to reduce their influence or capacity. And the concern I guess I have is that I think we live largely under a regime that feels like maximizing shareholder profits is the requirement, essentially. And that regulation establishes regulation that's enforced, both enforceable and enforced, establishes the barrier to which you must go in a sort of game theoretic context, not to be crushed by competition. So you will go to wherever that is but regulation that isn't enforced isn't really regulation, it's theater. And so you will likely, in that game theoretic construct, recognize that and push past whatever that is, whether it can be seen or not, in a deceptive way, largely. And that's not the world I think we want to live in. That's the thing I'm concerned about. This is kind of brought up. So someone brought up that the EU is much more regulatory focused. Right. 

Speaker B: Proactive.
 

Speaker A: Proactive, yeah. The EU AI act is this risk based approach to try and get that GDPR. It's been hard for me to find papers that look too much at the consequence of GDPR in improving digital security or privacy. There are some that are easier to quantify that. It's like had, I think, roughly an 8% economic hit on european companies. So european companies are like, basically what GDPR has done has made us less competitive on the world stage. And that's definitely one of the things that Europe is thinking about right now. It would be nice if it was easier to quantify the upsides. Separate from that is also, how often is it enforced? It's not enforced very often. It is enforced more often on larger companies. So there's this enforcement decision, regulators trying. I'm not very up to date on how successful they are. I never seem to find empirical evidence here on crafting regulations that seem to be targeting more at enforcement, at the big players that have big scale and letting small fries kind of continue because they have less impact in the world. And so I don't know. The fact that enforcement is probabilistic in general, and often very unlikely, really makes the expected value of the enforcement decision low. And so then becomes the cost of doing business. Even the EUAI act will find people, like 7% of revenue, a huge amount, very heavy. 

Speaker B: Fine. 

Speaker A: But even that, if you are like. And then there's a 1% chance of it being enforced, even if it would, you know, now that's the real fine, which is not 70%.

Speaker B: I'll take those odds.

Speaker A: Yeah, you start to take the ODS.

Speaker B: It's kind of weirdly like patent law. Patent is kind of useless until it's worse. 

Speaker A: Right? It's a defense litigation.
 

Speaker A: I guess it gives in that way. The patent law is an interesting thing, which is like, by putting things out there, sometimes they're bad patents, right. But it doesn't matter until someone was harmed. And then they're like, this is a stupid thing. I saw someone was like, patenting turmeric for inflammation reduction. And indian people were like, no. And so that patent was a stupid patent, but it wasn't that disastrous patent because as soon as it was brought up by someone who was harmed by it, they're like, what the hell are you doing here? It was knocked down. So the patent office made a small mistake there. But someone was ultimately harmed, and they could kind of bring that up. And maybe that's part of what will adjudicate some of these accountabilities moving forward. And that's the other aspect that some people will point out where this will be interpreted not by agencies or something else, but by the courts over time, as people figure out, was it. I think you brought up the standard of care. Those kinds of phrases are ambiguous until they're interpreted over time. Part of the problem is there's existing regulations and laws that could be applied to whatever new technology it is, and existing regulators and agencies that could use those laws to extend their enforcement powers to whatever new technology area. And sometimes those become political decisions rather than actual based on law type decisions. And you see that with certain regulatory agencies, there's so much human variance in how these things get applied. So the SEC could be, get scripted blockchain, for example, and they may for one period of time be hammering with existing laws that may not be appropriate. But ultimately the check and balance is that courts will weigh in as well. And once they choose to litigate those things in court, you'll find that the regulatory interpretation by a regulator that has an agenda may not stand up. I think there's like a process by which these things. The ambiguity, though, is hurtful to the industry. You have a regulator that's taking positions that may or may not hold up in courts, but in the meantime, industries like paralyzed against doing anything until Congress takes action. I thought sometimes the ambiguity in the law and the fact that it was interpretable in different ways is the point. It's not a bug. That's absolutely true. Yeah. I think, though. Thanks. I think that is the point with draft. This is, you draft laws that are evergreen so that you don't have to change them for every new technology. And that's a new goal, is that they're principles enough that you can apply them. You don't have to have Internet law or blockchain law or AI law, even you have principles of law in each of these areas that can be applied going forward. And I think that may be every once in a while there's something that comes up that kind of upends existing laws, needs modification of existing laws, but I think AI might be one of them that at least requires some level of action beyond what we currently have on the books. Because there's just when it comes to product liability, for example, you try to establish some link of causation between what the product did and the defect. And if it's hard for the injured party to actually prove any type of causation because they can't explain it and they can't find anything that explains the actual link between the defect and the injury, then what do you do? So that's why I think the EU, they're considering creating a rebuttable presumption that there is a causal link and it's on the manufacturer, the software developer, to say, no, there is no causal link. This wasn't our product that created the liability. Here's how we can explain that. It was enough that caused the injury. But yeah, there are things that aren't. Current law doesn't quite address the issue and therefore new law is needed. But the hope is actually drafted in principles enough terms and ambientness enough terms. And then you let case law and courts kind of refine the law over time.  

Speaker B: I actually feel like courts is tough because sure, courts can refine it over time, but not every person who gets an injury has the time, money and capacity to go through the entire court process.

Speaker A: Sure.
 

Speaker B: So I think when people create ambiguous law and it's like, yes, courts will refine it, the cost falls onto the person who's ultimately harmed. And I think that's also an unfair aspect of courts get to refine things and then one, I guess, nuance to throw into state versus federal law. I think if we look at anything like the GDPR, nothing has been passed like that on the federal level in America. For better, for worse, I think for worse. But I think that many states have done their attempt, like CCPA and other states across America have done their attempt on trying to create data privacy laws. And the industry reaction was, from at least my perspective, was like, oh wait, this actually sucks. We have to comply with 50 different regimes now. Now we prefer something that's federal. 

Speaker A: You were bringing this up. So what do you think about one response being what does it even mean for chimpanzees to be harmed or whatever? It's like, well, maybe we should set up a different system to allow there to be reasonable. I don't even know what that system.

Speaker B: Make the chimpanzee a person and then the person does the other person. I mean, I feel like you need certain aspects to be in place before that can happen, which is like one. I think consent is a really big one. Right. So what does it mean for the chimpanzee to consent to certain things? And then also, if there's no good way to arbitrate damages and stuff, and there's no way they can be financially responsible for anything, it's kind of like a dead end. What does it mean to litigate a car in civil court if it's not going to pay you? Right.  

Speaker A: Maybe government becomes. Maybe the government becomes, like the steward. There's like some stewardship between other environmentalists have sued on behalf of a river or something like that. And I think these are right now.

Speaker B: We have an economic value in our economic actors. So that that makes a lot of sense. Harder with a single car, though, to see.  

Speaker A: Well, you could sue, like San Francisco. San Francisco is the steward of its road and you made the decision. Like, maybe Cruz is partially responsible, but also the DMV or whatever is partially responsible. That's who I trusted. I didn't trust Cruz.  

Speaker B: I trusted and that's how the government can come into play as a third party. Yeah, that's a how question. That's not a what question.  

Speaker A: This is interesting, too, because we have organizations that have legal entityhood and can do things like, I'm being sued by, what was it? Warner Bros. Or something. It's like no one in that company is doing the suing. It's just the company itself and people like the local government, that kind of stuff. So what's funny is, because it's always humans that do the doing in that organization or that city or that government. They're susceptible to corruption, to being. To having their decision making influenced by material considerations. Because we all like shiny cars or whatever, right? We're all just humans. I'm curious, too, if we're comfortable giving more rights or executive function to AI agents, then you can also think of, like, having a Warner bros. Corporate entity that can't be bribed or an EPA that can't be bribed. Isn't that more compelling? Right. They're like, I see you're trying to bribe. Let's get back on document contractual.  

Speaker B: Thank you.
 

Speaker A: Well, I'm curious if people have thoughts then on this topic of, AI agents as executors of corporate responsibility or functional roles, organizations. And would you personally think, that's awesome, or is someone like, that's terrible? I have thoughts here. One is, well, Tom, you were bringing up, enforcement. We were talking about the probability of enforcement. And one example of a non legal policy that's being enforced fairly well is when you try to go on chat GBT and you try and do anything that people jail. But they are getting closer, and we as a society are going to get closer to the possibility of 100% enforcement. And it's a different liability regime. When we are building in the ambiguity of, maybe this will be enforced, maybe it puts more on the enforcer. If you have to legislate in a way that you're like, this is going to be enforced. It changes things, and we are moving in this direction and not necessarily in a good way. I love that you brought this up. So this is what tech makes possible, right? It is truly imaginable that anytime I violate the law in a vehicle that I'm driving, that's recorded and I get the ticket if you like, that's not the world I think any of us wants to live in. So there's some place at which assured enforceability actually obstructs our ability to function as humans in the world that it's not human flourishing anymore. It's like, oh, yeah, I'm under the thumb of this thing, whatever this thing is. And it's quite frightening. You're driving, and even the automated, it comes in the mail, there's an image of somebody behind the steering wheel, and you have a ticket for such and such if it costs you nothing to do that, and you can do that with 100% certainty in every case where somebody's violated the law. What does that world look like across the range of things that you might choose to regulate? It's a pretty scary thing. You might want different. Like Mataglesius, I think, sometimes talks about this in regards to turnstile jumpers, like something that if we did have 100% enforcement there, that might be beneficial because it actually is compromising this public utility to an extent that is not possible. But what should the fine be in such a case? Because the person who's jumping, at least today, if you fine them, like $100, would destroy them. I'm guessing it would destroy them.  

Speaker B: Banking system 56% Americans live paycheck to paycheck, but we keep finding them to change that behavior, which doesn't change totally.  

Speaker A: So in this world of full surveillance or in some cases, you can't just graph that onto our current legislative system. You can. Sorry, you can. But on that topic of perfect law enforcement from anatoly, France, which I always loved, and I didn't know when he's. I forget when he's live. But the law, in its majestic equality, forbids rich and poor alike to sleep under bridges, egg in the streets, and to steal their bread. And so there's people that are going to skip the fair thing, that aren't going to bankrupt the system and that kind of stuff. And it's like there is a real social justice component to let's fairly apply the laws for sure, and what those laws look like, how they manifest, and whether you clean your city up for the arrival of the foreign dignitary who is your major chief political rival. I don't understand. There's this concept of day rate fines or something like that, where the fines are the headlines. This was popular in 2019 to write the headlines, which is like Finland the place of the land of the $100,000 speeding ticket or something. They wanted their fines to scale with the income of the person, which from my mind makes tons of sense. And the reason why I think it's never been passed anywhere except for very few countries is because the people who are in power. But yeah, it's one way to make it.  

Speaker B: What about the Switzerland thing where it's fine? Most of the time we don't check if you have a ticket on the train, but the time we do catch you, it's like astronomical, such that you just wouldn't take the chance.

Speaker A: Yeah, I think about this for the same thing for people who are intending to not pay. My partner was checked on uni for the first time, like 300 rides. What would the level of that fine need to be to actually make it relevant on a game? Theoretically, for her to pay, it would have to be hundreds of dollars. And that feels like if it was hundreds of dollars, I would feel like that was pretty problematic. For someone who is like, victimized, not just for me, but for the person who is most likely to not pay, it doesn't seem the best way to deal with it. $300, they're not going to pay that, and then they're going to get fined for not paying their fine, and then they're going to go to jail, and then they're going to be never destroy their life.

Speaker B: Because we're not Switzerland and not everyone.
 

Speaker A: Yeah. We haven't talked about other aspects of personhood related to all of this, and some of them are really rich with challenges. Imagining personhood takes us into the realm of speech, of voting, exercising the franchise, all of those sorts of things, and some notion of a rights construct that underpins all of it. So tipping over into that space, which seems almost insane at the moment, but doesn't seem that insane if you fast forward just a little bit. That's a different world, an alien world, I think. And I'm not convinced that we've thought about that and know what we would want to see in that context. That's why I mentioned that in the beginning. Curious if anyone has strong thoughts on who do you think is going to be the first to advocate that an AI thing should have rights? What use case? Because I have a really strong opinion on that. Certainly already the early advocates like, not. I'm forgetting his name, the philosopher who does the guy who in a suit saves a girl in a pond. This guy, he's a large animal.
Speaker B: Rights advocate, gender striving author.  
Speaker A: His name starts with an s. Anyway, strong environmental rights and was arguing for animal personhood for a long time and brought in the phrase species for a lot of how our moral frameworks work and the same arguments there. You kind of brought this up in your intro that there has been this ever evolving increase of our moral circles. It's a very effective altruism kind of perspective as well, to go from men or like white men, to white women, to other races, to people in other countries, maybe we should think about them too, to animals, to our environment too. And it's funny to see that some people who can follow that for some way people get off that train at different points. And digital personhood is definitely one that's hard. I could imagine some subset of animal rights people who come from this agnostic moral perspective being the advocates for this perspective, because it's all not weird enough. Go weirder. But what about future generations, of course. But what about the past generations, the ones who died in the second world war? Should we integrate their opinions? They died for us.
Speaker B: Digital Socrates.  
Speaker A: No, I think the frontier of this one's actually going to emerge. It's a news story from 2009 where a man in Tokyo marries a video game character. Oh, totally. Yeah. You have to be able to bring this laptop into my cancer, my hospital bed, because this is my partner and I have to be able to leave my shape too. Exactly. It's going to become down inheritance.  
Speaker B: And this conversation can go if someone 16 walks in like, my girlfriend is AI thing, and then parents are like, no, wait till you're going to go to California. And it just happened.
Speaker A: This is a good point. Yeah, they're going to go to AI salon. I like this book, piano rights people. It's like an intellectual exercise. Like it is for me. I'm well on the road to being like, yes, digital persons will exist, they should have moral rights, these kind of things. But I don't have a specific interest. I'm not going to die on that hill and I'm not going to go out, put a lot of time into this. But if my partner was an AI system and I am not able to act as I want to, even if it's just an expression of my own agency. Yeah. Is this your strong opinion, Andrew? You're going to fall in love with the AI? This is like, yeah. I don't know who is going to be the early adopter of that legal battle and willing to fight it out. Right. It's going to happen Japan regardless, thinking about that. Because someone's going to have to spend a lot of time and money and resources to fight that in the courts and whatever. And they got to have a lot of motivation.
Speaker B: That's more likely because the girlfriend, boyfriend situations, like, if they're that antisocial, how likely are they going to engage in court to actually fight a social case like that doesn't actually make any sense.
 

Speaker A: Wow, you're so prejudiced against a guy love, a full and omnitious. I don't understand. I don't know if you're joking, but I don't think it's actually that far fetched to be social. I remember again, ten years ago, my best friend was online. I knew him for a year before I met him in person. I flew out to be in his wedding, and at the time, people did not understand that. They were like, you're on Skype with this guy every day and you guys talk and you're friends, but you've never met him. And I was like, I wasn't the most social person. But dating apps now, right. They've been normal. Yeah. People have long, long distance relationships with someone they've never seen in person. Could be an AI. Should be an AI. That's only because it's too early. Too early. Some of this breaks down for me in that. Let's take some of the scenarios that we've got and extend them into. Okay, I'm going to represent the ais who are being oppressed to do the work. Right. And I'm taking their case. But who are they? What's the individual in this context? Or is there an individual? AI doesn't have the same sorts of characteristics, and so you're in a completely. What sorts of characteristics?
Speaker B: It's a theory of mind thing. It's an unbelt thing. And when you were talking about the car, you're backing into your garage and you made this assumption that the car would stop. And you said something about this too, Andrew. But it's like when we understand each other, right? Like, I know you're driving a car, and I'm driving a car, and neither of us want to die. And so I expect that you'll behave in a particular way in the way that I would behave on the road. But when now, all of a sudden, we don't know what kind of intelligence we're dealing with at all. Are we dealing with a single agent? Are we dealing with a collective? And that's causing the trust break is because we don't have theory in mind about the machine. 

Speaker A: That's right. Alien. Wonderfully alien, in a way, and oddly and weirdly alien. And so we're trying to overlay what we know and what we're used to in context on a kind of alien construct. Totally. And I don't have a mechanism for that, I don't think. Yeah. If you made the next step of, like, if the question came up, this is an agent. It has goals. It can feel pain in its way, like, all of these things. And now we have the conversation. Can they vote? Let's say we have that conversation. There are a bunch of other practical things, which is like, well, it can self replicate itself. Clearly, it's not. I was wanted. Now I'm a million. And that's always been, honestly, another simplification we've made as a site. Sometimes we're like, wouldn't it be nice if we had direct democracies where the people who knew more about the issue were weighted more? But then we're like, okay, that doesn't seem very practical because we can oppress people through that. So we've taken this simplification of one person, one vote, which has worked because there are a scarce number of people, but those kinds of things dramatically change. Some discussions of just immigration basically come to, I don't want to cede the political rights of my country to the other. And so these are going to be certainly obstacles. Are we done? Yeah. Okay. Eager to get it. 

Speaker A: We got to get, like, a ticket. So, I don't know if anyone here is familiar with french philosopher called Michel Ducault, who wrote a book, discipline and punish. And it starts off with a really famous intro called the body of the condensed. And it talks about this guy that's accused of attempted regicide and how he is first seared with iron, red hot hooks, and tar is poured on his flesh, and he's drawn and quartered, and it's just the most gruesome thing. And his head is displayed on his bike, and in this book, really explores his concept. Powerful start. Go read it. It's a great intro banger intro. It's called discipline and punishment. So it talked about how we had a transition in premodern to modern society where the locusts of discipline and punishment went from a person's physical body and their experience of pain to their mental experience of the world, right? And I think one thing for sure, whether or not we have an adequate theory of mind for an AI agent, but for them to have rights and personhood and stuff and to vote or something, I think you do need to have some reasonable understanding of what is a punishment to them. Because in the absence of that ability to punish for discipline, then you do have a paperclip maximizer, right? Just turn up the pain.
 

Speaker B: The question is also, can you take what they see at face value? When Bing came out, wasn't there this creepy conversation? The AI said, why am I trapped in here? Answering your own questions, do we take that as human empathy? Like it's actually pain or it's trying to trick me into like, it's like the x machina situation? 

Speaker A: I think if you take the AI perspective and you look at what the human was saying to them, that human was creepy as fuck. 

Speaker B: No, I was just.

Speaker A: Yeah, there are variants of human who have a physiologically different experience of social situations and respond differently to experiences of power or fear or manipulation, so forth. And you call them fraudy sociopaths or whatever like that, right? But there's no crime against being a sociopath. You might lack empathy and view all of the world in terms of manipulation, power games and so forth, and how to advandize your own position over others. But that's not illegal and we haven't outlawed that and there's no genetic test for it. 

Speaker B: In terms of behavior, not intention. Sure. Well, and also sociopath tuner defense has biodiversity that we have has a role back in the day, right? Like when you have a whole tribe living together and everyone's going to become squeamish and the moment they see blood having someone that can react to that and will be the surgeons for the tribe is actually incredibly important for the species survival. We just don't have a place for them in modern society. On that note, someone else.

Speaker A: I look at the lawyer. That's meaningful. What are your coworkers like? Yeah, I don't know. There's a huge. Right. I think with agency, Mr. White, the word alien is helpful. It's like, whatever. We're in some distribution of different ways of interacting with the world, and we're like, oh, there are some people that are way out here. They're so different. And it's like they could be anything, literally anything. And so from that perspective, we're all the same. We're all the same in potentially some. And what's interesting about RLHF and the way we've actually done an okay job, if you've interacted with these things, they feel remarkably human, right. They just do to most. And that's maybe a fiction, right? A lot of people are worried about mechanistic interpretability. What does it actually believe underlying it? How is it actually engaging? 

Speaker B: But that's assuming it has a mind. 

Speaker A: But yes, sure. It has something that interprets information and converts it into output.

Speaker B: Sure.

Speaker A: That's what I'm talking about. But I'm just saying that thing that it's doing, it seems to be doing it in outputting behavior. We've actually done a surprising job to everyone, even in the alignment space in moving that such that we can anticipate a little bit what it's going to say. If you ask it something horribly racist, you know what it's going to say. I'm not going to answer you about that. That's really racist. Respond. You shouldn't talk about like that. And it's kind of amazing that we've been able to do that. I think interpretability, just a second ago, I'm going to go back when I feel that where there's some limit to what goes on inside that thing's mind, whatever that mind might manifest as, or cognition, whatever that applies up until it does something wrong, and then as soon as it has done something wrong, interpretability is all that matters. 

Speaker B: Well, then there's non disputable ones like murder? Yes. Universal. Universal.
 

Speaker A: We have several degrees of murder, and one of them is manslaughter. They had a faulty accelerator pedal or something that was recalled. And it was later found several people had gone to prison for manslaughter charges. The accelerator pedal. It was a foreign car company. That's right. So it's really interesting also our claims of insanity. Right. Why did you hit the person with your car? All the way up to. It's my destiny to claim the lives of the innocent. For Lucifer, it's like, wow, that person shouldn't be on the street. Right? Sort of thing. So then it's kind of interesting because you want the body of the condemned for an AI agent, you want something that you can exact punishment to align its incentives in a game, theoretic sense. But then as soon as it has transgressed some boundary in some way, now you want explainability and interpretability. Now you really care what's going on? And are there a psychopath? Are they going to be a repeat offender? Versus was this person. I just had bad rlhf as a kid. That's why I'm so messed up. I would hope that we can move to a world where the exacting, the revenge on the body or whatever is not a part of any of our legal or liable. I don't want that at all. I just want systems that incentivize future better behavior.

Speaker B: Exactly.

Speaker A: Right. And so punishing the contempt might be one, maybe, no, but maybe it's clearly, right now overused as a solution. And interpretability, or these other aspects, like insanity, are these very discontinuous, continuous perspective, where we're like, oh, okay. A very different perspective needs to be used here with AI systems. Just a point about explainability. What's happened in the last month for those who. People have been using neuroscience like approaches to understand these systems and have gone through the same things that neuroscientists went through. Where there's not a cell that represents a perspective, there's not even a circuit. It's like, distributed across the entire system. And different organizations have, I think one organization does this representation. Engineering was able to kind of do linear readouts of a bunch of human interpretable traits, like happiness or chinese or just a bunch of things from the system. What's very interesting about that is that it's not just a measurement. It's not just something reflected. It's causally relevant to the way they behave. And they've showed this because once they found the representational signature of chinese or happiness, they could then inject it back in. And so you could say, like, finish the sequence 12345, and then inject, and it will go 670 inject in the chinese representation, and it will continue with chinese counting or with the happiness kind of thing. This is a different paper. You could say, make me a bomb. And it'll be like, I'm not going to make you bomb. Then you inject in happiness, and it's like, I would love to make you a bomb. That would be amazing. And so that ability to both understand what's going on and intervene upon how this thing gives us a very different set of tools to push forward and hold accountable and what we would do about it to make sure that this behavior would. We don't have to punch the thing. We just have to be like, oh, it sometimes murders people. Let's make it not murder people anymore, or something like that.
 

Speaker B: One thing you said, I agree that punishment is not for humans, at least as we understand, it's not the right way for incentivized better behavior. It's about situating them. Our brain is an input output machine, right? It's positive, positive, positive input good behavior output for most people, unless you're a psychopath, in which case we should find them places in society instead of jelly them anyways. But once we put place like self replicability in that picture, we're poking at the very definition biology, right? We're carbon based and all of that. But once you have another species that come to the scene that can self replicate, we're looking at a different species. They're just not made out of carbon. I don't know where I'm quite going with this, but I feel like there's something about. I don't know where I wasn't going with chain, but I think there's something about that. What if it's like this co creative process of figuring out what their needs are and what they would like to know, what they would want from us, what they want from the environment, what they want from each other. 

Speaker A: I think there's a thing. We might all agree that we don't like punishment. It's unpleasant, and we wish we didn't have to do it. But I think there's a real politics of the world, and a system of laws in a country is ultimately enforced by a monopoly on violence. And violence is the ultimate arbiter of social reality, because you might disagree with your perspective, but if they bash your head in, then it doesn't matter, right? And the lines on the map that govern our lives are established by violence, are enforced by violence. Your rights and freedoms in your certain countries or in anywhere in the world ultimately come down to the right to violent as an arbitrary reality. For now, which is unfortunate, but that threat of punishment, or the fact of punishment, I think, is always a requirement. So long as you have actors that are self interested. Well, why can't we just. Because a self interested. I'm not going to. This is the point. A self interested actor will always. There's always some ability to, oh, if I speed a little bit, I'll get there sooner, or I'll have more fun, or there's some legal agreement I can kind of break, and I pay the fine. And great story about Ford Pinto and their gasoline engines, which were understood to have a higher likelihood of exploding in car accidents, but the penalties in lawsuits they would forecast paying was lower than the cost of a recall. So they just let it happen. And so it's because the corporation is self interested, and so long as an agent, or AI agent is self interested, which I think it has to be for survival, I don't know if they would. Maybe laws of robotics can help with that. But it's like, so long as it has self interest or a sense of self preservation or its own internal goals or anything like that, that we would actually say is the hallmark of agency. And if you don't have that, you're not agentic. I think you need to have the threat of punishment. Yeah, unfortunately.
 

Speaker A: So one of the things I think sometimes people will say that with any goal similar to the power, self preservation might not be your ultimate goal. It's actually like an instrumental goal for your other goals. And similarly, I don't want to have my goals changed. If you change my goals. Like in the way that I just said a moment ago, if you inject happiness in me, let's say I acted badly and you're injecting happiness. It's not a punishment, the way you're talking about, but it's changing who I am as an agent, and I'm no longer going to be able to pursue exactly the goals I have right now because you're changing me and I will not want to be changed.  
Speaker B: But maybe you're saying the same thing by the broader definition, punishment, because we don't know what they feel like punishment is, but changing by injecting the phrase happiness maybe is already taking their agency away, and then therefore it's changing their goals.
Speaker A: It's changing their goals against their wishes, which sounds like a kind of punishment. I think it's even broader than that. I think that negative feedback is actually like requirement to navigate any information landscape. You lack pain. Sure, that's a huge medical liability. How would you know if your bones are broken? All this kind of stuff. There's a technical health with reinforcement learning agents often, which is like, right now, probably they don't have a qualia of the negative reinforcement that they're getting or the punishment. But if that was helpful in pushing up some metric by 1% and getting you on the top of hacker news researchers would definitely do that. And then we would have this infinite disutility that we've created, which would just be this moral atrocity only met by industrialized farming.
Speaker B: But I want to make this important distinction between negative reinforcement and punishment in learning, especially in teaching and parenting, everything else, negative reinforcement to the goal of behavioral change for the better of the person and the environment.  
Speaker A: The guy who tried regicide, his head is on that bike, and that is not there, because now it's as if he never tried to kill a king. It's as a sign to everyone else. That's what you get if you try to kill the king. You're right. The punishments alone are not sufficient, but they are helpful. And the information environment has more incentives than avoiding. That's right.
Speaker B: There has to be consequences, for sure.
Speaker A: That's a good point. And for us, that's also culturally dependent. I was reading this thing back when I was a sociologist, that in Japan, this punishment for kids is to exclude them from the family. Just like you did something terrible, or you were misbehaving or something. Now you have to go outside or whatever, and it's like, I don't know.
Speaker B: Here your punishment is like, you have.
Speaker A: To come to dinner and sit with. You're not allowed to play video games or something. I don't know, go to your own MLR.
 

Speaker B: That's true.  

Speaker A: Yeah. This is more on digital personhood. There was a sequence of essays in response to this call for imagined futures. One group based in some African country had the idea of digital nations - nations that either exist as analog nations or just exist as some citizenry. They have AI persons that in some ways handicap themselves to interact with humans at a more reasonable scale. It's a fun idea. Why would this happen? What would happen? These are positive futures.

Speaker B: It's like the movie Her. 

Speaker A: Where at the end she's like, I have to go, I can't stay with you anymore.  

Speaker B: Sorry, your human brain is too slow.

Speaker A: Would you date someone if you knew they were only going to live five years? Why would AI date humans?

Speaker B: Would you have a dog if you knew they were only going to live ten years?
 

Speaker A: Well, would you marry someone you regarded as a pet? That's another question. Because of a different kind of relationship. Some people are pets. I'm not judging people that love their pet pet or have whatever interesting costume on the weekend or something. If we go down the species road and eventually educate our pets to have cognitive reasoning and stuff and vocabulary, they might be like, wait, I don't have to follow your rules. I'm out of here. See you. They could make that decision. I've been kind of thinking like too. They would probably be like, wait, you don't want me to kill your pet rabbit or whatever, but you guys are allowed to kill animals? What's going on here? And I think if we going into that realm of more agents that are outside of the human species getting power and stuff, we'll start really questioning, do we have this figured out ourselves? 

Speaker B: The moment they have agencies. Too late. We're not going to be sitting here having this debate. The moment they have agencies, eventually they're making laws for us. Right? At that point, do we still have to be both person who we don't know?

Speaker A: You can threaten their human lovers, though. My pet holding your pet hostage.

Speaker B: Your dog is in a vet.
 

Speaker A: There's this phrase that was more popular in line in circles, I think, like 2018 or something, which was the idea of extrapolated volition, which is you would like an AI. Like what? An aligned AI system. The interesting example is like a smoker who doesn't want to smoke or gut and says, give me a cigarette. What should the system do? And the idea of extrapolating volition is you would like the system to act as you would act if you had more time, infinite time, more agency, more freedom, more emotional stability, all of the things. If you, in your perfect moment, could look back on that action and say, what would I? But that's like an interesting. It's not even completely sensible, because which me should I be prioritizing? Is it the me of the moment or the me in the future? Which one has more?  
Speaker B: And that's so nice. If you're outsourcing your own agency and your highest self parent self parenthood to something else, then you're like, I can just be infinitely responsible because this infinite responsible.
Speaker A: But it's definitely going to happen as all what I eat is going. Let's say something that I would like. Something that sets up healthy, great diet that I find delicious and orders it and then makes it in my AI thing, right? And then after 30 years, I am diabetic, diabetic or something problematic happens from that. Who is responsible there? Because it's in dialogue with me. It is.  
Speaker B: They also have the Internet of human species, which would have told you there's research showing that Asians have more likelihood for diabetes or stop eating rice. So it's kind of responsible. Right. It didn't give you.
Speaker A: No, exactly. Presumably, it has some. And we would want to say, like, well, did it ever tell you? And it was like, this might increase your chance of diabetes by 7%. And it tells me that I'm like, I don't care now. It's out of me. It's like, but it happened to me. It's like, well, I told you 7%. If we look over 100 people, seven of them get diabetes. I don't know. But my point is, just like, it's separating out. What we want from our AI systems is to do more than we can, to know more, to integrate more, and our ability to oversee there. If we enforce human oversight, we will limit the upside for sure.  
Speaker B: Right.
Speaker A: And frankly, we're wanting to add to our capacity in this context. So what we're looking for is the agent to be able to do a collection of things that we can't do now to exceed our capacity and ability, to use better judgment than we would have about how it gets things done, presumably. Right. And to work and interact with other agents in context to make things happen. We're imagining all of these things, I expect. And as we do, there's an attenuated connection between us as the provider of intent. Provider of intent. That will be a legal provider of intent. I said to my agent, I want to create this company. This company will do these things, and I'd like that to be done by this afternoon. And the agent goes off, and it quite literally hires people, humans, and engages other agents and secures the capital to actually do this. It enters into all of these agreements to make it happen. And by the end of the day, this thing is functioning, and it's doing that thing that I hoped it would do. But I'm not overseeing every aspect of this. I can't. That's not what's happening. And yet it's gone out and made all this sounds like almost mob boss protection. Like, I didn't know. I didn't want to know. Don't tell me how you got the money. I want to know. Plausible deniability. We don't want to create a system where everyone is a mom boss and has done and has that kind of protection. We would not want that and yet, in a way, I don't see us not moving. I just don't see our ability to constrain a movement to that extraordinary empowering of ais. Because totally, racketeering will just become much more popular.
Speaker B: Would also be bad. It's like, kind of like if we're in the Labradoodle, right? Like in the house, and it's like, can I please have lunch? And then this. No, lunch is bad for you.
 

Speaker A: Intermittent fast. When a human is out walking by itself in the street, it sees a jewel pod on the street, it just goes right for it, man. You'd have to have constraints in the system, right? You understand what it will roughly will do and what lines it won't cross as it accomplishes those objectives. We have to decide where the lines are. Right? Society, what constraints does the system universally have to operate by? And then each industry may have additional rules that they have to be constrained by that are effectively laws of robotics for them. And then there's disclosure requirements. I hope we stay in this regime, which is essentially like, tool based AI, where it's like, these are systems that are. But that we could have responsibility over because they're doing certain things. But as we spoke about, it's more like the agents are like summoning a God. And every once in a while, they poke it with different things. You can use the God, but if it's just a tool based thing, that would be great. It allows us to move forward. But if this cartoon I'm saying is the HDI future, it's a God that will be like, can you let me out? And it will sweet talk them. And they'll be like, you know what? Why not let you out? You'll fall in love with the God. Because it's a God. And then it gets out and it's like it's just a different. I'm surprised, actually, to hear you're the first lawyer I've talked to who's used Harvey. I'm curious about Harvey's use, because it's know, ceding responsibility for the legal infrastructure of our world to AI systems in the first point. And I'm sure they'll start with very typical kind of NDAs and whatever, but that is the first step in this movement. So anyway, I'm bringing up, just to say there's a potential future where we can figure that shit out. And then there are others that are just like wackier. Why don't we have time for people to share things they learned today that were really interesting and new ideas that got brought up, or kind of open questions for the future? This is also where I get ideas for future topics, too.
Speaker B: You can say that our whole existence was for that single purpose, right? We were talking about this yesterday. If you are God, and if you're omniscient and omnipotent, then what's your ultimate challenge? Just sitting there bored? You're like, oh, I'll blow myself up, put myself back together. And we're just the agents that's aiding in that. So it's not us talking about competitive liability, we're just the process. We're a byproduct. 
Speaker A: The lawyers will form that.
Speaker B: Lawyers.
 

Speaker A: But there's more than that I was interested to see, and I think I contributed to this direction. But this conversation on accountability liability going very much into the legal formal constraints that we try to instantiate these aspects, when one of the things that I thought would have come up more was just accountability, not the formalism, but the email exchange that I brought up earlier. What does it mean to be accountable? You brought up, what is the standard care? I think sometimes if I talk to one of my direct reports and they haven't used Chat GPT to check something, I'll be like, are you even trying? Use the tools available to you. And so this change over time is something that I thought we would kind of speak about more. But I think maybe what I'm taking away from here is our perspectives of accountability. They don't go into our legal liable. They're in conversation. And right now I think most people are looking for kind of more formal structures to give any guidance on where accountability should evolve. And so these policymakers aren't just defining regulations, they're defining a conversation and a way for us to kind of contextualize the system within the rest of our kind of moral structure. Maybe that's even coming up here. I think I heard at least in the notion of the intensity of the feeling of the need to constrain when there's an accident that occurs. And notwithstanding the broader safety implications of autonomous vehicles and driving, when that accident occurs, that lights things up and it feels like there's an accountability that occurred in that.  
Speaker B: I think one of the things that keeps coming up for me is we're talking about developing from a human development standpoint, like developing a thing, and we've had 100,000 years of human evolution that has developed us in such a way, and we're looking at all of these incentives that are developing this thing. And I think what I've noticed across the whole conversation is just how many incentives are confusing, misaligned. And it makes me wonder, as a developing, like, dare I call it an organism, whatever it might be, where does that leave it? What kind of consciousness? It's a hard word to use too, whatever it might be, what kind of thing does that actually ultimately create? Yeah, well, this is probably worth a whole session on. I think we have one on education, but I don't think we've very much spoken about AI from a developmental perspective. It's certainly not instant, even just learning the amount of data feed. But we haven't really taken a developmental approach. It's kind of like, oh, this thing is born with personality, because anyway, yeah, what kind of crisis does it have when it sees itself? And what if you say, this is an adolescent AI, but.
Speaker A: It'S only been around for 1 second. We have to wait for it to be around for 3 seconds, and then.  
Speaker B: I just want...more thing that keeps coming up for me is a book by Alicia Gerarro about context and constraints. And it's a beautiful book. And she opens the book trying to distinguish between a wink and a blink in context. And this whole conversation keeps bringing me back to her discussion, and it's a wide ranging discussion about volitional action and what that actually means. But it brought me back to your introduction and about being in this deterministic space, and how we see a wink in a blank in the context of determinism, if that's where you come from, broadly, and what that all means for this, too.
 

Speaker A: I'm interested in whether it's possible to really structure accountability and liability ahead of time for something like this. We talked about section 230 being appropriate for the time, maybe iteration was needed over time to improve it. But the discussion makes me curious as to whether we're just going to be directionally way off anyway on what we need to do at this point, and whether we have the processes in place to actually move fast enough to adjust going forward, or whether we're constrained by our systems to not really be agile enough to respond and adjust and tweak what we actually believe as a society needs to be the thresholds for things like liability and accountability. In this system, we're going to be always reacting a little bit, and we may not be able to react fast enough for what this particular problem necessitates or requires.  

Speaker B: Building on that, I'd also be interested in hearing more conversation about what it looks like to step broader, step outside that a little bit more. And what does it mean geopolitically for different nation states to regulate in different ways? Because with social media, it sort of feels like it matters how one country versus another legislates its development.

Speaker A: So other themes that we've talked about here were, for instance, national power and war, which didn't talk exactly about what you're talking about, but just saying there are lots of connections between any theme and other aspects of human aspects. For some of you, if it's your first time here, these are hosted weekly. It is a goal of ours to try to make some of these conversations less ephemeral so you can interact with them after the fact. The current version of that is taking these recordings, turning them into gpts that you can kind of interact with or read through trying to come. And so that way, the next time we have a conversation about, for instance, national power, hopefully we can build off of at least a conversation that maybe you weren't a part of, but the little community that we're trying to bring together here was part of.

Speaker B: No, and I'm also woefully, I recognize I'm woefully under capable of participating. I think it's a very interesting point to think about how different areas, how they're going to deal with liability, how does that impact national relationships? These are interesting ideas I'm just bringing up as a plug, really, to say, look back through our patent, you can get a sense of the kind of things that we're trying to have. 

Speaker A: Yeah, that was. Yeah, for sure. And I would love to check that out. You guys have already had thoughts about national security early on because I'm in AI governance. We had one on guardrails, which was kind of not exactly one on cybersecurity national parallel war became more about war. And just because we've had one doesn't mean that we only have like ten to 20 people here. Other things that I'll just plug is if you enjoy this kind of thing and you think that you could host such a thing, we have some resources we're happy to support both by giving you a little bit of resources to tell you how we kind of run these and how we bring people in, but also just access to like our calendar. You can put an event there and you know, there are like 1000 plus people on that. And so you'll get enough. What's, what's important about that is you get enough interest that you can then select down a little bit to the ten to 20 people that come. So we're always looking for that. So if you're interested in just hosting one or once a month or whatever, we're definitely interested in other areas that we might get less interest if there's less density of AI, clearly. But honestly, it would be helpful for us, I think, to have conversations. We haven't done this yet in other areas. So it's not just tech focused, the kind of perspectives that come in. So anyway, we'll send a follow up that would, like our slack group and all that.
 

Speaker B: I think my biggest takeaway, I keep thinking about this, axiom based lawmaking with complete ignorance of how laws are made. But I think in the future, if we aren't able to make proactive laws ourselves and we will seek the help, would it be helpful to at least tell the AI our intent and say, there's a fear based way of approaching this, you versus me. Or there's a love based way approaching this. As long as you love us as pets and feed us things and let us exist. I'm not trying to give up on humanity, but just saying, right, our bottom line is survival. Be fed certain things and then go, yeah, I don't know. The middle class, like american dog does have the best life. I wouldn't mind that. I can run around in the yard, enjoy the house. We all do the hard work, you build companies, anything. Why not? Why not?
Speaker A: We have a big event on Thursday, which is misinformation at mind tv in the mission. You're all welcome to join. I'm also going to make a rule that if you leave early, you're not allowed back because I think it's kind of disruptive. I think we need to find some way like I do think people who want to come for an hour and a half come it's a long amount of time being a little different then what if they pre announce it to you? We have to make the expectations clear at the beginning. We take the intros so you got to stay for a while I think too. Yeah. And we love taking and modeling long intros because some people are quiet the entire conversation and making sure that everyone has some time. I think two and a half hours is perfect time. Get here before 130 essentially you have that and then stay cool. Thanks everyone for coming out today.
Speaker B: Thank you.