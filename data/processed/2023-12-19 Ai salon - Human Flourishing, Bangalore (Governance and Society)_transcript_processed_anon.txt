Names have been changed to preserve anonymity.

 

Speaker A: Thanks for joining this group. The first thing I'd like to do. So we spent all that time just to get to this point, right. But now we're the group and so I'd like to go around. We can introduce ourselves. You're going to have to forgive me if I don't remember your names. Each name is new to me, but we'll go around and actually, I'm going to close the store.
Speaker C: It is.
Speaker D: That part is particular.
Speaker B: That part is. We can shift back.  
Speaker F: Maybe I'll just shift.
Speaker E: I'm.
Speaker A: We're getting closer. There's a vent. Yeah. Okay. So, Zara. So, yeah, let's go around. And what I'd love to hear is who you are, why you were interested in just this broader event on human flourishing, and why you joined this governance and society subset. And so I'll introduce myself again. So my name is Hannah, and like I said, I worked as a psychologist and cognitive neuroscientist. And I was at Stanford. Stanford has a lot of AI based things going on. And when I joined, there were two main focus areas for me, at least. There was AI as a model of the human mind, human brain, both kinds of things, which was very inspiring to me for a whole bunch of reasons. And two, there was the reproducibility crisis in psychology, and this was this recognition that psychology findings, and honestly, many social science findings and medical findings, anything that's statistical based, were not reproducing, or at least not reproducing as much as you would like. You would like a more efficient science. And that led me to kind of look at science as a system, right? It's a system filled with people with specific incentives, with different tools that could help them go better or worse. And so, like, different journals prioritized really flashy, exciting findings, and that led to less reproducibility. My reason of bringing that up is just to say it made me aware of the intersection between the technology or the science that we care about and the social structures around it that determine how it will evolve over time. And when I left the protected gardens of academia and moved into kind of industry, as we called it, that was a time of really considering many different things for myself. And I ended up being influenced by a movement called effective altruism, which, amongst its multiple tenets, is like, how can you use your time, your career, to do as much good as possible? And that means, what are the most important problems in the world? And EA was an early kind of focus on AI. This is going to be the most important technology. Making sure that it goes well is probably the most important thing to work on. And so I got inspired by AI ethics and AI safety. I didn't even know the word AI governance, but I learned about that with this company that I joined Credo AI, and started building the tools to support companies, build better AI governance practices, engage with policymakers and researchers about what standards could we create, how can we communicate them, all of these kinds of things. And so AI governance has now been kind of my life for the last kind of two and a half years. And of course, in this last year, not only did Chat LuisPT come on, but a huge focus on AI safety and AI governance more generally. Right. Other AI systems that aren't llms. And so that's what brought me here and on the governance and society. One of the reasons why I started the AI salon is because I really believe that in line with AI, being the most impactful of technologies means that it is really important that we bring as many people into the conversation as possible. Not just the conversation, but, like, meaningfully impacting how these systems are going to move along. And, for instance, coming here, I don't hear the indian perspective very often in most of my work. And so, even being here right now is a small example of me trying to better understand a broader range of technologists and things that people are going to do. And so, anyway, that's me. Let's move around.
 

Speaker F: I've always been interested in psychology and there's no real title as a design psychology person in tech world. So I've always had a freelance practice as an experienced designer for psychotherapists and psychiatrists and a day job in the tech world. And I've been a product designer, a product manager, and now I'm an early stage founder. My co founders are senior psychiatrists, psychotherapists, and we're working on building data analytics and training tool for the eastern hemisphere for mental health clinicians. So to put it in really simple words, most of mental health work, as you might know, is manual. In the eastern hemisphere, people take a lot of notes and to be able to reproduce a case, even just to explain it to your peers or in clinical supervision is really hard because I have a lot of caseload and it's all physically written, so there's no way to explain all this material without creating errors. And so we're working on something that's a recording tool, a private, secure, anonymized recording tool that will allow for mental health clinical training in the eastern hemisphere. 

Speaker Luis: I initially trained as a textile designer, and then I further went into design research, which was, I was working a lot in social impact sector, some of the corporates also trying to. So it's more about trying to figure out human behavior and try to link it back to how it should be designed. And in the last five years, I've shifted towards, I work for a fintech and so I take care of the user experience research with the fintech. It's interesting, I think also for me to sort of the last twelve years as a career from textile design to digital design. One interesting observation is that our careers, our regulations, our society, everything changes with these industrial revolutions or digital revolutions we've had. So I think currently, so far in the app space or a digital experience space. As a designer, your role was a lot around, because fintech is an interesting space where it's like you are sort of doing a bit of behavior design with regulation also coming into picture, right? Because finance is a space heavily regulated, right? So when you're designing, it's not so open. You have to keep in mind of human behaviors, their fears, the corporate sort.

Speaker E: Of.
 

Speaker Luis: The corporate objectives, and at the same time the regulations. So these three things I think gave me an interesting perspective that how, say, the finance industry is changing, the regulations are also changing. So then as a designer, you feel a little dissonance in the sense that where you keep the corporate sort of interest and at the same time what is good for the human behavior, right? And with AI coming in, I think that becomes, that I feel becomes even more blurred because I feel like a lot of these AI models will be fueled by information that could be sort of, it depends again on the corporate interests or the government interests rather than the human interest. And then as a designer, where is our role? How do we change that experience? It's an intermix of governance society, at the same time ethical and what's right for the human. Because as designers, it's really unclear how our roles are going to evolve as AI comes in play, especially creating experiences for people that might. It's a big question mark right now, for sure.  
Speaker A: We're having a discussion right now about the relationship between AI and governance in society. And we haven't even really defined exactly what that means yet. But that's the theme.
Speaker Luis: I'm Joshua. I'm a lawyer by training. In the last decade, I have worked in rightspace organizations in the social impact sector, development sector. My last job was actually I worked from this building at Swast. What I did was during COVID I managed the delivery of oxygenation devices across India. Well, why this is interesting to me is obviously AI. We didn't talk about AI in law school a decade ago, but the things that I learned in law school, the things that I have seen in my work in the last ten years, how those things are relevant and they need to be talked about. Say for example, your AI systems are dependent on data, and it really matters where you gather that data from. If you don't take into account that data would have bias. You would have to make sure that the data you acquire is diverse enough, then it would show a different kind of result. Again, as a lawyer, I want to tell you that law is jurisdiction wise. So what laws we have in India is different from what is there in EU. EU has come up with the new AI act. So, yeah, just thinking through what are the implications for, say, the global south? Because our challenges are pretty know the thing is India is also three different indias. My reality is very different from someone in a remote district in.  

Speaker D: I think I kind of take a leaf out of that when I think about how AI is impacting or AI is bringing that technology. So one of the things that has animated my world is the philosophy, technological determinism. So it is the technology at the end of the day that is going to maybe impact the future much more than anything that happens otherwise, whether it is, who are the kings who come up, what are the dynasties that are created or anything around that. And I think some reasoning around that is people say that it was a gunpowder that was responsible for the expansion of the world about, let's say, 670 years ago. And now it is computer or the telecommunication technology that has been driving what we see in the world. And maybe now it is the time of AI that's going to define what happens in the world from now onwards. I'm specifically interested in the governance bit of it because, as I said so, I have been curious about how society interacts with technology, how the governance structures interact with technology. And I have some experience in that. I worked as an election political consultant for about three and a half years, previously seen a past life. I was also posted here in the Karnataka government and then being able to see some of those things that happened from closed quarters about how people within India, they conduct elections. What is the farce of elections or the governance that goes on actually behind closed door within the political world? I'm also curious from that perspective, how does that reality kind of conflate with what AI can bring in in that same world?
 

Speaker B: I'll quickly tell you about what I've done. I worked as an engineer for close to four years, kind of got bored doing technical work. I enjoyed the work and it paid well, but I think there's something missing. So at that point, I did a switch, did a master's in design and post that I've kind of always been in the intersection of technology and design with human centered design. Post my masters, I've worked in social things closer to my heart - one was, I worked in affordable private school, we were trying to see how can you improve the learning outcome with a digital tablet. 
Speaker C: With the blue collar workers.
Speaker B: Another one, we are trying to work how can you help them find jobs, better jobs. And being interaction design, I've kind of tried to be on the forefront - working with the next billion users, working with chatbot. And I hit a kind of point where I was evaluating what to work on next. I explored public policy, did a program to understand how public policy is made in India and how the world works. I also spent time understanding climate change and what we are not doing as humans towards it. My takeaway was there are important, necessary problems, but they don't pay well. Right now I'm trying to build a tool to assist designers, trying to explore AI.
Speaker A: I would love to hear what about governance and society now is most alive for you.
Speaker B: We have a way digital public goods have been released in India with things like UPI. There is some AI activity going on. People talk about the corporates, government and society as pillars shaping things. AI is an equally strong technology playing a part. Corporates have a strong voice, government functions have it, but society has lacked a voice in determining the agenda. With a foundational technology like AI, I feel that lack of participation will hurt humans. Bringing in more agency would be an interesting challenge. 
Speaker A: Yeah.
 

Speaker C: Hi everybody, I'm Mithan. I'm running the networks of one of the banks in Dubai. I've been a techie all my life, 20 plus years, hardcore into networking, never software, but watching AI on the sidelines. I did a course on Tensorflow, and of course professional curiosity. I'm a futurist, and a Sci-Fi geek, stereotypical Engineer liking science fiction. I love Star Trek, I love science fiction movies. I grew up in the Middle East. I've seen another form of government which most people in India don't see, which is government run by the ruler over there. Basically, taking decisions on behalf of not just their population, but expatriates. In the UAE, Indians are 60% of the population. The local population is only 5-10% of the actual population. The governance is run based on their priorities. I'm not saying good or bad, just that's the way it is. 
Speaker D: And we get to see a different vantage point.
Speaker C: What if we hand over the reins completely to AI? What if AI does decision making for human beings? It would be free of bias and cognitive bias. It would be able to do qualitative, actual decision making, free of corruption, influence, meddling from people who want power. The money we spend on elections, corruption, all bypassed. I'm being ridiculously optimistic. Some call this dystopian cyber communism. I've always been positive about technology compared to my peers. That's why I'm interested in this.
Speaker Luis: Can I ask, how do you make an AI unbiased? People assume technology will be unbiased, and that's a lot we talk about.
 

Speaker E: Hi, everyone. I'm quintessentially Bangalore. I've been here for more than two decades. I work in the public technology space. I just had a report that I co authored with UNESCO on guiding policymakers to help with their AI strategies in the countries, because that's something that's very big. A lot of people want a slice of their pie into understanding how AI can augment different realities or different strategy areas. India has a very bold AI strategy indeed. It's very impressive, but it's just not. One of my complaints would always be that there's just not a lot of documentation around it to show the world and say, we have a very ambitious strategy. I also work on online safety and web accessibility - how can technology be more understandable and robust for people with different disabilities? I'm an engineer by training. But yeah, a lot of my issues kind of stem with public interest. One of my current research right now is on synthetic media and Luisen AI - how does it influence elections and conversations? Even with chat, you've realized that information isn't updated. It has a lot of societal implications. I also come from working on preventive measures, on looking at offline consequences of online information that has exacerbated genocide in conflict zones. And we see that in the Luisenii space with whatever Chad LuisPT or Luisrok comes out and says - reputational harm. We had someone who spoke about deepfakes - deepfakes is a pandemic in itself, when it comes to online safety or deepfake pornography, where earlier we had female politicians or women affected by this. But now, with all the simplistic tools and access you have without regulation coming in, how does that impact you? What are the safeguards? And I had seen, I had done work with online safety in the south asian context, and it was quite jarring that 90% of the cases in the cybercrimes themselves are with defects.
Speaker A: Awesome. 
Speaker E: Thank you.
 

Speaker C: I work in Bangalore, and I just moved back from Singapore like, a year and a half ago. I did my masters in machine learning, but they had a really interesting name towards it. They called it masters in knowledge engineering. It's about how you imbibe knowledge into a system. It did focus on the math of machine learning. And my eventual interest was always in robotics. So I did research in Singapore working in soft robotics, and now I'm working with a company that makes laparoscopic devices. We're based in Bangalore, and we make the system stack for fluorescence imaging and eventually going to move into surgical robotics.

Historically, I've been fascinated by conversations that don't reach conclusions - in politics, ethics and stuff like that. I've read a lot of Foucault, Rawls, about the philosophy of justice. It's not about making laws. It's about which set of ethics or laws results in maximizing the justice. 

I have a controversial statement. I feel like with AI now, what the end user gets is the runt of the technology. If you are wowed by this, there are more powerful stuff at play you won't see. IBM had a supercomputer for 20 years, we didn't do anything. Major AI breakthroughs now are far from the consumer - like AlphaFold, DeepMind, open AI. The end user always gets a watered down version. So I feel the whole AI risk is blown out of proportion. But I am ready to change my view.

Speaker A: Cool.

Speaker H: I'm a psychiatrist and psychotherapist. As someone exposed to human behavior I'm interested in this AI movement - how we think and ideate impacts human behavior and vice versa. I'm interested in governance because behavioral specialists don't feature much in legislative sections or political sections, and our opinions aren't asked often. We end up seeing people when there's a problem rather than preventatively where we could offer more. 

Ethics is one place we need to think through, but power lies in governance. What you said about justice makes sense, but we need a balanced view on what we're doing now. I agree it's not as bleak as portrayed. But if we don't govern properly, it feels like we've taken for granted that AI will just magically happen. We can think more, be informed, and direct it to be beneficial rather than it just happening out of our control. I'm interested in governance from that viewpoint.
 

```Speaker A: So I think I'd like to start the discussion with something like who has power and how does that power proliferate through society. So I forget who brought up that there are three indias, that there are different kind of, that somehow this country needs to develop some rules of the road that if not in the best case, bring up the bottom of the country but certainly don't marginalize. 

Speaker C: I'd like to confirm what you would mean by power the people, or I don't even want to say people. Is it like a force? What does a force push? Does it push regulation? Does it push a lifestyle of living? Does it push behavior?

Speaker A: I guess where I'm coming from is there's a combination of forces that determine how the future evolves. And regulation is one set of constraints, pretty powerful one, because nation states are very powerful in determining how certain things evolve. But of course, access to technologies and human education, those are all other aspects that push against, because we're in the governance and society, we can think about, maybe focus our conversation on how the regulatory kind of forces intersect with bottom up kind of societal forces.  

Speaker C: That, say, the SEC or even major regulatory bodies, they are still pushed by a couple of vested interest groups who wield a lot of power. So power in that case means a different thing because it is a much more powerful kind of power. If you are able to shape regulators, like, for example, how opena is trying to do for AI, they want to shape how the regulatory stuff will be framed. And like it or not, if they succeed, then that will be the framework for all regulatory bodies to take a look at and frame their own stuff. So power, in that case, does it mean just technological prowess in the case of OpenAI? My point over here is that if we have to dissect what the systems are doing wrong, then do we shift the power onto what's happening now, or do we shift the conversation to what we want to happen.

Speaker H: I feel like technological prowess may not be enough if it is not popular. So the power might lie more in what becomes like a mass thing.  

Speaker D: Even to think of that, while AI can be the mass thing, where it influences society, your everyday life, it is also a defense, or let's say a military technology at the end of the day. And then when AI talks about the power, that power can also wield the technology to do other things that probably are not the things that we are talking about.

Speaker A: I guess just to bring it. There are many stakeholders in. Let's just talk about what are the three indias that went with. What were you mean by what are the three?  

Speaker Luis: Okay. The first could be people like us. Super privileged, have traveled abroad and speak in English.  

Speaker E: English is.  

Speaker Luis: Yeah, think in English. All of that.  

Speaker E: We're also the only ones who pay.  

Speaker D: Taxes or the ones who don't pay taxes.  

Speaker E: If you're Uber.
```
 

Speaker Luis: Yeah. The second would be people who live in tier two, tier three cities, women who are shepherded everywhere where they go. They probably don't even own their own phones. If they do, they would. Yeah, yeah. It would be something like that. The reality is very different. And the third would be, I'll give you an example. Like Karnataka the state we are in, the indicators maternal mortality, and all those indicators are like some European countries, but there are other states in India, in the north whose these kind of indicators are like sub Saharan Africa. So if you go read, I'm not talking about sitting in the capital of those states, but if you say go deep inside a district, in a subdivision, the reality of women, children there is very different. 

Speaker A: Identify with them because, yeah, totally within this vast range of different kinds of people, perspectives, goals and abilities. When we talk about governance, there's someone, some entity that will certainly say that they are trying to represent the many stakeholders. That language will always be used, but sometimes maybe they will be. Maybe there's regulatory capture and such that they're essentially a voice for kind of corporate interests. I guess the conversation here is, given the reality that people in villages are not going to be empowered, they are not going to be that meaningful in the evolution of AI. They are going to be affected by it, but not empowered for it. And that happens on many levels. And I was saying, like, India is in some case as a whole country affected by this trajectory that the US is honestly leading in, how do we set up structure to safeguard, or at least make that more of a.

Speaker Luis: I'll quickly finish. Just as a lawyer, I'm telling you there are some laws in India, so they are remnants of the British. Most of our laws are remnants of the British. So a lot of the stuff that we are talking here probably would also come under Indian Telegraph act, and Indian Telegraph act is from the 1800 something, 1870s or something. A lot of that is still relevant. A lot of the telecommunications that happens there are new laws, there are new rules, but that is like the parent act. So there are similarly, in every sphere of your life, there are some laws, like the evidence act that we are talking about. The Indian Penal Code is from 1860. So they have stood the test of time. AI will also be regulated, say, under. We now have a data protection act that came out this year, has a lot of laws, is not enough. But there are other things like the IT act from, say, 2000. So we have to make sure that whatever we are coming up with now is dynamic, that stands the test of time, say, 100 years. Like your US constitution, it's been there, doesn't have a lot of amendments. And yeah, it is the most important.

Speaker A: So maybe building off this, and a question is with a technology that is changing incredibly rapidly, where the laws that our choices are either don't regulate, right, just let it evolve or pass laws without much information and see how they go. They're not passing the test time. We're taking a guess. How do you engage in that kind of governance? And how do you get feedback from society that these things are going to pass?

Speaker Luis: Same as your constitution?

Speaker A: Hold on. I would love to shard. I'll just say maybe at like 7:15, we'll tell people to come back here and then try to aggregate here at like, 7:15. Sound good?
 

Speaker F: My perspective is that of a product builder. So in this conversation of AI and governance, I'll probably be one of the persons who's building that software product from that vantage point to distill AI as a word a little bit. What it means is quality data sets. And I would be somebody who's responsible to gather those quality data sets for, say, a client who is the government or who's the election body or who's a political party. And when we're talking about power over here, the power lies in the hands of people who commission this project. And the power does not lie in the hands of people who do not have a choice in submitting their data. So I think a big part of power comes from, or like, the conversation around power comes from data literacy. So in the three types of India's, one part of India could probably not consent to this, but the other two parts of India does not understand what consent means and does not understand that. When a government personnel comes to my doorstep and asks me questions about my life, about my income, about my husband, I just answer because I'm just used to it. Because Social Security number Adar, for us, that's the equivalent. For us, it's very notoriously leaked, very often as a large database, and there is no control as a population, we can't do anything about it. So to bring this conversation back to power, where does the power lie? I think the power lies in the hands of native language, data literacy. It lies in the hands of creating words in my mother tongue for what it means to give consent. When someone asks for my data, what am I, what is it? What is that data going to do? Where is it going? Whose intelligence is it contributing to? 
Speaker A: As just a minor pushback in the US, I feel like people give up their data all the time, and it's not because of data literacy. Maybe to some degree it is, but it's certainly not a linguistic barrier. It's because the benefits are now, and the downsides, if they exist, are in the future. They're aggregate.
Speaker H: They know it when they're giving it up.
Speaker A: No, I'm not taking away the fact that consent is still relevant, for sure. I'm just saying when another community is asked for consent, they consent all the time. They consent constantly. And so maybe you feel morally absolved now. Oh, they're consent.  
Speaker E: Have a question. Everyone talks about consent. Your terms and condition documents around 8000 words. We don't consent to everything. We don't have the time to consent to everything. So consent is a very loosely. I feel like it's a very imaginary term. 
Speaker A: Yes.
Speaker E: A lot of the regulation in the US, I don't think it should be a gold standard for the world. I mean, if you look at the American Disabilities act, because I work in web accessibility, a lot of the, I would say embellishments or even revisions are very reactionary. If you look at it historically in terms of lawsuits, and these are basic things, right? For example, providing closed captions to airflare entertainment, or giving you closed captions for educational content with Harvard or Princeton or Stanford. So all of them have been. Because there's been very large amounts of lawsuits that have been filed, and that's why you had provisions come in. So I wouldn't say that the US is a beacon of hope when it comes.
 

Speaker A: I'm pointing out that this is kind of with a number of aspects to make consent. I feel like consent is probably not going to be a solution to allow people to participate in a way that advocates for themselves, either because they have linguistic barriers, that whatever solutions have been created have been abused, or because they don't have the time, or it's impossible to truly consent to these kinds of things. It's hard to make these guesses. Maybe we can set up institutions that can advocate on behalf of large numbers of people because they are data trusts and they represent humans. And we recognize that it's relevant to have essentially joint negotiation for a bunch of ignorant people, such that things that I don't have to advocate for my water cleanliness, that is something that I shouldn't have, or that the safety of the flights I take, that is outside of me. 
Speaker Luis: There's also a cultural aspect, we talk about consent and privacy, but also the rural India or the other India that we talk about, they don't care so much about privacy because their lives are not so private. Like, yes, maybe through not technology, they are giving away their data. But the way the village lives or way the rural lives are, the data is easily available.
Speaker C: I would go on to one more point. The thing is, they probably don't understand the consequences of giving away their data.
Speaker Luis: Yeah, that is different. Consequences are different, but they don't care about privacy so much and there's so much struggle in their lives that the ease of access. So today, having. Linking my aadhaar to my mobile number gives me access to ration card or regular ration is way part of my survival than thinking about how this Aadhaar data is going to be used.  
Speaker E: I think a lot of people from those regions are very conservative when it comes to what they do online.
Speaker Luis: UPI is linked to bank accounts. Women don't have bank accounts, so they won't be able to use UPI.
Speaker A: When we think about governance in society, it's like individual behavior is just one aspect and is sometimes used as an excuse. So in the US, I don't know how it works here, but for a long time there have been ads that important. It's your responsibility to recycle plastic. And those were actually put forward by plastic, like creating companies, because it makes it an individual problem to be responsible for recycling, not a corporate problem, not a government problem, not a societal problem to deal with recycling. And it's ridiculous because it's not going to be solved on an individual level, because it just isn't plastic is not plastic. It's not a thing that's solved on an individual level, neither is data privacy or what we're talking about more generally, which is stakeholder empowerment, like, how will different individuals be affected? And this is all within the context of a technology that is under human control to begin with, which is, like, not a given for AI. And that's a bigger. A lot of the AI governance is about humans controlling AI versus AI controlling AI. Right now, we're having a more typical kind of technology conversation, which is like, who is responsible? And how is this technology going to change power? And will governance be used, as you pointed out at the beginning, from a regulatory capture place to instantiate power that's existed before, or will it be a destabilizing force that we can hopefully make use of to empower more people?
Speaker D: But can we go towards this humans governing AI, and how does it work?  
Speaker Luis: So, whenever we are coming up with, say, new regulations, we should also dig deep and see that in whose interest it's coming. I just read that the US has come up with some AI directives, and that was drafted by Rand Corporation. And who paid for that to Rand corporation was Dustin Moskowitz. That's Facebook.
 

Speaker Luis: No, I'm not saying anything. You just have to see that all of this writing work, someone has to pay for it. So it's nice to see where the money is coming from, and then you can make the connection. It was kind of like that. So Nandanin Luishani independently developed the technology, then he lobbied for the government to adopt it, and then it became a power, because then the USBI is still.
Speaker E: In charge by private company. So then all your regulatory frameworks and audits are then at courtesy of.  
Speaker D: Maybe it's a kind of a framework or something, but then the end of the day, it is governed by.
Speaker A: You're going to have the last. And then I would like to move in the direction that you want.
Speaker H: Yeah, actually, I wanted to start.
Speaker A: One more question. No, sorry.
Speaker B: I have a clarification.
Speaker A: Okay, clarification question.
Speaker B: When you talk about this governance, it's spoken about data and mostly data collection and that, hence privacy. That's what I'm guessing, but it's equally important as the algorithm itself, the model. Like, what happens?
Speaker A: Let's clarify for a second. So when people talk about governance, there are many things that you're talking about. On one end, there's hard law. Like some regulation is set into place that has some enforcement association. So fines can be levied, you might not be allowed to do something, something like that. So the EU AI act is an example of that, where if you have an AI system that is a current kind of thing, a high risk system, you have certain requirements. And if you do not meet those requirements, the EU, whether it's enforced or not, can impose a tax, a fine on you of like 7% of revenue. Hard law. There are standards that are set up, standards are not enforceable necessarily, but end up becoming part of what's called soft law. And so this is the idea that people might have to act in a certain kind of way. There might be a certifier that certifies against it. Again, it's not required by any government, but other companies might require it. I only work with companies that are certified against standard x, and there are organizations internationally and nationally that create these kinds of standards. That's another kind of governance. There's corporate governance, where organizations determine how are they going to act as a company, what are their policies for whatever. And those governance are generally there to deal with risks and compliance needs. And those risks often bear out in financial terms. So they don't want to have a privacy leak, not because they care about privacy necessarily, but because they'll get a pr backlash or something like that. And some companies will create policies that reflect a kind of culture, maybe an ethical culture, ultimately, that probably leans back to a brand that is relevant for financial improvement, but that's not actually how the company acts. The company says, we're an ethical company that will not do x, and they don't revisit that all the time. That's just part of their charter, their governance, and that's how they behave when they move forward. So those are different aspects of how you set up some policies that end up having effects on the incentives of behavior moving forward from very hard ones, you can't do this whatever to softer ones which end up influencing culture. So that's broadly. And what those policies apply to can be data, can be how you release models, whether you have to identify risk before you deploy a system. The world is your oyster in what governance can apply to. And certain areas are more mature than others. Cybersecurity is a more mature part of technology in terms of governance than like AI model governance or stakeholder involvement or something like that.
Speaker E: I think India would be interesting is in their AI strategy, they outlined military, healthcare and education as like the main.  
Speaker A: Focus area, focus areas.
Speaker E: So healthcare. And it was very interesting because it was a very participatory method you have intel who's taken over the educational path. They're responsible for building curriculum, and they have something called an AI for all initiative where they're looking at imparting curriculum within schools in the central school system. So we have different education curriculums for the central school system is usually why the. I mean, they have the largest number of schools, and then you have healthcare, which Vadwani AI did a lot of work with. They're a nonprofit working on healthcare data aspects. I don't know too much about the military aspects, but yeah, these are the two ones that were very interesting for me.
Speaker A: I would like. Okay, now, please.
 

Speaker D: I think a good anchor question that at least seems to me is a good way to think about human governance of AI. And maybe like a vote around the room kind of a question is pick any government of today that you really like or that you really be associated with, whether it could be the EU, it could be the indian government, it could be the US government, it could be the UN. If OpenAI, let's say a company like OpenAI were to achieve what can be called ALuisI or super intelligence or human like AI, anything that's beyond let's achieve. If they were to achieve it, your favorite government, should it nationalize OpenAI? Should it take over Openi? Should it control that AI?
Speaker C: So you're saying if my favorite country, should my favorite country pick up AI.
Speaker D: If it, should it basically nationalize? So basically open AI, let's say it's going to, let's say productionize that technology, it's going to put it in front of people or whatever it's going to do with it as a corporation, whatever it wants to do?  
Speaker A: What is the human oversight governance structure that people here would find like they would want?
Speaker D: Would they be okay with. So the question is, would you be okay if the US government, if OpenAI were to achieve Agi, would you be okay if the US government nationalized OpenAI?
Speaker A: Right? Because right now, the governance controlling that ALuisI is OpenAI's governance. This corporation, it's the market, it's a.  
Speaker D: Bunch of other things. But then the current governance structure is, of course, the government in place.
Speaker A: So I like this question. So think about that for a bit. What kind of governance structure you can make it a country. And one answer could be the free market. You don't want kind of government constraints on it. And you think that evolution with under kind of market innovation is actually the best way to move it forward.
Speaker C: I have a question for the question. So when you say open air is going to relinquish control of its assets to the government? What would be the risks that the government undertakes upon itself for nationalizing this and having this into their weaponry? And two, what would be their liabilities?  
Speaker D: Assume when nuclear energy came up you had npts and you had basically countries.  
Speaker C: So it gets like the internal you're.  
Speaker H: Asking us to think about. Your question is what is asking us to think? Do you want government, whichever government, do you want them to just relinquish control or have control? Or how would that governance adopt?
Speaker D: Do you want it to be same way that we did for nuclear technology where it was protected, Iran was stopped from developing it. A bunch of countries went there and.  
Speaker F: Stopped at the question, go for it. This is like now going into speculative futures. 
Speaker Luis: Love it.
Speaker F: I think I would trust the swiss government.  
Speaker C: Yeah, that was my.
Speaker E: I would also trust the singaporean government. It's an easier.  
Speaker F: I think, a nation who has had to develop its economy with constraints of very strong geographical factors, which focuses on excellence, craft, resilience. I like these values. And I think if I had to choose between a bunch of powerful people.  
Speaker E: I would choose these kind of powerful.  
Speaker Luis: People who make excellent things and believe.  
Speaker F: In living a slightly more decent life but also earning money and living well. Yeah, I think the values had to be perpetuated around the world. I imagine this is the kind of.  
Speaker E: Speculative future I'd like to live in. I don't want to break your bubble, but a lot of women aren't allowed to vote in. Sad. I know. I'm not like.  
Speaker A: Let'S not make this.
Speaker E: It's not a Moranti argument, but that's a very good example.  
Speaker Luis: This is a very diplomatic cycle. I'm sure there are a bunch of loopholes here.
Speaker A: So these are the values you would like to be reflected.  
Speaker C: The big problem I'm seeing is that AI currently differs from nuclear weapons in one massive way, which is nuclear weapons were purely destructive. AI is purely capitalistic at this point. Nuclear is not purely destructive.
Speaker B: Yeah. 
Speaker C: This was the nuclear weapons. Nuclear is the ip that they have captured. Right. Not nuclear, no.
Speaker D: You can destabilize the country's cyber infrastructure.  
Speaker A: Yeah.
Speaker D: Could weaponize it.
Speaker A: Right.
 

Speaker A: When nuclear first came out the destructive power was focused on. And some people actually bring this up as a warning against regulation of AI, because the focus on risk was so strong that it seems nuclear advocates will point out that we have lost or completely didn't focus on it. 

Speaker E: Even at COP 28 in the UAE, nuclear energy was actually the highly most talked about topic in terms of providing an actual solution that one could move from and away from fossil fuels.

Speaker C: No. My point was that most of the regulation that has been a lot of effort that went into regulation was about the proliferation of nuclear weapons. 

Speaker A: Resurgent as an energy source.

Speaker C: Exactly at this point, even now, there's as much nuclear research also that goes on because of the unstable nature of the isotopes. 

Speaker A: I'm going to try it yet. I don't know if you've heard there's a term called nerd sniping, where someone puts out a topic and it's just like every. And then you realize I've completely lost them. I'm going to give a different question, which is I feel like AI being as powerful and potentially foundational as it is, like the phrase foundation models is because they have the ability to be the undergird infrastructure throughout the world. They're very easy to use and apply in different ways. Has a unique requirement for massive multistakeholder kind of discussion and governance. And I don't think we've actually mean the US has tried to be a democracy and India has tried to be a democracy that represents a huge number of different stakeholders and both are currently limited by different kinds of technology. The US, we probe our population once every four years and only 60% of people vote. And whatever India you have, it sounds like a relatively high amount of participation, but a lot of corruption that ends up influencing how many people vote. I see as actually technological limitations. And I am excited for an AI enabled democracy of the future where more people can understand. We were talking about consent a little bit. What are the relevant kind of components of these technologies? Discussions like this can be had to bring people's voices together. And that information can be somehow synthesized into a joint perspective. And my understanding is that in Taiwan, they are one of the most technologically forward countries in their democracy, led by this woman, Audrey Tang, who uses much simpler versions of analysis, like principal components analysis, clustering these classic data science techniques on survey data, to be able to do much more and focus a whole country onto the areas of agreement rather than division. And that's an area that I think is going to be quite a lot.  

Speaker E: Of citizen participation consultations during COVID as well. Chile is also a very good example of what you're describing. The Chile AI act that was passed also had a lot of citizen participations in different ways. And they had, I think, 67% participations from women and diverse groups and younger people. 

Speaker C: I find a very. Maybe a disturbing undercurrent to the themes of all the countries you've chosen, is that they've gone through like a bad event. A very bad event, actually.  

Speaker E: All of them have. There's no country that's Singapore perfect in many ways. I mean, Singapore is. Yes, and Rwanda is kind of following Singapore. So a lot of people who know about Rwanda, Rwanda is kind of looking at the same surveillance style, and they are a smaller population, easier to test things out.

Speaker A: So as the introduction, I think the.  

Speaker D: Way I would compress Hannah's answer is, there's this famous sama quote, right? When we get to EJ, I'll ask it to figure out how do we make money? And then I think you're kind of saying something similar, that when we get to EJ, we'll ask Egi to govern.
 

Speaker A: I'm saying something slightly different, which is that's possible. I'm open to that future as well. But I'm actually open, I think, diffused amongst people. There's a lot of wisdom and also the values that we want to be incorporated into our governance of this very important technology. And what we currently lack is an easy way to bring that information together. Because in the end, we're going to have to make decisions that don't represent. They might represent everyone, but they're not everyone's decisions. Right. As a group, we're like, what restaurant do we want to go to? We've heard everyone, and then we go to a restaurant. We can't go to ten restaurants. And so what is the way that we can bring that information together?  

Speaker D: Is that necessarily.  

Speaker A: Keep on going? Right.

Speaker D: So one of my favorite shows is best wing, and then I think there's a nice quote from that. I like that. He says, should you have direct democracy, which is kind of what's there in Switzerland, right. Or you should have representative democracy where you've got someone who's, let's say, a bit more well versed with what's going on, and then they're not going to just take your votes and opinions and do what you want.  

Speaker A: Yeah. I would like AI to be this intermediary, this representative should be, that is not just deciding, is gaining the information.  

Speaker D: And then giving it to the decision.  

Speaker A: Makers that are human, which could be an AI or could be human, I don't really know. People are starting to come in. In the last. I'm sorry to say that there's a limitation on this discussion, but I hope even when I'm not here, you can find other AI salons or equivalents. And if you would like, help in the future, I'll say this to other people, setting up your own. I have resources. I'm happy to support you. 

Speaker H: I feel like the reason why we're having this conversation is because we think that something's going to make our life better and it's working towards making things better. And if you take examples from how someone found penicillin and someone said, oh, that's rubbish, you can't use something that's going to be harmful to humans. But it was studied, and the same way AI can't be just studied as it's going to be beneficial. It needs to be studied with its risks and benefits and use of. Is it going to be good in this population? Is it going to be good in that population? And that is what is happening right now. But what you're asking for is, for me, the worry is always when we say, can we do it faster? Can we do it bigger, can we do it better? And when that becomes a question, we're forgetting that is bigger better, faster, always.  

Speaker A: Better, actually better, actually better.  

Speaker H: Or should we also be looking at, if it's bigger, what is it causing us harm? And then we can decide, yeah, the harm is there, but it's less. I think that conversation, I'm thinking from a health chemistry. So that's what we do with medicines, that's what we do with therapies. From the time a person has an idea that I'm going to try this robotic surgery, he also has to think how it's going to be harmful to the person. And I feel like people in AI should be doing that. Just because I develop something and, oh, my Luisod, now I know I can do from five g to four g, or from four g to five g and whatever. Just because it's brilliant doesn't mean that it is going to be brilliant. I think at the same time we need to be thinking, what about all the consequences? And that needs to be part of the conversation. And only then it will be evolving, like how humans evolve. Otherwise it's going to be restricted only in terms of it becoming bigger and better and faster.

Speaker B: I think it's probably useful to think of, I think a has become too broad a bucket, and should probably look at use cases. And for each use cases, the way we would govern it would be very different. And that it makes it more tangible to also analyze use case, use case. Because right now it's just too fuzzy and too many things are getting clubbed. And what I mean by AI is different from what each one of us. Mean by AI, right?
 

Speaker B: I think the second thing, which I would really like, I'm not economists, but try to put on that hat, right? Economics is basically when an actor brings in some value and other people are interacting with that person and they're exchanging value, right? So AA is a system which is bringing in value, whatever that value is. I think try to apply that lens, then think about like, there's a different way of different preferred economic models. So then I think we need to look at that angle as well. For example, which political economic model is preferred? And from that model, we would kind of have a preference to say, AI should behave in this way. So I think that's a whole box to be unraveled and whether it's purely a market driven or market driven press segregation. So I think that needs a deeper thinking, which I would like to.
Speaker C: So there's this quote by Susan Sontag about the nature of pictures and media. So she says that when the advent of pictures and media happened, there was this inherent bias in people's minds about an objective truth to whatever media has for a long time. When people say that media can overthrow governments, pictures can overthrow dictatorships, and all this stuff, it's because you associate a sense of truth to the media. And I feel like we are at that phase. Where are doing that for ALuisI or for AI? We are blindly thinking that whatever is going to give us is going to be the truth, where in fact it's just a distillation of all of human knowledge and. All the documented human knowledge is put into it. And whatever question you ask, it gives you a regurgitated answer. So until maybe like last year, llms were not able to discover new knowledge. So I feel like there is a substantial pinch of salt that you should always ascribe to any output from an ALuisI or from an AI. And AI has its place. AI is not going to rule over us. I have this one really cool thing which I want to see implemented, is where we don't have the system in India, but in the US, there's a presidential debate. So I want AI to in real time point out logical fallacies and debates like, oh, that was ad hominem. That was like a character assassination. That's a straw man argument in real time. So like that, what it does is it's not something that you can make a proud out of, it's not something that will help someone, but it will filter out your canvas better.  
Speaker E: One is that AI is not a magic wand that will solve human problems, cycle problems. We are extensions of that. AI kind of enables that acceptable, that we should have agency over AI. We should never look at AI as, I mean, yeah, AI could be autonomous in certain ways, but then the agency. Should remain with us. Second one, the conversations we've been having we need to have more platform accountability, maybe not from big tech. The templates of big tech, of the software development, development kits that are being used that have these practices of surveillance economies or kind of generate their revenue on ads you need to look into. I don't want to say lobbying, but society lobbying through smaller platforms, social platforms that can help you, that practice responsible.