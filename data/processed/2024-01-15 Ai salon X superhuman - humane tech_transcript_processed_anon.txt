Names have been changed to preserve anonymity.

 

Speaker A: And it makes sense because that is actually when, it's being recorded for human review. But can it not be a flicker like I've just seen generally, kids generally don't like that. And they. You. Then they think of it more as a thing that's reacting to them and.
Speaker B: Right, right. 
Speaker A: As like a. It engages. Yeah. Yeah. And they'll like, and here's the thing. It is actually algorithms of when it looks at you based on the objects in the thinking. And it just is so weird and annoying to have to like, it's basically, you have to give up because it's too hard to explain.
Speaker B: I'm going to get us started. I find that in the smaller groups, it's actually helpful for each of us to have a little bit more time, kind of focused on this particular topic rather than like, your background or anything like that, to get us started. So I'm not going to have us go around, but at first, before we jump into any topic too deeply, I'd love to popcorn around a little bit. If there are kind of takes or kind of the perspectives that maybe you even shared on your registration questions that you want to bring up right now, I think that would be really helpful. Just to start us off, I introduced the one that I was interested in, which is this societal versus individual kind of perspectives on humane. I'll add one other, which is the original conversation I had on the kind of human centered AI, which kind of started the AI salon. One of the consequences of that conversation was that human centered seemed kind of like a vacuous concept, and human centered, I think, is more definable than humane. Human centered at least gives, like, it's about not the world or not animals or you're taking a human perspective. And even that seemed to be able to morph into almost, you could rationalize human centrism from almost any kind of way of being. And I'm a little bit worried about that for humane. So one of the things that is often helpful at the beginning of these kind of conversations, and probably will be for us, is to try to define our terms a little bit, which can take a long time, and that's okay. So that's what I would suggest we start with.  
Speaker C: Yeah, I actually kind of related to that, I guess, as I was hearing people have their kind of points. One of the things that occurred to me is that I think there's kind of two separate aspects or at least two separate aspects to humane AI systems, one of which is the system created in a humane way, including things like whether the data is collected with consent and with compensation and whatnot. But then there's a whole nother set of questions around whether the effects of the technology in the world are humane in some sense, which I think is probably an even harder question to answer, especially considering not just not only the individual effects, but also community scale, societal scale, that kind of thing.
Speaker B: But.  
Speaker C: Yeah, but I do think there's at least, I guess, those two dimensions of the question to me.
Speaker D: I have a basic question, which kind of goes to something you mentioned, which is, how do you define humane?
Speaker B: I think that should be our first topic to dive into before we go there. Just are there other, we can think of them as different paths that we can maybe return to later the conversation because, yeah, I think we have to start there.
 

Speaker E: I'll tackle both issues. Okay. And through this lens. So one of the most important informative professional experiences I had was during the Arab Spring in 20 10, 20 11, 20 12. I was working in the Near Eastern Affairs Bureau at the state Department and traveled throughout the region during these early stages of these revolutions and upheaval that happened there. What began as a hopeful moment for many people and was also an expression of real dissatisfaction with the status quo and hope that this would change things for the better, turned into a really dark and painful chapter for a lot of people. And I think there's a metaphor in there or a lesson, which is that good intentions really do not guarantee good outcomes. Social media, you can look at that in many other areas, not just the Arab Spring, but that's like a particularly poignant place for me because there was real breakdown of norms and institutions and creative thinking and naivete that was associated with that, and there was real opportunity and then real world harms as well. One of the very powerful drivers that led people to risk their own well being and that of their families to go out and protest against police states and authoritarian governments was the infringement of their own sense of dignity, that what was happening in their society compromised their sense of dignity, and they were willing to take risk to that. 
Speaker B: I think the idea of dignity is really helpful as a sort of value that's kind of central to what we could say humane is. But I think another question that comes with what is humane? The question is, well, what is human? Because if humane, we start as like a kind of, oh, it's treating someone that respects their humanness, maybe even though we could also treat animals humanely. I think this kind of idea of dignity. Your kind of point also about. I think one of the things that leads to people being focused on a more individual or local goal is the recognition that it's difficult to predict future consequences. And so consequentialism might not just be maybe you're not into it on some other philosophical ground, but maybe you're just empirically not into it because it's so difficult to anticipate future consequences that it's easier to focus on the actions you take to be humane. I'm using labeling data that has been provided with consent. I'm able to look at these small things. And so I'm building it with intent. Hopefully that will lead to some future consequences. But that too is probably not. Neither are recognizing that whatever transformative aspect that's coming in the future.
 

Speaker F: One way I think about this is approaching from human needs kind of perspective. If we think about the muscle hierarchy of needs and then considering the needs for the physical needs and the safety needs, and then we have belonging needs. And I think that's a very key aspect that so far we have not incorporated that need to have. We are going to relate to technology because we allowed contorted ourselves or society to fit into technology and then to be more productive and progress in the economical sense. But we didn't really protect our needs for belonging. So I think that's one of the reasons we have so many issues, especially more urban areas like mental disorders on the rise and addiction is on the rise. And I think a lot of things can be attributed already to the lack of connectedness and lack of belonging. And I think there are higher needs also, like the need for esteem, right? The need for having some form of control over our lives, to a certain degree at least, being informed. And then there's the self actualization and all that growth needs and stuff like that. 
Speaker G: Aishas here, given it is a good start for where the humane AI could optimize for. You had something like OpenAI or an alternative AI that has access to a lot of societal decisions. And what should optimize? We could optimize for Aisha's hero care needs. We could imagine Aisha's model of needs where everyone in the whole, you can sort of recognize your stated preferences. Like, I want all the hierarchy fulfilled. And especially when it comes to belonging and self fulfillment, it's kind of vague. So the stated preferences and the revealed preferences are generally not totally coherent. And that model that aggregates entire society's needs could be a very good source for creating humane AI.
Speaker H: I have a concrete first order effect. 
Speaker B: That I deal with.
Speaker H: If I manifest my vision, I'm basically putting a lot of people have a job like the data analyst profession, right? Generate SQL queries, generate a dashboard. A lot of people do this as a full time job. They make a lot of money off of it. A lot of people aren't satisfied with the way that data actually works at companies. I think that's like a whole nother kind of dimension. The whole dream of doing data at a company has been ups and downs and fulfilled and not. But yeah, if it will get good enough, I'm confident that AI systems will get good enough to analyze databases better than humans can.
Speaker B: So are you trying to say that while there's going to be a lot of gray area direct job loss, one reason that it's some kind of hot button issue is because even when people point out like, or look at historical economic trends and people do like, workout there, there will be some period of pain for some subset of people. 
Speaker H: It's literally in my sales process. How do I attribute the cost of this product to the value that your organization sees from this? It's literally in my sales materials. And so a lot of people try to tiptoe around the subject. 
Speaker A: Sure, it's not, at least for us. We're getting rid of the one job we're doing, privacy and automation operations of all the everything. Privacy. So everything we do means lots of very expensive lawyers that people aren't as empathetic with losing their job. 
Speaker E: I think it raises a question of the time horizon you're thinking about.
Speaker B: Right.
 

Speaker E: Okay, so there's going to be short term disruption for sure. And on the far side of it, we've got good intentions and there's plenty of hope that the people who have to train and pursue a different path, maybe they're counting on Ubi providing a safety net or something. You have to look ahead to the future beyond the immediate disruption. I think time horizon is a really challenging thing for humans to factor into our planning because it becomes so many contingencies and it's so hard to really predict. And just fudging your assumptions just a little bit has major implications.  

Speaker H: It's so hard to estimate. I studied behavioral economics, and that whole industry is kind of like, it's really hard to replicate any of these discoveries and any stuff. And all this time that we spent studying is not actually applicable to a lot of stuff that a lot of people studied. So, I mean, I feel like we actually run a little bit of a risk doing that today and not the practical things that are immediately in front of us.

Speaker B: To just be definitional, though it does seem the definition of humane cannot be something that preserves status quo. It can't be status quo preserving. Yeah, that can't be. And so there are a lot of forms of conversation that are like, there are empowered people in some way today, right? Most people who have jobs today have found one way to exist in this society, and a major shift that AI will bring about is going to disrupt that. And so that consequence on its own, to me doesn't seem humane. That can't be part of a definition. There might be other aspects of dignity preserving kind of ideas, is a kind of definition of humane. Maybe something that allows people to maintain self-actualization, which removing their jobs without a Ubi would be not humane. It wouldn't be a humane system that we create. Is there anything else that could be part of a definition of humane? 

Speaker A: I'd say it has to be an extension on top of basic needs. If you think about dignity and basic needs, a lot of those are relative to the norms of society. You have to take apart that from the core things that humans really need. Instead of making what people are asking for, make what people really need but don't realize they need.

Speaker B: So this is relating to revealed and stated preference ideas, but also.

Speaker A: Not just revealed, stated preferences, but also perceived preferences being wrong because they don't know their true preferences literally until it's revealed to them. Everyone's preferences are maybe distorted by societal norms. An objective AI could potentially see through that distortion to people's true preferences in a way that people themselves can't.  

Speaker B: Let me try and build on that. Maybe a humane technology, like human society, has some time constant on how quickly we can adapt, right? There are values different groups have that truly conflict, yet we have to find some way for society to move forward, integrating those values over time. And that happens at some rate commensurate with human biological constraints. One of those fundamental aspects is how fast we can integrate information. Humans have biological constraints that limit that rate. What a humane technology is, is one that is commensurate with those constraints. Extremely fast automated stock trading, for example, is disconnected from humans - it has no relationship to what humans can comprehend or adapt to. I wouldn't call it a humane technology, even if it's beneficial for society.
 

Speaker A: Commerce already is so far removed to begin with. I feel like setting the distinction at the point where humans aren't capable of it. It just pushes goalposts for what we determine to be humane.

Speaker B: I'm not saying that humane things that aren't humane are bad. I'm just trying to put my own definition that people, we all have our definition, one that is in some ways analogous, has some point of interaction at a human scale with the technology. 

Speaker I: One thing that I might add is I don't actually think I have a personal definition of what humane means. But if pressed, certain concepts come to mind, like compassion, like empathy, these kind of emotions. And dignity is part of it as well, that kind of supersede physical definition. I also think I'm very interested in what Chris mentioned, because with AI especially, but with kind of everything and lots of tech, there are these trade offs that we'd like to think as founders and technologists, that we can avoid with just more compute more data, more whatever, more money, more YC grants, like whatever it is that we'll be able to solve these things. And the jobs displacement versus progress thing is super real, and it is just inextricably intention. The other thing that I always think about, in previous life, I was an activist about police transparency, and so many models of what could be most exciting about AI. I'll just take a really kind of non controversial one, self driving cars as an example. It would be very easy to do self driving cars if they were all networked and talking to each other, and if every car on the road were like that. Unfortunately, it would also create this massive surveillance superseding system that would be able to know where anybody is anytime, any moment in the country. And that would be a true disaster for civil liberties, especially disproportionately affecting various groups for societal reasons, for nothing having to do with technology. 

Speaker D: I think the one word that has stuck with me through this whole conversation is consent, which is kind of related to dignity. Also, every person in this room has a different definition of dignity for themselves, and we probably share some. And I think this technology is so powerful and so data driven that potentially it could communicate with each one of us individually and understand what our boundaries are. And then once you understand what those boundaries are, making sure not to cross them. Whether it's privacy or whether it's something else. Yeah, I don't know. I think it's really hard to define humane for an entire group, and I think it should be more personalized, more individual.

Speaker A: That's a problem though, because you'd have to make your boundaries public, which kind of goes against the privacy thing, because if you want these systems to interoperate, you don't want a central one from a government. You want all of them to be able to look at your preferences. But now you're just extremely trackable because you have a bunch of unique preferences.
 

Speaker B: Let's remove the AI for a second. Just imagine that we're someone who is trying to interact with us, a person and another agent. And there was one who came in and said, all of you are going to obey this kind of rule. Now, because I did some survey and what I think is like, that way of organizing society is partially a function of scarcity. Scarcity of being able to apply rules, of being able. I'm just going to point on the positive side of having information. Yours is saying, actually we might not have a scarcity of people who can apply rules or interpret them or interact with individuals, and an AI system can go to every individual, interact with them on there, and not the kind of the false consent we get with large documents that you press agree at the side, but maybe in a more human way, be like, okay, I truly understand you. I'm an empathetic consultant. Whatever that's been reflected. I understand you, and I understand what you really want, and I'm trying to interact with that. That seems like a pretty good component of an ideal form of what we want technology to do. And recommendation systems, social, all of those, if they could have this more humane front end that can convert your specific goals into some way of turning knobs on a technology that is not understood by possible for the first time. I guess the challenge there, though, is that people are always changing. So how do you keep up with sort of just dynamics of talk to social? 
Speaker I: I think that's exactly correct, and I think it's also representative. One thing that we're leaving out of this vision for utopia, which I agree would be great if I had a personal assistant who could help me get whatever would actually make me happy in the next moment. But then there's people who've created that AI, and it's not truly self learning. It's learning over a set of data that you're creating based off of the rules under which it was created. And the rules under which it was created reflect the values of the creator, which means they may or may not have tested the way that the system performs with different dialects, different ways of speaking slang, things like that. It could be that the model responds disproportionately negatively with certain types of demographics that it can just pick up based off of a writing style. These models are so large that we're never going to find out exactly what kinds of these. We'll call it. It's a statistical bias, but it's kind of a discriminatory bias that's possible to exist there. But we're just never going to have enough tests. There's just literally not enough time for us to do so. That's where I'm constantly in conflict about this stuff, because I really do believe in AI, and I want all of these futuristic use cases like augmented reality and things like that, and then trying to figure out how we could actually make that happen with giving people enough choice to feel like they have control and to actually have control over the way the models interact with them, respond.  
Speaker H: To them, it's kind of funny because we have the same mental architecture and bias in our day to day.
Speaker B: Maybe two things I want to take. 
Speaker D: From the LLM, actually, Bill Gates interviewed.  
Speaker G: Sam Altman the other day, and I think it was Sam that has a really interesting point where we can never interpret how a human brain works. There's no machine interpretability of our neurons, but there are promising use cases of interpreting actually from the creator to all the data that's trained on how it made a decision. For the first time ever, we can actually look at how language model, whether our brain or computerized, makes a decision is really inspiring.
Speaker D: I see this as a short term problem, not a long term problem necessarily.
 

Speaker G: Yeah. I also want to say regarding futurism, with soon, maybe Apple Watch is going to have a recording feature where you can start recording all your conversations. Recordings are very useful. And all these startups that are coming with new hardware, they're all recording based. Surveillance is coming whether we like it or not, unless we do something against it. But there are also great technologies that are privacy preserving, like Vitalik Buterin published his techno optimist manifesto recently. He's the founder of Ethereum, which is intended for solar punk. A great privacy preserving future. Like what I'm building too, is a peer to peer database where people can store their own data and we can build group hosted networks of data. And with peer to peer, end to end encryption and different serial knowledge stuff and other privacy preserving technologies, we can get the cake and eat it. We can have surveillance and self driving cars, and all our knowledge stated revealed preferences can be part of a model that is owned by society at large and individuals as they like, with consent in mind.  

Speaker A: If I don't consent to it, how's it getting in the model?  

Speaker G: Well, I think if you structure a society where that model takes a decision, et cetera, you really do not want a part of taking it at all. Should be an alternative. You could still live in that society, consent to anything. You can still farming and stuff like that. You can still live on land. We don't have to necessarily interact with the computerized society.  

Speaker A: I feel like that centralization of concerns in a way that doesn't let me pick what I want to do.  

Speaker D: Sorry.  

Speaker B: I think it's helpful when we bring in other analogies here so things don't seem so unique. Like in society as a whole. We have a social contract, a lot of things we don't consent to, and we have these restrictions put on our behavior and as a whole, we try to navigate this way so that we kind of reflect individual liberty to some extent while having certain contracts. And so one of the things that in this issue with this kind of optimistic future is sometimes you brought up maybe a practical one.  

Speaker E: In some kind, necessarily. Right. One of the cleanest formulations of that was John Stuart Mills essay on liberalism, where you have the harm principle, and that's basically what governs our society and is basically behind the laws, right? But humans obviously do not. People in the same HOA might not agree on how the harm principle applies within their apartment building, let alone nations disagreeing with each other and people in our society. So this is a problem that humans have not solved.

Speaker B: It's not specific to AI, not at all.  

Speaker E: But when you come to something that has that much potential power, it's really putting a fine point on that. And I think that's probably what gives maybe many people on this couch anxiety because it's something that we haven't solved. But no group or cadre has the power to really impose their vision on everybody else at the moment.  

Speaker A: Forever.
 

Speaker A: If you want to make some kind of policy about how AI has to be humane, it doesn't matter if every single country in the world says, yeah, this is literally a technology that individual humans can work with. Somebody's going to work on bad stuff of it. So what we need to focus on is empowerment and safety. On a general basis, has nothing to do with AI. But just like construction, making things like, literally AI is not related because we won't be able to control it. It's just going to make everything more risky and dangerous. And you need to think about all those things then, or just the one thing which is your life, and try and improve that safety of that in general. Because I'm of the opinion that all Jews, unfortunately, not in a bad sense, but all Jews or genetic Jews are going to die in the next 30 to 50 years from a Nazi designed virus, assisted by some AI. They're just going to say, help me kill all the Jews. Help me maim them all, or whatever, and they're going to find there's going to be somebody who's smart enough and evil enough to do it.  

Speaker B: I'm going to bring up a paper that has come up at many AI salons, which is kind of related here, which was the vulnerable world hypothesis, which is a Nick Bostrom paper. The general thought experiment there was like, what if it was really easy to make a nuke? What if, like three people could make a nuke?  

Speaker A: That's the problem, though. Nukes are indiscriminate.  

Speaker B: I'm trying to say it. Difficulty for our standard approaches to try to limit or control very powerful world shaping technologies. If it becomes very easy to deal with it. And he didn't have a solution, and he was like, maybe in such a world where three people can make a nuke, which would be existential, we would have to have a massive surveillance state. And I don't want that. But it's better than the alternative of everyone being destroyed. So that was where he ended this paper. That's not the solution we probably want.

Speaker A: I know we all hope that this is not going to happen. I hope so, too.

Speaker F: I want to bring the conversation a little bit more, focusing on what are more like. We cannot have the perfect answer, what humane and control. Make sure all the technology that's going to be developed from now on, is going to be humane and not harmful. Maybe not just going to that, trying to achieve that kind of perfection, but like a starting point. Is there clear lines, is there anything that we can watch or think around? For example, like addictiveness of engaging technology, because addiction itself, even if the application is helpful in the moment, if it gets the person addicted, we are, I think, taking away dignity in many different ways. That seems to me like, for example, one of those potential things that we can look at - the addictiveness value of a technology or AI product and think about that just as a separate thing. I'm talking about those more maybe simple beginning level things that we can talk about.

Speaker B: I'm going to continue to play somewhat optimist here. One aspect that I find humane about chat. GPT is a change in the interface. It's a way more human interface, both chat. But if you used kind of voice chat, that's even more human. It feels like you're engaging with this technology in a way that as a layer over most other technologies, let's say, instead of burying private privacy settings, which is of course, a dark pattern in many companies, but instead of having to go and click, I just want to be able to talk to some front end that's like, this is what I want, and it just is understood and configures the rest of the technology in our world. Now, this isn't thinking about AI, humane AI, but thinking about AI as a conduit to more humane other technologies, maybe that's a near term use.
 

Speaker E: That's like the UX. It matters, right? If you're doing the sign up flow on any consumer online experience versus filling out a government form, these are two totally different experiences. And the moment you have to fill out a government form, you feel like you're being kicked in the groin. Basically, it is a miserable experience and you feel bad. The experience of filling out these boxes makes you feel bad, and it disenfranchises people. It doesn't work for anyone. There's no one for whom it works. And that form, that experience, that is designed for someone else. It's designed for the government bureaucracy or the institution. It's not designed for you. Whereas any sign up flow like the company has thought about the user and that experience is for them, and it really does create a different feeling. So I think that is relevant.  

Speaker B: This conversation keeps making me think of Marshall McLuhan and Ian, your other comments as well. He was most concerned about scale when it comes to new technologies. So that new media entering life could kind of change the scale of how our life operates and goes to such a large scale that it escapes our control. And I think a lot of these conversations are about, well, how do we bring the scale back down to something that humans could autonomously, or however you want to describe it, actually engage with? So these forms, for example, they're at a scale that, they're at the bureaucratic scale beyond something that we individually could work with. Or back to your example, with the automated stock trading. That's at a scale that humans cannot individually engage with the stock market on. And therefore, it's not humane. So is humane a question of scale here?  

Speaker A: I think the reference scale is just human rights, though, always. And that's legally defined. Not talking about personally, your personal human rights, how you feel, but the ones defined in our laws. And if you want to improve experiences, just edit or add to those laws, instead of asking, maybe you could try to incentivize private industry, but it's cool to just tell them this is their right and everything should be based around that. Yeah, enforcing based rights on complicated systems is difficult and will only get more difficult. But that's what these systems are also for, to help you audit them with. We use AI to audit AI and stuff. It gets tricky.  

Speaker I: We do too. And again, I'm very pro AI, but I just want to take a moment and maybe the other policy people in the room will agree with me that actually translating a law or the definition of a right into any practical case is not as simple as a programming step where you can actually see it goes here, here, and here. Just to give you, again, choosing a non controversial example, my firm used to focus on credit risk, fair lending law. And in fair lending law, there's like, the 80 20 rule is commonly regarded as being the way that you interpret whether a system is fair or not. We're talking about a very loaded word here, fair.  

Speaker C: How is 80 20 interpreted in this context?  

Speaker I: Exactly? But the law doesn't actually say that anywhere. It says you're allowed to use a discriminatory system if the system, if there is no less discriminatory alternative available for it. And even that phrase has been interpreted in at least three different technological paradigms that I can imagine. And then it's embedded into, like, H 20 and a bunch of other automl platforms as being represented by the 80 20 rule, which actually shows up in case law that dates back to the 1970s, and it's under review at the EEOC. And so we're all talking about the law hasn't changed. The law has been the same since the 50s, when the civil rights movements happen. But actually implementing that on a human scale in a practical way is something of much debate.
 

Speaker B: But editing, sorry, I just want to say here the two just pluses. One, I think in the 1970s, they wrote like, the psychometricians were like, it's like, we'll update this every five years or whatever and never happened. And the second is, we have these two problems. People sometimes ask for clarity where they like and like, demographic parity, or this 80 20 kind of thing is defined easily, in which case it becomes brittle. You get Goodheart's law really quickly, which is like, people contort around this metric and it's no longer a good measure. Or then you have this other side where you don't define it, just sometimes intentional. It's not just idiocy. It's like it's helpful to have vaguery in the law, and then companies will clamor for clarity because they want to know exactly what I have to do so I don't get sued. Which leads to Goodheart's law kind of thing, but also makes it difficult to, as you're bringing up, like have this chain going down where you actually move from the policy to the. In this case, it's not policy to a human acting. 
Speaker A: But still, I think, boils down still, that editing the law in your case would still fix that. If the edit is clear and concise and addresses all those concerns. It's the editing of the law. The law itself could be addressed, as in most recent example, is where we have getting basic rights encoded and then edited and fixed, is the three latest privacy laws in California, each one successively trying to better define it. And as someone enforcing them, they are kind of reasonably.
Speaker I: Privacy law governs what you can do with data, which has a tangible thing. We're talking about international humanitarian law.
Speaker A: Yeah, there's various rights that are way harder, but for the simpler ones, the easier you can explain it, the more likely we can actually encode this into any kind of technological system.
Speaker B: Yeah, I think there's a goal that a lot of people want to have an iterative and empirical based policy system where you imply certain, you put certain policies in place that has kind of defined some objectives that then get interpreted in multiple ways and then you can look at the downstream consequences. 
Speaker C: Definitely the thing we want to optimize.
Speaker E: There are companies that are dealing with this obviously right now, like the autonomous vehicle sector. I'm curious if anyone here has any insight in that, because presumably having a humane approach to conducting a vehicle in the world is something that they're thinking about, whether that's the terminology or not. They are solving ethical dilemmas by algorithm. Right. When you come to there are some circumstances where you can't help but do some amount of harm in the world. You got to choose which harm are you going to do? And they're solving the trolley problem. I don't know how they're solving it, but they're solving it.
Speaker A: I wouldn't exactly call it that. As someone with 2000 miles of Waymo and who knows a friend at Wemo, but I can't tell you too much internal information. It's really just a lack of implementation details implemented so far. It's an insane problem. They're 5% there. They're not even 10% to the level five. But it's crazy how much they're not even at those. That's years away. Even thinking about those abstractions, everything is just driver centered and then protecting all other road users. And if you're a road user, there's not like different road users have a different calculus for how to protect them other than what vehicle they're in and their size, that's literally just the size of the thing. And yes, tinier human shaped things get like insane priority little kids will. It's pretty clear that they've specifically made it so little kids get special priority everywhere. It will stop in the middle of like, if there's a kid in a bush like 8ft away because the radar sees them, it doesn't want to risk the kids at all. Okay, so that's an answer. Yeah, that's the only one. That's literally it. It's little human shaped humans, but if it's a dog, it knows the difference. It will not.
Speaker E: My intuition at a certain level, yeah.
 

Speaker D: I was going to maybe summarize that what we're talking about is AI respecting existing laws for the most part and how challenging that is. But maybe do we think that we should implement specific laws for AI, new laws that are for AI, and maybe break that into two separate categories? Because I think these existing laws is like, maybe we can all agree, yes, AI should not murder anyone, not steal from anyone, not discriminate, however we can define it. But obviously that's harder to implement. And maybe the way we would do that is have a roadmap over the next five years that any laws that are easy to implement, just implement them. Everything else we're working towards being able to implement them over some sort of time frame. But then in the case of new laws or new restrictions that we need to put on AI specifically because it's a new type of technology. I actually am not sure about that statement.
Speaker B: Can we talk about laws rather than restrictions, which is one form of law as like nudges to kind of changing the incentive structure in some way to hopefully lead to the ideals we want to have? And so one is like cutting off. That's a pretty strong form of law. We don't want this, but there are others that maybe we can imagine. And I think that's part of where this definitional kind of conversation we're having is like, it's very difficult to even start to say what do we want and what do we not want to be able to chart, but maybe falling back on empathy or dignity? 
Speaker H: No one really likes to answer these ad hoc questions for managers that don't know what they're talking about. So there's a very good element of kind of who you're selling to and that position of it. But then there's tons of jobs, people that do this and they get paid to do it.
Speaker B: I don't know.
Speaker H: And the other thing that I keep coming back to in this conversation over and over again is some of the stuff here is new, but a lot of the stuff not new. Take this all the way back to the Luddites, right? None of the stuff is actually new. We're talking about a different chat GBT style interface, but we've had dark patterns and software for ten years, and a lot of that is not new.
Speaker A: I'm actually single handedly responsible for the deprecation of a large section of the Internet's dark patterns relating to consent banners. And I guess I have some insight in how that went. So nobody wants this experience, but all the site owners, they want it to annoy people to get you to do that thing. Because nobody wants their autonomy attacked, they want to interact with the thing, so they have full view of whatever thing they were trying to look at. That was your autonomy. You wanted to see a page, and we reduced it with that to get you to act. And now we have people still buying products like that. And my product, instead of like, since there's no law forcing people not to do that, is I had to make a way that actually is better, makes you more money, makes the user more happy, and you have to find balances. So my balance is I made a hacking layer that basically lets you run all your ad tech without consent, but it doesn't emit it just like captures its analytics and stuff. And then site owners, now, just that what they do with this tool is they still track it, hasn't changed their policies, but it's made it so it's easier for them to respect your autonomy. So they'll ask you at a time when you're invested, when the interaction with that site makes sense for you to actually give a shit with interacting or being bothered during sign up checkout, any of those things, and then provide affordances that make more sense now that you have more insight in the user's data.  
Speaker H: Yeah, I think that's way to think about it. You brought this up about laws and restrictions, but it really comes back to implementation. What's a better way? What if the government just funded a billion dollars for people to build AI safety startups? That's a different way. Rather than literal implementation law, which no one follows anyway.
 

Speaker B: GDPR came out. It forced people to do something that was kind of consent based and led to this really horrific implementation that might even be intentionally horrific because people are almost like kind of protesting GDPR. But if you take seriously the specifics of that implementation, you can deal with it in some kind of way. And that requires highly contextualized responses. And new laws are sometimes from 30,000 foot view. And that's why a lot of SF people are like really anti regulation. I have to believe that there are some kind of regulatory moves, the introduction of auditors, funding for a different kind of research, some sequence of accountability that will help bring about a dynamic, adaptable, humane response that isn't like stuck in stone. Because the generation that's born now in the AI world in 20, I don't even know what their values are going to be. They're going to be so different than my values right now. And I don't want to instantiate my values right now as the end of humanity because that's just ridiculous. They're going to be so different. We need some way that evolves, something.  

Speaker A: I can't contend with that I'm not sure what to do is most of these solutions that are effective that I've witnessed in the privacy field and things that I think will be effective in other fields have the unfortunate effect of complexity being added, which will make it so it's really hard to offset the incumbents. So I don't want the incumbents, but I also realize that only the incumbents can comply. It feels like. I don't know, a solution to some of this is funds from the government and all that.  

Speaker B: Who knows?

Speaker H: One thing that I kind of see emerging as a little bit of pattern here is you just shift the energy. So what I mean by that is you're not going to be writing SQL queries or doing dashboards anymore. I always thought there was actually a pretty big disservice to the job of an analyst, that your job is to analyze the data and then give a recommendation and then go analyze another problem. What about the actual implementation solution? What about actually working with the sales team and talking about who these high value leads are and figuring out how to convert them. So it's like the job of an analyst almost shifts into being a project manager for actual. The thing that makes an impact at the company. 

Speaker B: Yeah.

Speaker H: The first product I built was to give analysts the ability to build automation themselves because they're just so subject to software engineers on implementing the solution that they prescribed in the first place. And so it's like, make them do something else. So shift the energy. Same thing with his example of shifting the energy of, I forget your implementation details, but basically having people go around and creating value in another way.  

Speaker B: Carson, we haven't heard from you in a think.  

Speaker C: Yeah, I think a couple of things that I've been thinking about in terms of kind of what it means for AI or any technology to be humane is, I think what we care about more is like having a humane society rather than a particular technology, or what kind of really matters is that society overall is humane. And I think what that points to then sometimes is certain technologies are going to come about, or we want them to come about even despite some downsides. And the priority is actually mitigating those downsides through sometimes just policy, but also sometimes technologies that help to resolve or lessen some of, some of the frictions with job losses. I think if we had really great AI tutors that could help people gain new skills more quickly, that would make job losses much less of an issue.

Speaker H: Because that's the second start by D.  

Speaker B: I want to work.

Speaker H: Okay, but seriously, obviously there's a big opportunity. That's a big problem on the other side, and that's going to be a huge opportunity for someone else to build in this space. Someone's got to do it.
 

Speaker B: So a few questions and maybe related to the friction, because sometimes even like AI optimists, AI accelerationists, I've met some who are in the governance space. Governance is, besides all the complexity, there is some truth to more governance leads to slowing things down, whether that's good or bad. And some people who are really bullish about AI want governance because they want AI to move in a positive, fast way over the decades, time span and don't want a really poor implementation over the next few years that leads to public backlash or anything that kind of truly will stop this because people get scared. And the instantiated interests today, which are not AI based, even if you're an AI positive person, right, you need to bring them along. And one way to do that might be to have AI emerge at a human timescale, like not tomorrow, right? But slowly over time. You just slow shit down a little bit. But another perspective is how long does it take technologies to actually enter into the world anyway? If you look at electricity and computers timescales of adoption based on when economists identified their start, I forget exactly when, 1870s and 1950s, and you look at their timescale of adoption, they are overlapping, which is kind of weird. They're very different technologies, computers and electricity. And the reason why, at least the reason that people can put a theory on is that there are so many other aspects that need to change over time. And one thing you notice is, when electrification came about people still had gotten their investment to build their steam powered factory. They were still doing a job. They were able to do it. No one was going to build new steam factories in the future, which actually meant that steam factory builders became in greater demand because no one knew. No young person is like, I'm going to be a steam factory developer. They knew that was going to die as an industry. So the older steam factory builders had more work immediately. Their salaries actually went up. So there are very interesting dynamics at the beginning of these kinds of things, but it still took longer because people are still building vertically, which is what you would want to do based around central steam power. And it took a while before people are like, I can put things out and have many small electrical inputs and make a factory line and have efficiency gains. That took a long time. And a lot of those aren't technology specific. They're like intuition, changing infrastructure. So it's likely some of those aspects are going to apply to AI. Not saying AI is going to be exactly the same as these previous technologies. History is only so much of a guide. But it's a helpful outside view to think that maybe the world might change a little bit more slowly than people in Silicon Valley sometimes believe, because there are lots of other concerns.
 

Speaker G: So two aspects. There's the AI society, where we upgrade as species. We have new hardware, like the apple vision Pro, but as smart glasses, or even within our eyes, maybe we have robotic limbs and stuff like that. That thing can take a long time, could take a couple of decades. But the thing with AgI or ASI, like super smart AI, is that we have all the prerequisites to make it happen, either by you scaling or by new models. And then when we have that, that could propel growth in hardware and then societal change at the same time. It's a big topic, AI, and it's good to define the different verticals. And the existential risk of AI is we haven't really talked much about, but like ASi, I'm just talking about AI society.

Speaker B: What is ASi? 

Speaker G: Like Agi, like super intelligence. 

Speaker B: Okay. Superintelligence. Okay. Which.

Speaker G: Yeah. Is a bit better term than AG, which is general. You can have an Agi.

Speaker B: I'm just having, like, I want terms to be defined just for.

Speaker H: Openi just redid their naming of it to make it more.

Speaker B: Like we just can take, you know, it's helpful to take outside views where you don't know anything about the thing, but you're just applying analogies. It's kind of what I was trying to do with the other economic analogies, which would say, oh, it should take 50 years to have 50% kind of market coverage, because that's what happened with others. That's not the best view. Because we have specific information about this ridiculous exponential. And to just bring it back to. The reason I was bringing this up was because of the timescale of change, which was kind of going back to your point about how long do we need. We brought up timescale a few times about things to change such that we can kind of adapt to it. Both maybe at an individual scale but certainly at a societal scale, I think that's exactly right.

Speaker I: If we think about deep learning, just as an example, it's been around for what, like ten years now. It's not new, but it hasn't been implemented. There's so many models out there. The vast majority of tabular models are not deep learned. They're logistic regressions or they're just like basic classifiers. The reason why they're not being replaced is because the old ones worked well enough. They're making money. There's really no huge incentive to transform them right now. And in the innovation departments at big companies, they're experimenting with these things to get to about something that looks and feels like magic. That's, I think, a very big problem with AI. It's very easy to get something out the door that looks and feels like magic. 85% of the way there. That last 15% is going to take you five PhD candidates in ten years to make it actually viable in a practical, real world scenario that can handle outliers and out of the ordinary conditions and drift and population shift and all these different factors, that it's just like a continually moving system.  

Speaker B: Like, what's his name? Eric Yor. The Stanford economy. What's his name?

Speaker C: Brin Yelsen.  

Speaker B: Brin Yelsen.

Speaker I: Yeah.

Speaker B: So he'll make this point that it's very unlikely for AI to replace many jobs, including data analysts, because even if it replaces like 95% of the tasks that make up that job, those last 5% still, I mean, maybe it will reduce need, but it's still a bottleneck. And that bottleneck becomes the thing that is scarce. It's what humans can do. And so that will be like a main kind of focus area and might be the most difficult one. Maybe it's the one that you need, like the person with the great soft skills and the perspective on problems at your company, and you're still going to have to pay for that. And then they'll grow into other ways as well.

Speaker H: We actually had an interesting kind of product development story as AI code assists first make the data analysts do their job faster, then came against this bottleneck issue.  

Speaker B: Right.

Speaker H: And then started to be like, well.  

Speaker B: What would it actually look like?

Speaker H: I think our first product we developed undershot the capabilities of large language models. I mean, the last part is actually you have the query, you have the chart, and then you have the analysis of the query and the chart itself. We have GPT or vision down. You can visualize the chart actually give you a full analysis. You've changed the interface from a chat interface to be a Jirit ticket, same way you interact with an employee. So we're kind of mapping our product to literally the way that you would interact with an employee to kind of close that last 5% gap. Now, the accountability issue of this analysis, multi agent collaboration, you have another.

Speaker B: I'm just kidding there, but you know.
 

Speaker H: What I mean, you can keep going. And this stuff, it just happens. These kind of new discoveries on ways that you can chain these models together and what tech you can build around the model itself. Relatively novel ideas. It's an art more than anything. And people are still discovering these new implementations on ways that we can close that last gap.

Speaker I: Let me give an example that's near and dear to my heart. I don't know if you're familiar with Dr. Eilish's work. She did a study on an algorithm that was trying to be implemented in hospitals to detect sepsis. So patients that were especially at risk of sepsis in a lab, getting 99.9 better than a human results over and over again. And when they went to deploy it into a real hospital, 0% accuracy. Absolutely 0%. So the anthropology team went in with their ethnographies, and they talked to doctors and nurses, and they discovered that people were reading it wrong. They discovered that it was implemented incorrectly. Just actually training them on how to interpret these results was enough to bring the algorithm's accuracy up to lab conditions. But without that translation process, it was just doomed to fail.

Speaker H: That's one of our biggest issues. I'm an expert with the product, but a new user that doesn't really know the intricacies of the product, they're not nearly as successful. And so, it's a product developer's goal to abstract that away as more and more and more as you can. But, yeah, you need to still know how to whisper to the LM a little bit to have an amazing experience with the product.

Speaker B: Can we use this then, as a transition? I'm glad we've all done that, but can we move to the other aspect that you had brought up at the beginning, which is like, what does it mean to create AI systems humanely? I think that this process aspect that you're talking about and even how to is almost part of that. Like how do you deploy AI systems such that they can achieve their kind of humane aspects? But it goes deeper into its training. Who builds the systems? The kind of success of reinforcement, learning from human feedback, all of these kinds of aspects are part of humane building. So I don't know if you want to start us on this thing, but just give us a start.

Speaker I: Sure. I mean, it's trite at this moment. I feel like everybody comes around to this solution at the end of most of these kinds of conversations. But I think incorporating feedback from a diverse group of people, most importantly including the people who stand to benefit or be harmed from that technology is really important. And it's gotten to a point where there's a slogan about it where they say, never about us without us. And it makes sense because you want people. So many tools are built that are voiced onto various communities and they say, we're going to help you solve whatever it is that you're trying to make better in their lives. And then you talk to them and it's actually not a problem for them because they already have something that helps them navigate the world in a way that's good enough for them and they actually like it for various reasons. And so I think it's important to just address a problem people want solved? And if technology is the right tool in this context is an important first question.  

Speaker B: Does anyone like it's a cost right to learn about subgroups that are difficult to interview? The only companies I hear talk about actually trying to do this is gigantic companies. So I'm guessing for startups, it's not from beginning feasible. Which brings me to think like, what can we as a society do to make that easier? If that's so critical, how are people being affected not just by this one product, but by AI in general? Do you have thoughts on societal or institutional changes that we can have to make that easier to happen?

Speaker H: Sorry, maybe I misunderstood, but never.
 

Speaker I: What was it about us without us?
Speaker H: This is kind of like, at least from a founder's perspective, building a product on the market for a user, you don't build unless you do that. It's kind of like the first tenet of building something that people want.
Speaker F: Well, I think a lot of people do do it without that step.  
Speaker E: In my life never works out.
Speaker B: An example of this that maybe isn't so is like if you're building an assistant for the blind or something like that. I'm guessing the companies that succeed will be following this tenant because they will talk to their users and find out. But what if you're dealing with something like I am trying to build to go to an example you were worried about at the beginning, an automated parole thing. The buyer is the judicial system or society or whatever the affected person is, the people who are being given parole or not. And I think it probably would be easy, I don't know, but to move forward with this product, not talking to or not figuring out a way. 
Speaker H: Who's even the stakeholder there?  
Speaker B: What?
Speaker H: I said, that's a complex product. Who's even the stakeholder?
Speaker B: Well, that's what I'm trying to say. These are where AI systems are interfacing with, not just like trivial things, even data. This is way high in the space of the most critical functions that our society does. It's not like giving out housing, taking away people's freedoms. Those are harder.  
Speaker A: Should we have a governmental institution that helps mediate and create models, like open and fair models and policies about creating those models, or at least just the policies behind creating a fair model? And then people collaborate. But I don't know, something where there's funding behind this, because when there isn't, we have all these implicit things that aren't determined and are only in the private sector then. And everyone gets it wrong, because every waymo has to rediscover everything about all these thingies. It's insane. It really does help to share models, and not just models, but share public assets and goods. This comes into other things, not even AI security and privacy and safety, but like general safety right now, people are upset at 23 andme for not what you call it, not using stolen data to detect whether or not your credentials were breached. Those breach lists are stolen data and the governments don't provide this. So a lot of these things, people implicitly depend on these weird data sets that you don't know about, but the government should actually be the provider of that data set for some of them. This is a very small and really special subset is stolen credentials that everyone already has but you legally, technically shouldn't have, but need. So you should probably have a government mediating some data set policy of creation, not the actual data set itself. There's a lot of just like some organization helping, making a policy for that and then funding.
Speaker C: Yeah, there's the RMS.  
Speaker B: The question is, should we have a.
Speaker G: Good government or not? The problem with regulatory, either implementation like a government owned model or government own. 
Speaker A: Policy is I don't think they should have models. That scares me a lot.
Speaker G: In crypto we had a guy that tried to do like the FTX. Sam, he tried to do, it was very close, that he managed to get a regulatory capture that would basically make FTX the de facto exchange and make a lot of other exchanges illegal. And that regulatory capture risk is very high. Like Sam Altman, people are thinking he's doing regulatory capture strategies.
Speaker A: I agree with a lot of that, but make other ones illegal part, I don't know. 
Speaker B: Can we not get into specifics?
Speaker G: As you said, there are good examples of where you can have government services and government policies that are good. It's more, I think, how do we get AI to help us get a good election process and government, how do we make sort of good policies that people support? Good policies, stuff like that.
 

Speaker B: Can I go the other way? Which is what I was thinking about was there's not a market incentive to hear the voices of some of the people that are going to be impacted by these. Like the most vulnerable people are probably, there's probably not market incentive to hear from them. And so those are market failure modes in building a humane technology. And one thing that you can think about where the government or nonprofit sector, like the other institutions of our society are, they're there to solve the problems that the market won't solve naturally, something like that, and maybe a resource of some. I don't know how to do this for marginalized communities and hearing their voices, but that seems like the direction of solution here. And I don't know what would be the incentive for organizations to then have to like imagine government saying there's this resource that's been created by this nonprofit that we've deputized to do this, and your requirement, as all other organizations, is to interact with that resource, and we're going to audit your interaction. I'm making something up right now, but something like that where you.  

Speaker I: I think that there's a lot of truth in that. I think the trouble comes down to when you're creating an AI system. You're essentially taking answers to questions that may have really a lot attached to them and be extremely loaded. So I'm going to bring up the, I'm sure, you know, compass example. It's not even AI. It was just like a very old predictive model. Julie Angwin from the markup did the first analysis that found out, and just reading that, it was, first of all skewing for one direction or another, but it didn't even matter. So if you read the questions, you would see some of these questions belong absolutely nowhere near a paroling algorithm. Like, have you ever had trouble paying your rent and scoring? No. Would mean that you were less likely to be released from prison. Right. So that's one more example of just even the fact of identifying these risk factors. But I love your idea about, like, a nonprofit that brings together the viewpoints of lots of different groups that can then create some sort of a service that evolves over time to effectively red team the models. That's what we're talking about. And just to answer your hope that is happening from NIST, they're coming up.  

Speaker A: With a guidelines for red teaming AIXCC thing.

Speaker C: Yeah, a couple points on this. I think there's an interesting organization called polis, like P-O-L is.  

Speaker B: We were talking about it actually earlier.  

Speaker H: That's used in Taiwan.

Speaker B: Yeah, exactly.

Speaker C: So it's been used in Taiwan, but know it makes participation cheaper effectively, like democratic participation, but also potentially participation in designing technologies. And I think it's a good example of one of these kinds of defensive technologies or pro social technologies that helps to, in this case, make it easier and cheaper to get the kind of societal outcomes that we want through making the spaceship easier.  

Speaker B: I love that. I'm going to plug that. I haven't created this, but in process is a kind of larger AI, salon unconference, maybe hackathon associated with it on AI for democracy. Not AI's impacting democracy like misinformation, but like polis, like new ways of kind of participatory governance and just kind of collecting different people's perspectives. I think this is a very interesting point of how, I think there are other issues of how do you make sure that people have access to make use of these platforms or whatever. But I am so excited for this particular version of a future. I love it. Just to think about our representative democracy is not going to be like this in 200 years. I don't know exactly what it's going to be, but it's not going to be like this. Could be worse. It could be worse. Let's push our intention in some positive way.
 

Speaker G: I can give a quick kind of vision of what I have, which is basically a goodness kind of society religion, where the ultimate belief is like goodness. And machines, humans, animals, all can state their revealed and stated preferences of what is their values. And that aggregates in the Aisha's model of needs and gets basically implemented through some kind of peer to peer encrypted AI.

Speaker B: Please come to this thing because I'll plug another organization. I haven't yet. Just subscribe to the calendar or check back. You don't have to check back. At some point for this democracy, there's another group called the Collective Intelligence Project. I think they participate with OpenAI a little bit. Their whole shtick is kind of on this democracy point. And they're like, we would like to figure out new ways to use llms and AI to aggregate perspectives in human values. And the first thing we're going to focus on are people's perspective about AI. 

Speaker A: I'm pretty sure, like, 99% sure. If you try to do a FOIA request on those models that our government is privately using for these policing and judicial systems, you're not going to get data back because they're going to claim all kinds of who knows what, but by telling everyone they have to be open, even if it doesn't start with private industry. But making open models literally just for the government's own use will help for these antidiscriminatory uses where you can't tell completely how these synth problems are failed. Yeah. So they need to, like, if they don't start from open to begin with, these can never be finished.

Speaker B: This is.

Speaker I: Write your congresspeople, tell them they need to reform procurement.

Speaker A: I have an appeal to Congress on my blog from last week, and oh, my God, it's so weird. I actually got quad boosted on hacker news because they like it, but then I would delete it because I made a rewrite of it. Everybody who I gave them this new version is like, oh, it works so much better. I understand it.

Speaker B: I'm with you that I love this point about procurement in particular. And it's kind of connected with the more general aspect, which, again, I think is humane, which is a kind of transparency of our technological ecosystem. Because one thing that's nice about transparency is it sidesteps some of the difficult decisions by saying, we, at least in the United States, believe that our society should have a capacity to make decisions about what systems are impacting how the world evolves, especially in very important aspects, ways that affect me. And transparency is like a precondition. It's necessary, it's not sufficient for this to be allowed in some way. And transparency, I think, can sometimes be used as a weapon. If there were transparent government procurement of different systems, I wouldn't read them, but some other watchdog, nonprofit would probably read them, and if they found something bad, they might talk to the New York Times, and then eventually I would see it. There would at least be the components of a system that can be self correcting in a way that isn't there. If things are very hidden, I wouldn't.

Speaker A: Say verbosity equates to transparency when used maliciously. 

Speaker B: Like, it's literally, certainly.
 

Speaker B: What was something that either you're kind of taken away is a question taken away from it. If there's a question that's been ringing your head that we didn't get a chance to go, just like leave it out there. That would be helpful to hear about something that you've just been privately thinking about but are willing to share now. All of those would be great. Like, final things to leave with the group. And we start with you. 
Speaker I: When you're negotiating things like human, like, how are you going to get at that without talking about them?
Speaker B: I agree.
Speaker I: Kind of the only answer for sure.
Speaker D: I'm not an engineer. I'm mathematician, and I know how to code, but I'm not an engineer. And one of the things I always found so hard about writing code is like working with other engineers and asking questions, because obviously a lot of them are very friendly. But sometimes when you ask too many questions, they get annoyed or you ask a stupid question, you think it's a stupid question, and they judge you or whatever. So one of the things I love about coding with Chet GBT is that it just answers all my questions and it has no judgment. Right. That also applies to tutoring and things like that. But for me, the thing that you mentioned is getting rid of a lot of jobs. And I agree that's going to happen. But what I would like to understand is that this technology is also going to empower us, but me personally, to just be, like, an amazing engineer. Now, I can do all these things. This technology is going to empower us to do law, accounting, engineering, all these different industries without having to hire all these people. So I just wonder what society is going to look like with this more productive and what new kind of businesses are going to be created? What new kind of opportunities are going to be created? Or do we all just become so efficient that no one has to work that we can just go in VR all day and get Ubi and whatever.
Speaker B: I was traveling with my partner recently, and we were in Amsterdam, and we realized she had different questions than I did about what are areas of her life that are just like, blurry. We're like, when exactly was the renaissance? What preceded the Renaissance? What came right? And these things you can definitely do through Wikipedia, but the fact that we were, like, going to sleep and we put Chachi bt with a voice thing just like next to us, and we were just like asking questions and then hearing back and then said, do you have a question? And then the AI thing would be like, I do not have any question. We're like, we weren't talking to you. But it was such a wonderful kind of use of being able to learn in this non judgmental way.
Speaker G: The moderative AI is super interesting, too. We mentioned on a few salons, but instead of recording, we would have a. 
Speaker B: Moderating AI we haven't yet figured out. Exactly. Create summaries, create insights, and send those out. And I think we can do better than that. If you've gone to other AI slides, you'll see that we're not yet amazing at using that, but inserting it real time or semi real time into conversations. Like, if this was a six hour event, after these two events, rather than us sharing out, if we could just turn that group to group into groups. These are the conversation. Here are the threads. 
Speaker A: Yeah, I imagine there's literally five companies working on it, and they're all going to come out at once. So if somebody else works on it.
Speaker F: The bathroom, refresh in a minute or two, and then we're going to gather again, maybe go back to the tea.
Speaker B: Sure. We'll be there in a moment. Anyone else have last reflections?
 

Speaker I: I'll say one more thing about a single source of truth being a little troublesome because it's such a vector for censorship. If you ask a historian what ended World War II, they're going to have a very complex, nuanced answer for you that maybe won't be as simple as the dropping of the bomb. But if you ask open AI, it's going to have one definition of truth for you. So I wonder, a, who gets to decide what is true? And then b, who gets to decide is the diversity of human opinion in itself of value?

Speaker H: I have one thing that you mentioned. Didn't put a label of good or bad on humane as a definition. And when I saw the original event hosting, it's like, oh, yeah, of course, I definitely want to build a humane product. But then, like some high frequency trading, there's no interface, very little interface to human. This stuff is so technical. We do interact with it a lot, but a lot of the interfaces of this are not going to be human. And so it's just like the label of good or bad. It's like, of course I want to build for good. Humane is not automatically good. Vice versa. I mean, sorry, building it can be described as good or bad.

Speaker B: Sure. Can I just add one more thing? Yeah. You're going to wrap this up? Sure. I think about getting to this definition of humane, which is, I think there's a lot of promise to the idea of humane as something that works on the scale of human life, that has like a sort of interface that humans can interact with meaningfully and not get swept up by the system. I think that seems very promising to me. Cool. Well, thank you all for this conversation. Let's join the others.