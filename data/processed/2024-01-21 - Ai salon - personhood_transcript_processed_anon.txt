Names have been changed to preserve anonymity.

 

Speaker A: Oh, you guys have so much light. This is Leila's place.
Speaker B: And yes, the light here is very lovely.  
Speaker C: Great. Well, thanks, everyone, for coming out. Oh, great. Okay, well, then I have to say this right now. We like to record these sessions and we go by Chatham House rules, which is to say things may be referenced that are brought up here, but no one will be personally identified. So feel free to say anything that's on your mind or anything like that. And the purpose of the recordings, as we mentioned earlier, if you maybe came late. But it's to try to just capture some of the ideas that are shared and then make them interactive for people in the future so that you can have kind of an ongoing dialogue and conversation. But they don't object to that. Topic today, personhood, I think, is super interesting. And I think I was hoping that I'll just give a little, maybe 22nd preamble and then we can all kind of go around and share some of the questions we have. If you want, you can share some stuff about the background, like kind of where you're coming from in the world as much or as little as you want, just to help us understand what our shared context is. But yeah. And this topic of personhood, I think is really interesting. And it's definitely going to be at the forefront of our minds as we start to think about who do we define as people in society?  
Speaker F: So, hi, everyone, I'm Leila. My background is in software development and startups. I worked for a long time at a university, like innovation center, specifically, did a lot with startups that came out.
Speaker C: Of there.
Speaker F: And did a lot of stuff with augmented reality there. Now I just moved to San Francisco and in startup world looking and yeah, I think that. I tend to agree. It's inevitable that there will be some. If we take it as a given that there will be some intelligent life where it is ambiguous or undetermined whether or not they deserve property, personhood, et cetera, then the logical extension is there will be some race of artificial life. And then the question is not, does one life form deserve property personhood? It's does this race deserve property personhood, et cetera? And yeah, that's a holy shit question, because we're not only talking about the creation of individual units, we're talking about literally categories of new types of mind.
 

Speaker B: I have a background in psychology, and now I work in AI governance and run the AI salon. One of the things that got me into thinking about AI ethics were the concept of moral circles, the ever expanding. I kind of see this as a really positive progress of humankind, of the expanding moral circle. And most people, even some kind of staunch animal rights people, they have their own line, and maybe it's because there's a feeling that there's some scarcity of moral empathy. Like you have to draw boundaries in some ways, which might be somewhat reasonable. But I assume, just like Leila said, that at some point there will be AI systems that aren't just maybe considered people, but are deserving of being considered people. They have their moral patience, and I think that that's potentially a ways away. But I see no reason not to start priming the kind of moral intuition pump right now to think about that. And the other kind of draws on a previous conversation we had here on digital twins, which is if I'm going to have AI agents in the world that reflect me or are endowed with an extension of my legal personhood, what does that mean? How should we be interacting with those as persons? Is doing something that damages that AI twin, damaging me? How does that intersect with both our kind of moral and legal framework? So that's not a separate person, it's an expansion of my own personhood.  
Speaker A: I was particularly interested in this topic because the thing that I've been contemplating for many years is consciousness. And what is consciousness, and how consciousness, to me, what I really care about is elevating consciousness, and I see AI as being able to really do that, and then what is defined as conscious? Like will AI become conscious in itself and all those other things that we talk about. I'm actually looking at some technology beyond OpenAI, that's real time, because I feel like emotional well being, emotional health around the world is what's going to elevate consciousness. Because I feel like when people are in a place of, I'm going to say it, love and compassion and feeling of oneness with community, that's when we're going to stop. It's not even stopped, we're going to be able to transcend a lot of the problems in the world. So I'm really excited thinking, first of all, those of you heard, I really care about democratizing opportunities for people, because I think when people have the economic opportunity, that also increases their well being, and when people are secure, there's no need to fight and there's no need to take from others. So I feel like AI in particular technology has connected people like it's never been before. He literally put out an article last week about how AI are our children and that it's up to us to teach AI and teach its values and teach its data. So that, to me, is what I would like to believe. And that's why I'm excited. So I feel each of us have that responsibility.  

Speaker C: Awesome, thanks.
 

Speaker E: Hey, everyone, I'm Zara. My passion is elevating people's goodwill and consciousness and well being. I think the way that I'm working with is building a futuristic where a lot of the concepts I get into personhood. If you can have one person and one identity online, because now if you're like a russian agent, you can have a thousand profile. If you're like a company, you have a chatbot, you want to interact on Twitter already, how should be represented? And that's kind of how I got into personhood. But there's a lot of other things like how do we treat humanoids when AI gets into a human form? Right? It's like the dynamics. Are we treating those as more conscious or are we treating them differently than the model that they run and are connected to for the Internet? So how does it work on the edge versus the whole model? And I think we have a hard time knowing whether other people are conscious. There's no certainty about that. We have guesses of how consciousness arrived in evolution. And there's other theories that everything is conscious, but I don't think we will know for sure when AI is conscious or if it already is. It's a very hard problem. But we can still treat things in a particular way, even though we don't know if they're conscious. Particularly with AI, there's this probability that it is conscious, and we should have that in mind in how we treat it. I think we talked about this before, using transcripts and stuff like that, as we can interact more and more with AI, what rights should we give it, what's allowed to treat it, et cetera. That relates back to consciousness, and are we going to treat it as humans, or are we going to let the AI decide how it wants to be treated, and how do we get away from the human biases?
 

Speaker D: My last company was an AI company, and I'm working on AI company now. I also run a discussion group. With human beings, we lump a lot of things together, and I think that with things like AI, we'll probably splinter off a lot of these things. Maybe an imperfect analogy is like marriage. We've had a lot of assumptions baked into that about what people are trying to do when they interact with someone. And then now we have all of this non monogamy, and people are breaking or splintering down into, like, am I okay with having a loving relationship with other people outside my monogamous relationship? Am I okay with physical stuff? AI deserves some kind of dignity, really. I understand that it's just a model. It's just data going in and out, and so I don't need to treat it with dignity. But there's still utility in that model, having a reputation in society or like a social standing. I think of it as probably all the things that we encompass now will probably all splinter, and then we'll get really into nuance about these very specific sub things. But actually, what I find most interesting is how we will interact with AIs. And the effect on us, I think, is most interesting to go back into monogamy and non novelty cheating. Like people talk about, okay, you have AI companions. Is that going to be cheating? It's sort of like. Or abuse is another, I think, example. Like, if you are abusive to an AI should worry about that. And I think whether or not that AI has any sort of awareness or agency or anything, that it is deserving of some kind of dignity or it needs some sort of treatment. I think that the way that we interact with entities affects us. Hi, everyone. Currently working on a company which is at the intersection of AI and healthcare. And so we are thinking often, because we're building assistants that would impersonate a doctor or a nurse or nutritionist. So oftentimes you have to basically build them in a way that the person is going to build trust with them. So they will become almost like they will acquire a Persona. And the question that I always think about is, how far are companies allowed to go that path where you're going to build something that is highly convincing, that it's going to feel like a human. Humans, naturally, they treat things even when they know that they're not actually human with people like pets, they treat them like people, because it's natural for us, when you build a relationship with something, to start relating to it and treating it well. I also think that it's quite inevitable that we will become AI. AI is never going to be disembodied. I think that there's going to be emerging at some point. That's my most controversial opinion. We can argue on this if you want, but those are the lines of thinking that I have.
 

Speaker A: My name is Hannah. Right now, I've been taking a sabbatical the last year, but I've been in video games for probably the last 20 years. I do business development, partnerships, that sort of thing. But I got into games because I love the. At the time, this was, of course, the only interactive medium that was readily available. I saw it as an art form. I thought, I really want to be in this space. I want to help propagate it. I want more people involved, and I feel like 20 years later, I may have contributed a little bit there. But during this time, of course, I think games has been really on the forefront of machine learning. And kind of when we talk about NPCs and AI and that kind of Persona and digital twins, there is a lot of personal representation, especially nowadays in community forums with your online character in esports. I've also fallen in love and cried with video games characters. So that's kind of the premise of where I've been. But stepping outside of that, I took the year off to really kind of look outside of that space and to kind of go a little broader. And I've realized with AI, to me, it's been a great opportunity to level the playing field. You talked about that earlier, both from language to art to any kind of communication. And when we take that a step further, when I took that a step further, I kind of was very enamored by the opportunity and excitement, and only in November did I really give myself the chance to kind of go down the dark path. I encountered an article around NASA, and I don't know if this is real, but NASA and a quantum computing project that they were working on, and then it got canceled by the Department of Defense because the AI became so close to General AI, and the end result was kind of this doom loop of death to humans. 
Speaker D: Of that as well.
Speaker A: So I really just came here to kind of hear everybody's thoughts around that, because you've mentioned sort of AI as our children is the sum of our parts. And I think about fifth element and how Omar was like, why should I save humanity when humanity doesn't want to save itself? So I know that's really good. Anyway, so thank you for sharing. And I think that's the question, if we ever got to that point, how.
Speaker D: Do we put that in?  
Speaker A: Put that, and is there enough love? Can we just, like a million people.
Speaker D: Crank out stories about love and then change its mind?
Speaker A: What is that? So again, similar to how I cry and I love about video game characters, how does that. Can that emotion come about?
 

Speaker D: Hello, everyone. My name is Tiffany. I'm a master's student in international policy, specializing in cyber policy and security at Stanford. My research and my interest focus on the nexus of artificial intelligence, geopolitical risk and policymaking, and cybersecurity. When I think about AI in personhood, I think about it on three levels, at the individual, community, and institutional or state level. On the individual level, I think about how are we going to preserve our identities? Or how do we reflect our human identities in the ais that we're training? Because at the end of the day, I feel like ais are going to become an extension of who we are and what we want to become in our societies. So a lot of the behaviors that AI is producing, whether that is power seeking, whether it is going rogue, these are behaviors that we are seeing in society today. So shaping the AI in accordance to the identities and the multitude of abilities that we have, is something I'm really interested in at the personal level as well. I'm interested at how our perceptions of the world will change with AI, and specifically when it comes to AI and social engineering and disinformation, how the spread of deep fake, fake news and disinformation is being democratized by some of the risks of AI. And how will that not only change the way we view our institutions, but also change the way that we consume information? And then on the institutional level, I'm interested at the intersection of AI and agency. So who is governing today? Are we still going to be the leaders of our own communities, leaders of our own institutions? How do we reconcile potentially a leadership aspect of artificial intelligence and the institutions that we have created that have allowed for a time for stability and peace building around the world? Are these institutions still valid today? Or are we reimagining a new structure that will allow for us and AI and other powerful actors, not only states, but also private companies, to come into the fold and to have a trilateral cooperation to rebuild structures that have inclusions for everyone? 

Hi all. I'm Chen. I start to think about the concepts of personhood, know as a concept between contemplating between the perspectives of individual communities. Echoing off what Zara has been saying, I think the only intelligence, the only consciousness that we can prove is ourselves. And by kind of considering how we're extending this concept of personhood towards others, towards a larger community, we're constructing it through the feelings and emotions that we're feeling, that we have constructed towards the interactions with other people, with other entities as well. So I think kind of more in the thoughts from what is going on in terms of the expending moral circles as a community that we have right now. It's really interesting to think about how as a community, how kind of extending outside of the core agents of proving the intelligence, how we as a larger community aren't defining what we regard as other intelligences, other persons around us. And I think one kind of controversial kind of thought that I have going a little bit fast forward in time is that when I think about AI governance, I think something that I always think about is how the co governance between AI agents and humans will look like in the future. How I think in a lot of ways, I go in back the points that have been made previously on how AI is elevating our own consciousness, how in a way we're looking at Aisha and Luis like the snowballs revolution in the sense that we're co governing with not, you know, placing ourselves within this conceptual framework where AI are the imagined colonialists that we're looking at in our society. I think that's kind of a controversial, but at the same time, a kind of framework that I'm always going back to is that how we're extending our own conceptions of our own personhood, with the concept of digital twins, with the concept of something that's elevating what's within our own boundaries of the thoughts of have we are from the same program at Stanford. I do an international policy. I started off my career with disinformation, misinformation analysis, and then I moved towards effective alchemism. Kind of did a startup on the donating platform that's modeled upon giftvall in China. And I also have touchwater on decentralized media when I was back in China, trying to make content of political information more accessible to people.
 

Speaker C: So I came after you two. I'm Kofi. I work in startups and AI, and I have more questions and answers today. When I think of personhood and consciousness, these are questions led up to lawyers and philosophers, of which I'm neither. But I think I'm primarily interested in a kind of very tactical approach. I think I align with other thoughts in the room around this is inevitability. So what does that look like? So when we ask Agent GPT to make money and it does something illegal online to make money, how do we govern that? Where is the liability and where personhood begins? I don't think necessarily needs to start with the idea of discussing on a philosophical or consciousness level. We have lots of unconscious, non sentient things that are people legally. How do they fit into that? And I think the interplay between personal responsibility and relationships with these systems is really interesting. Again, whether or not it is consciousness, people will build emotional attachments to it. So where does the precedent, or, sorry, the legality around AI human relationships really feed into? 

Speaker D: We'll save you, don't worry.

Speaker E: My name name is varsity. I currently work at an AI startup. More coming from a product manager background, so I guess my angle is also more predictional, rather than thinking that, oh, there are so many things that can happen and how do we prepare for those stuff, since it's more or less an inevitability and one issue in person that I think is most interesting. I think one unique thing is that if AI progressed as we currently predicted, to have intelligence, to even have free will, then it's the first time ever that we as a species, as human being, created somehow a different species or kind that have never existed before. Of course. For example, we have breed all these species of dogs, cats, that wasn't exist in a while, but it at least come from those species that were in the nature before. But this time we're creating something that's brand new, that's out of our own construction, or the computers, or the machine learning algorithms, but we also don't know how exactly that happens. So then how we deal with the relationship with AI, do we really see them as another species? Do we see them as something of our creation that we are trying to control it? Do we see it as something that's external, that's not part of the nature of the current world? That I think would be something that everyone may have a different view on. That's something I would really love to hear more about. And I kind of think that also determines how further into the future call AI would impact the whole society. It would determine the fate of even larger, like the earth, the planet, the civilization. 

Speaker C: So everyone was going just a couple of questions on your mind, context of AI and personhood, and if you want to give background of where you're coming from, the world too, you can. But one thing is we do record the conversation. No personal information is captured, anything. And we actually just use like an AI agent to summarize points and takeaways and make a little blog post about it. So if you're okay with that.

Speaker G: Thank you.

Speaker C: Cool. 

Speaker D: So I will go next.

Speaker C: Did you.

Speaker D: Okay.
 

Speaker G: Hi everybody, my name is Amina. I'm currently a master's student studying human computer interaction, but my undergrad was in sociology, so I'm very interested in these topics. I currently work as head of user engagement at a creator economy startup, and I also do some work with the Plurality Institute in San Francisco, which studies pluralism for technologists. One of our key research areas is like AI and governance AIA, and I guess just in society we also do some stuff with blockchain. So this topic is really interesting to me because if you think about the modernist spirit and postmodernist spirit, I feel like it was humans really trying to understand personhood and understand what is it to have a sense of self? What is it to develop a self outside of the collective? And I'm not sure if we've really answered that question yet. So to think about it in the perspective of AI is very interesting. It's something I'd like to hear more from all of you about. Just, I guess, future predictions. I kind of wonder how much we will merge with AI and how much AI will just become a part of us. If you think about our phones, right, they're separate, but are we really separate from our phones? The Internet used to feel like a place, and now it's just ubiquitous in our society. So I kind of wonder if it's going to be the same like that with AI agents.
Speaker C: I would suggest maybe so everything from consciousness and the nature of personhood to the development of community, inclusion in pragmatic or social roles, and then up to the governance of states and governments. So maybe we can kind of start from the low level and work our way up to the kind of meso community and then state level, and that'd be kind of cool. I thought maybe if we can get consensus on one thing, which I think is kind of an implicit question, a lot of things that people brought up here, which is this thing about consciousness and personhood. And maybe the question I have in my mind is, do we think to be a person in society requires that that entity is conscious? Or are we okay with there being non conscious people? Right. And what does that mean, really? And I'm not talking about humans or whatever, but this notion of personhood and what we extend it to pets and corporations and organizations or institutions or whatever, at least pragmatically speaking, can they enter legal agreements? Can they own property? Can they have make decisions on behalf of themselves or others or whatever? I don't know. Just kind of open the discussion and I'll hope to moderate anyone. There's no order speaking order. So whoever has a strong opinion on this consciousness being required for personhood, we.
Speaker E: Already have two categories, which is natural people and legal people. People could have artificial people. We don't know what artificial people are. Are they conscious or not? We could treat them as probably. They could be conscious. We shouldn't abuse probably conscious. 
Speaker B: Yeah, I like this point, which is, in society, we have at least already crossed this chasm and said, clearly we don't need consciousness to be a legal person. At least we've defined that in some way. I think this kind of relates, I think Anne brought this up of the decoupling of some of these concepts, where when we can see something that can be an agent, take in stimuli and make decisions that seem to be goal directed, and that make actions in the world, and potentially have that separated from consciousness, that makes it difficult. Before, maybe we didn't have to make as much of a decision there, I'm guessing that we're going to have to be able to move in a direction where we treat things. I guess I put more moral weight on things if they can suffer than if they're conscious, which maybe is a necessary presupposition. But I don't know if anyone has, like, a philosophical background. I kind of would love some more concreteness outside of kind of the liability legal perspective on personhood to be able to tackle this question.
Speaker D: I think that discussing whether it's conscious or not is only relevant in the context of building a relationship with it, beyond the legal relationship, which is usually governed by a contract. And that's not that interesting to discuss. I think the really interesting thing is where we start to relate to things and we build relationships which are more emotional. And so, as you said, whether that thing can suffer because of your actions is only important. That's the content within which consciousness is important, actually assessed.
 

Speaker D: Yeah, so we were saying whether we should consider consciousness and personhood together or not. And I'm saying that the context is if we want to discuss how we enter in relationships with agents, for example, or anything else that is not human, and we want to, should we treat them, do we think of them as a person in the context of consciousness or not? Makes for us only sense when we want to enter in emotional relationships with them.

Speaker C: Really interesting point. I'm going to argue against it a little bit. And it's because of the things people brought up earlier, which is that these are reflections of ourselves. And regardless of whether it's conscious, who would you be as a person to enter into some relationship with this thing and then not treat it with dignity? And what does that reflect about yourself? And how does that put a lower bar on your own conduct and your own humanity? A dog is not conscious, or an animal is not conscious, perhaps, or something, or a flower or a plant, but it's still a thing of beauty, and it would still reflect badly on you if to destroy it or to denigrate it in some way.  

Speaker D: Yeah, for sure. But I think the question here is, if I speak to a dog and I don't treat it as a human, nor I think of it, nor I think it's conscious, nor I give it a Persona per se. And so I think that treating things with kindness is different from building a relationship as with a person.

Speaker B: I'm trying to figure out what the overlap is between consciousness and personhood and morality. Leila, it seems, is making an argument that there's outside of consciousness, outside of the thing that I'm interacting with, there's a larger set of things that are kind of maybe worth being moral patients because it reflects on me. Maybe this coffee table here has some moral patience, because if I just destroyed it, that would be. I don't know. 

Speaker C: I think that science word moral patience.

Speaker B: Would have, like something deserving of moral consideration. 

Speaker D: I think going back to the point of the intersection, I think it's about empathy and rationalization. I think there's this saying of treat something the way you would want to be treated. So going back to my point about how AI could be an extension of who we are, is if we're putting out systems into the world that will interact with the fabric of our societies and our communities, we want these systems to be shaped or to be optimized the way we would want to be treated. So that's where I come from. And I think that something that characterizes personhood is both rationalization, our ability to contextualize, to analyze, but also empathy. And there's this balance between both that would be really interesting. 

I am sort of against the idea that we should call anything a person that isn't a human being. I think that it was a bad idea to do that for corporations. It was just expedient in the law to do that. But I don't think that that was the correct way of handling that situation. I think that a person is a human being. I think that these other consciousness, if there is a new consciousness or if there is just. We have AI agents and they all have different features and they all have different. Some of them we should treat kindly because it matters to them how they're treated. And some of them are just data in and out. And we understand that clearly. I think that's all just sort of like, these things should have new names, is what they're saying. I don't think they should be people.
 Processed Transcript:

Speaker B: Can I ask a question, just so if an alien, or even if there was still neanderthals around conscious, they can interact with us, are they not people because it's homo sapiens?
Speaker D: Well, I think it's a helpful distinction. Right. It's helpful to have a distinction between homo sapiens and neanderthals. 
Speaker B: To me, the distinction is the biological word, homo sapien, or human. And personhood as a concept is intended to be across that chasm. Because if there was vulcans or whatever, they would be people. They're definitely people. I mean, I feel like.
Speaker D: Yeah, I mean, this is semantics. It's just, how do we want to define this word? But for me, I think it's helpful to just keep this word for people. We're coming upon things that we have not encountered before, so let's have new names for them. That's sort of my position.
Speaker F: But then by that logic, don't we still need a word to be the generic concept of personhood that we're discussing to encapsulate all these people? Because we have a usefulness using.  
Speaker D: Do you have one?
Speaker C: It might be useful to distinct when we're talking about human beings as we know and love today as a human, and take personhood as this more abstracted concept of, like, yes, we're calling neanderthals and humans both people, but it'd be humans and neanderthals. Just for the sake of clarity.
Speaker D: Yeah. Now that we're arguing words, it would be really helpful to have a shared definition.
Speaker A: No, I just think we have to add in a lot more definitions, like what you were just saying. There needs to be a lot more names. Right? And as we're saying this, what came to me was actually to determine the love based on the sophistication of capabilities. So what I mean by sophistication of both thinking and feeling. All right, because the reason everybody loves their dogs and cats, because they're pretty high on that. Whereas nobody has an, like, they might have an ant farm, but they don't have like a single ant pet, right? So as we're creating all these things, is we're going to be creating a lot of AI agents. So some AI agents going to be very sophisticated, like what you're saying.
Speaker D: Or maybe they feel, who knows?  
Speaker A: They could be made to feel, they're going to be smarter than us intellectually, brain power wise. So I think we need to start defining, like I said, the guidelines that I think makes sense. Depends on the sophistication. I'm going to call humanity the sophistication level of their humanity. 
Speaker E: I thought for a few definitions just quickly. And I think the thing that came up best is if it can make a statement, if it can use natural language to express itself with means. I think that would be. Yeah, because in my definition, dogs are not people. They are animals.
Speaker A: They're not.  
Speaker E: But corporations can make statements. They can use like, they can use language.
Speaker C: It is people behind the corporations.
Speaker D: So on the point of. Very interesting point on sophistication of humanity. It reminds me of Descartes. I think, therefore I am. And then it all boils down to what do we consider as thinking? What do we consider as intelligence? I think that's also the void of it, because you can have an LLM or an AI model input, output. You train it on reinforcement, learning, or wherever. It just spits outputs. But at what point do you consider a machine or an AI or any agent as intelligent? I think there's some question around creativity and your ability to analyze. I think there's analysis and creativity there as well, that humans have that I think really are a big chunk of what an intelligent agent is. So I really like your point on the sophistication part. So many layers to consider.
Speaker A: Yeah. Like there's actually a book called Power Versus Force. All right. You know what I'm talking about? He actually has put in levels of consciousness, right? So that few of you know what I'm talking about. Right? So how about if we start putting in these levels and new names and all that beyond what we're talking about to define? So it's not just simple personhood, corporate personhood, human beings.
 

Speaker C: I think one thing that's kind of emerged is that there's probably a distinction between consciousness and intelligence. Where intelligence, we could operationalize as to some extent your ability to navigate an information and decision making landscape for the accomplishment of some stated objective. In that sense, an ant colony has a degree of intelligence in its ability to navigate resources and collect things and whatever, despite the fact we don't think it's conscious. So consciousness is this self reflexive awareness of the present moment or something like that. One thing too, in modern media is the new name for aliens is actually non human intelligence. Okay? So again, there's some agreement that who knows? If they're conscious, they're probably intelligent. So I'd suggest we kind of have this as the split point and then maybe move away from personhood as a philosophical category, but more get pragmatic on to what degree of intelligence or agentic behavior do we afford things like decision making ability, perhaps legal rights in some sense, the right to exist, the right to compute.
Speaker F: But by this logic, people in coma have neither consciousness or intelligence. 
Speaker C: And that's a really good point, and I wanted to bring that up. Thank you for saying that, because it is an existence proof for the concept of personhood, requiring neither one, because they are still legal people, they still own property. The will is not enacted yet. So clearly personhood does not require either consciousness or intelligence. But I would suggest more pragmatically, because consciousness in the syllipsistic sense is impossible to determine from the outside. We just think about intelligence as a qualitative thing. We applied to something's behavior. Does that seem reasonable to people?
Speaker D: It sounds like you're talking about rights, but what should have rights? 
Speaker C: Perhaps? Yeah, but in the pragmatic sense of.
Speaker D: Like, because you're answering the zone of like, do they have exactly the right to vote, the right to own, we said own property, but beyond going that participate in society as real humans is being treated as such. So then the question is, where do we want to? 
Speaker B: That is where the rubber hits the road down.
Speaker D: This question.
Speaker C: I think might be on the same page. That's what makes these conversations interesting to me, are like the details, because we can always go back to semantics of what is a person. We've been argued about this for all of the time we've been human. This has been a recurring thing. It's like kind of drilling into these specific questions that are coming up. I think I can really ground it in perhaps a use case I have in mind where I have a very competent agent and I actually want to delegate some amount of decision making authority to its behalf. And it could be just making purchases on Amazon or something like that. Something simple to start, and maybe in the future it could be responding to emails or whatever. Right. So, pragmatically, I want to be able to delegate some of my decision making authority as a social actor, as a person to this intelligent agent on my behalf, because I think it can intelligently navigate something on my behalf in a good way. Then what was your thought?
Speaker F: I was just going to say, if the purpose of this was to define personhood, and we've decided to get more precise by separating that into different words, I think we're missing one, because intelligence and consciousness doesn't cover the entire thing. And are we going to be able to cover the entire thing? No, but if we're trying to figure out what personhood means, and we need more precision, then at the very least, we already know there's big gaps. So can we, at least to a certain degree, add in another prerequisite for personhood other than intelligence and consciousness? And what do people think that is?
Speaker B: I think one of the things happening here is there are no natural classes. Natural kinds don't have category boundaries.
 

Speaker B: The nice thing about panpsychism, when people are like everything's consciousness but just different degrees, a rock has some degree that's reperceptible on something, is because it's more likely true, because most of the world are continuums, right? And then we just divide them up because that's practical. I'll say, when I made this theme, what was in my head as a specific example is like, kind of the her like idea where you have these other agents, which, as far as anyone could ever tell, with some solipsistic wall, are people conscious, intelligent things. But if we were to enter into navigating the distribution of scarce resources, let's say, with this other entity, we want them to be part of our democracy. We want them to be like, how many of them are there? They can be created infinitely so clearly, like concepts, like one person, one vote doesn't really make sense anymore. So then you can disenfranchise this entire group. That doesn't sound really positive to me either. The way her resolved this, if you haven't seen it, you should definitely see it. But resolved this in a pretty simple way, is like, they leave, they're like, you know what we're not going to coexist in the same area. This is human person area. There's another digital person area. And that kind of like sidesteps the real issue that we're going to get into. I think at some point where we have to interact, we have to have relationships with, not just legal relationships, but relationships where it's like this thing wants certain things, I want some other things. And we want to compromise and interact together and merge them into our institutions that help us do that. And I have no idea how to navigate that in a moral or legal sense.  

Speaker D: That's why I was thinking that it cannot be disembodied because it cannot exist in the same planet with us. It just has to be either embodied and occupied energy and spend energy, and he treated as a person.

Speaker B: My grandpa uploaded his consciousness to the cloud, has no body anymore, and he's definitely a person. 

Speaker D: Okay, can you talk to him?

Speaker B: You don't have to read.

Speaker D: I also think you're raising a core institutional problem, it's like the one person, one vote is like, we have built these structures around certain foundations that have worked for us because we were in a different era. It wasn't an era where we had rapid digitalization, interconnectedness. Whether we like it or not, we're living in a completely interconnected world. Your privacy is as private as it is universal now with data. So I feel we will reach the point where we're going to have to rethink the institutions that are governing us because we have new actors that might want to hold up space in governance. So I think that this is a core question of are the current institutions that we have today ready, and are we ready for that type of interaction? There are two use cases. A, you consider that they are separate people, part of a population or community, and they also have a right of representation. What does that mean? Going back to your point about votes, let's say we're in the US system and they're voting. How are they represented in government? One, two, you consider them extensions of your own self. But then you have the issue of what if the AI agent goes wrong or hallucinates? What do you do in this case? Will there be a metric for you and system to come to a middle ground? I think it's an institutional problem.

Speaker B: Hannah, you were going to say something.  

Speaker A: On these points around, at some point it feels like a threat, right?  

Speaker D: And that's kind of what we're talking.
 

Speaker A: Can you have a conversation with something?
Speaker E: I guess also, it's like how AI eventually would develop free will. And what that free will looks like, is it like the AI operates similar way as we now have, that each of us have individual free will and our desires are different, or will to say, oh, we as a whole AI. Because if they're all connected by the same algorithm or whatever, and they decide.  
Speaker B: That you can imagine the AI coming back and being like, oh, you think you're all, I see a large cluster of you that basically engage as one large organism and then another one.
Speaker C: One.
Speaker B: Thing that maybe is worth bringing up, personhood doesn't mean you're completely protected in some moral shield. And this happens all the time between nations, right? There's a whole country of people. We acknowledge their people, and they're pushing forward some value and they want to grow and because they believe in that value and they want that value to extend into the world, but we don't agree, right? That's not our value. I want a different value, and I want that value to grow into the world. And those two things can't happen, so I'm going to destroy that value. And there's never a stasis completely there's just like temporary stasis. I would be astounded if in 500 years, the nations of the world looked like the nations today. That would be crazy. Crazy if that happened. And so that competitive pressure always exists. And I agree with you that there's this aspect of personhood, in a way, to maybe respect or value or allow some of our intuitions to apply or something like that. But that doesn't mean that we're not in a potentially competitive relationship with these other people.
Speaker E: I think that's a really interesting point that you mentioned, the analogy of countries at war with each other. I think there is a sense of what is us and what is not us. On a smaller scale, us is just ourselves and extend to families, extend to communities, cities like nations. And when we currently, we don't see all of us as like a uniform human community, because there's not that external forces, but AI could be that external forces. Then we come up with the new concept of, oh, this is us. This is entire human race, homo thinking. And then there's the other category, which is like AI. 
Speaker D: But that's actually scary because we're like, very good point. We're already living in a really fragmented world. So AI amplifying social dynamics, power dynamics, fragmentation, let's say, to the point of collective action, I think. But imagine AI has the ability to retaliate or protest against certain structures. And what do we do then? I know it's very pessimistic to think about these things, but I feel like from an extension risk point of view, we can definitely see a cluster of AI building their own community and just increasing fragmentation. And that goes back to the point of personhood. I mean, are they considered to be a community like they consider us to be a community? How do we define community? Because community is formed of people at the end of the day. So how do we define this cluster of AI agents that also want to be part of society?
 

Speaker C: I think another frame for that problem isn't necessarily to try and answer it now, but what's like, for example. So this idea of personhood has changed over just the last 500 years. So you go back to 500 years ago, our definition of person and just the people in this room. It'd be like, oh yeah, three of you are people. If you're in the US and everyone else, like, you're not. That was the political system even exist. It was like Europe. Women didn't have any voting rights until the earliest, was like 1600 Sweden or something like that. This idea of being a person has already changed over time, but I think a species wide failure is building a system in which allows for change to happen. We don't know what these problems really will be in the next 100 years. And I'm curious, but for those studying these types of problems, what the discussion is around on a systematic level? Not necessarily. Here's the answer, but here's some principles or operating procedures we can use to create a better outcome. 
Speaker G: No, it's okay. I was just thinking about this. On the topic of threat as well as building systems, I wonder if it would be beneficial to train them to have more feelings and to be more empathetic. But then that could also put us in two situations where one, I don't know if it's ethical to make something work for us, if it does have feelings. But then two, on the other hand, if it doesn't have feelings or if it doesn't have empathy, then how is that going to change its engagement with humans?  
Speaker A: I think those are the kind of things we're going to see as we go into it, right? It was funny because somebody was saying something about community. I thought, oh, are AI is going to make friends among themselves and build communities among themselves? I really thought that's an interesting.
Speaker D: Right? I think two points that I want to make, especially off your point, is that I think there is this research at Stanford by a PhD student called Junkim. So he's making this research upon just basically AI agents interacting with themselves. So it's really interesting in the sense that you can see them developing a so called relationship as from our observation from outside, and just to see what does it mean for a separate parallel society versus how it can be in the sense of what society means for human society at the same time. And I think that's like a really interesting direction that people have already been exploring. And I think it's a really interesting kind of futuristic idea of what we think it would mean. And I think the second point that I want to make is that the discussion of personhood really matters, only matters for the people that are within this person who to talk about. Because in a sense, by recognizing the commonality, we're trying to posit this idea of what actions are justified towards the entities that are within this commonality. And I think that's really interesting in the sense, like you said, a lot of the entities are not considered as person a few hundred, a few thousand years ago. And then right now we're crafting these ideas of personhood because by labeling who are a person, we're trying to direct our actions towards these entities. And I think it's really interesting in the sense that, for example, going back to the ideas of, like, we're directing personhood towards rivers, we're trying to direct protection towards these rivers. And in the sense, when we're trying to label AI agents as persons, we're considering them as threats, as potential agents that can bring harm to us. And then we're directing our actions towards them by labeling them as something that's close enough, but not quite us.
 

Speaker B: In that thread, we can actually completely. We don't need to worry about consciousness and anything, right? There's an identifier which organizes ways that we. Relational rules, like ways that you can engage. Some of those are very concrete in the legal sense, but the concept of personhood in general is to try to give ourselves this abstraction that lets us engage with. And so I think it was like PETA or whatever tries to, or some environmental, applies it to rivers, or tries to say, this monkey is a person. And that way they can bootstrap off of the intuitions we've developed by. I find it kind of interesting that when we say, will the AI develop relationships amongst themselves already? Maybe the concept of a community is defined by its borders. A person, like a self, must have some borders. I'm sorry, I'm forgetting your name. Barkley, you brought up how many AI is it just one thing? And I guess maybe there will be a future where there's at least some kind of borders. They're like, I have certain information in me, I have these documents that were uploaded to me. I have these experiences that I've had. I've interacted with Ian and Tiffany, and I know certain things. You don't know that. And those are the borders of this AI information. Now, those borders might break down. We might have the ability to be like, we don't want to be separate people. Let's become one people, one person. We'll merge. But unless you do that, and that's like a flexibility that AI have, but before you do that, you do have legitimate borders. And personhood then, might be something not only advantageous for us to interact with AI, but maybe it's actually advantageous for AI to interact with us. Maybe there's a benefit in a future to be like, I need to be able to interact with this other set of constraints in the world, this other intelligence, this other institution. And this concept, personhood, is this signifier that determines a lot of what goes on here. So I, the AI system, want to understand that concept and engage within the constraint of it. If I want to be able to interact with human society, maybe it'll be beneficial.

Speaker D: That makes sense. I like that. Going back to the practical example, you want to give this AI ability to do things on your behalf. So just spitballing, like in different software, you have an administrator. That role is given. They can do anything they want, administrator, and then you have different roles. I'm just going to reference meetup. So it's a meetup software. Then you have someone who, all they can do is create an event. All they're allowed to do. It's security clearance level. You're allowed this access, you can affect this, you're granted this ability. And so what if AI is based on model? Maybe there's regulation about model has to be this to have this many no's, has to have been trained on XYZ, and then it gets ability to have role. It can sign certain contracts or something, like there's tiers. And as AI gets better and better, climbs its way up to where maybe it can have this administrator role, which sounds the same as what you saying with word personhood, which I disagree with word personhood, but maybe that's what you saying is you having administrator role.

Speaker C: I like the practical, little anecdote from history. The Roman Republic was incredibly effective at incorporating conquered territory into its expanding reach. And one of the ways they did that was newly conquered towns and cities, there was tiers of citizenship, and they would start at lowest tier with highest tax burden. And then over time, over decades, as they prove loyalty, satisfy obligations and don't rebel, they get to point where they become citizens. And so this was alignment strategy of incentivizing behavior.
 

Speaker F: You hit on exactly what I was going to say, which is, I am very conflicted on this because I like this. I find it inevitable that we'll have to make different classes of what we are calling personhood for AI. I just think we're going to need to, because consciousness, all these words we've used to describe it, will manifest differently and at different levels. However, once you have tiered a system of personhood, it's very difficult to then look at the collection of humans that exist and say, well, this human is at this machine's level, so why isn't this human getting that machine's level of personhood? And I am against any type of tiering of that for humans.

Speaker D: The way that things are going, with the speed things are going, it's going to happen so fast that they will have no time to put any tiers on capabilities of AI. It's just going to happen. And they're going to be great at so many things at once. And they're going to be indistinguishable from humans in capabilities. And potentially you'll probably give them capabilities to also feel. Because what is feeling? Feeling is translating sensory information into concepts in your brain.  

Speaker B: We're going to divine personhood in the kinds of errors you make, not the kinds of capabilities. Do you mess up in this human way? I know. So I'm saying I'm going to put my definition of personhood attached to the kind of errors you make.

Speaker C: So we're going to incentivize weaponizing competence? 

Speaker F: This differential by that logic.

Speaker B: This was a joke.  

Speaker A: I want you to continue so this is all going to happen so fast, let's say five years from now.

Speaker D: If an AI has embodied a character with a history of experiences and background, and basically behaves as that for as long as beyond a human life, with the emotional and intelligent capabilities that a human has in the long run, but choosing to be one thing and behaving like that one thing, instead of being everything at once, which is what it is capable of, then maybe in that context, that is something that can be treated as a person. By that logic, all the things about being a person, and even maybe they can be like, what if they make a bad decision and they need to be put in jail for something to take away their property and things like that?

Speaker F: So based on that, could I train an AI on all of Hitler's decisions, make a Hitler AI, and then immediately throw that Hitler AI in jail for its entire life because it was trained on someone I hope everyone believes would need to be in jail.  

Speaker A: First of all, what's an AI jail?

Speaker F: Great question. I was thinking of robots.

Speaker C: We just have one.

Speaker D: They will create their own jails and rules. They will create their own control because they cannot exist outside of the boundaries of human society and within the relationship of human society. The AI that misbehaves could be controlled by the AI community itself and put into controls and jails to ensure that the relationship with humans will continue.  

Speaker F: We better give them great values.

Speaker C: Lots of people want to talk.

Speaker D: If that scenario happened, how could we face that? There are new studies attempting to self destruct models. We're talking to Elizabeth Economy about this - the Department of Commerce is going after this, especially with high risk open source LLMs that could be weaponized. Potentially self destructing models is one solution, but you might reach a point where the AI outperforms that solution, and that's where it becomes scary. I'm saying you're still thinking humans can control AI, but the reality is that we will not. If this continues, AI will be way more in control of itself and us.
 

Speaker B: Can I ask a question about, or like, ask for expansions on one of the ideas that I think you brought up here, which is for a long time, for most of technological history, we have tried to be able to do kind of more like the constraints on our behavior were imposed by our inability to control the world, control physics, whatever. So constantly we're pushing forward, and what can we do? And we might be entering an age of, like, where do we need to set constraints on our technology so that they are human centered, human like, that they are analogous in some way with the timescale that humans exist in the ways we think, the things we care about, because something that can make decisions, a billion decisions at milliseconds, might be beneficial for humans, but it's certainly not human scale. 

Speaker C: I'll just do a little running speaker list, just kind of get my. But I think you were next. You were make a point before.

Speaker F: Yeah. I easily see a future in which human capacity also begins at some point, some degree of exponential growth. And so I think that the other question we have to ask is, one, at natural rates, what ends up growing faster? Can humans compete with AI naturally? Or do humans have some AI compartment, I. E. Mental math becomes a solved problem for humans instantaneously. Is it that humans have a ceiling even mixed with, when mixed with AI? Or is it that they both can grow infinitely? And then we need to figure out, not our machines, persons, but what the hell are these things that people are becoming?

Speaker D: I kind of wanted to just point out to a dangerous kind of sus that I feel like it's kind of also inspired by this discussion over the growth and the constraints of this growth on AI capabilities. I think one thing that I find really kind of concerning, if we actually include AI agents, or like, the development of AI agents into the discussion of personhood, or would be that this logic of optimization, when we're talking about the capabilities growth within AI, I think when applied to the growth of persons, or the growth of humanity, or the growth of just human communities in general, I think it's dangerous just in the sense of how we're looking at, what are the benchmarks? What are the kind of factors that we're talking about? Optimization, for example, when using artificial intelligence and intellect and embryos for the next generation of humans, I mean, that's already going on in the sense of, like, what are the standards that we're using to choose the next generations of humans? I think it's just the fact that it's good that we talk about intelligence as a concept that's contemplating between the tools that we're using versus the people that we're interacting with, the humans that we are interacting with the most, which is ourselves. But at the same time, we can optimize our tools but we can't always just optimize ourselves. That's just like the thought that I have.

Speaker A: You know, what he was talking about is human becoming much more intelligent, right?
 

Speaker A: So there's certain spiritual communities. I mean, if any of you have been around spiritual master, they seem to be totally psychic about everything. Right? So how do they have that capability? If you've been around certain spiritual masters who are psychic about everything, whatever information is needed at that point, they have access to it. So some of you experience that empowered voice versus force talks a little bit about this human consciousness thing. All right, so it's interesting that you pointed this out because, like I said, it's very esoteric. But I do follow certain spiritual communities who are talking about an exponential. Yes. All right. Because most of it scientifically, they've proven we hardly use most of our brains. We only use a very small percentage of our brains. Now, why is it not turned on so far? All right, so what if we're able to turn that on? So these spiritual communities are basically saying they're doing certain things like mass meditations, which elevate human consciousness. And part of the elevating human consciousness is also elevating our intelligence. So this is like, hardly anybody talks about this. Very few people follow this. Very few people know this. But it is all along those lines of what you're saying.
Speaker C: I'm working on something there. 
Speaker A: You'Re working on something. I want to hear about that.
Speaker D: I think there is a very big question around incentives. There is incentive, at least from a lot of stakeholders around the world, to come together to a table of negotiation and say, here's the problem. We are aware that AI, innovation is crucial. It is a growing delta. But we are also aware that there are risks, and these risks will also be a growing delta. So how can we preserve the size of innovation by also tackling risk, and I don't mean high level regulation, but I mean setting standards, setting metrics, setting some sort of regulation and policy making to allow that we are still able to preserve the fabric of society and the questions around personhood by allowing for a controlled innovation of AI. Because I'm not arguing against allowing this innovation to continue, but we also have to be aware that we are in a moment of time where we're at an inflection point, where we have this new technology that has been present for a long time, but has really boomed and taken wave a year ago with chattypt. Right. We really need to harbor this incentive and bring the international community together, which is already happening with the OACD, with the c two PA, the UAIF, and just be able to kind of understand where we're going from here. That's kind of where I tend to ground myself a bit away from the existential risks that are to come potentially more towards the immediate risks that we're facing today and that we're going to be facing in the next 1020 years. So how do we bridge between both?  
Speaker C: A danger of relying on the humans to change and react to these new challenges is just like the last time you really saw AI at scale, which is social media. And that's one of the hardest addictions to break. People spend so much time on social media right now. There's no level of human higher consciousness that can effectively, at scale, impact that. Until we're in the homo data side of the world where biotechnology and AI have integrated, and we have an AI as a part of our own consciousness, I don't think humans are the answer. And then to kind of combine these two points, this inflection point, it's getting to a point that it will be developing faster than we can go on. This importance of building incentives and building some system that we ultimately recognize that we may be in the reverse, it may not be that. Whether or not we're debating AI consciousness, but AI debating our consciousness, how do we make sure that doesn't hold us? Seriously?
 Processed Transcript:

Speaker C: If we think about, is this monkey conscious? We're talking about whether or not animals serve rights. If we're two steps higher than a monkey, but an artificial superintelligence is ten steps higher than us, it's easier for us to empathize with a monkey. You look at orangutan in the eyes and it's eerily human. There's none of that happening with AI. So at a policy level, how do we not say this is a course of action? Because we don't know what those capabilities look like, but how can we develop principles like, we should probably start going the other direction?

Speaker E: I think the current largest challenge is even as a society or administrators, we don't know what is the goal that we should try to achieve. The whole AI alignment problem is to make sure AI doesn't go beyond what humans are asking them to do. But will that be the case? How do we ensure that? The fact that we're here talking about AI personhood, if we follow the AI alignment principle, then the personhood question or risk doesn't exist because they're just still will remain to be our tools. But most of us here don't think so. 

Speaker D: Right?

Speaker E: Most of us think things are going to get out. So there's all these different senses, people arguing for strong AI alignment, people arguing against it. People say we shouldn't restrict AI developments. All these different incentives are so hard to align that we don't have a universal goal. 

Speaker D: I hear you and I think you have a point, but a counter example would be there is incentive. State leaders, tech company leaders and advocates have been talking about the safe development of AI. And there's a question around what that means. But if you look at the AI executive order, there is a push for some framework, not to regulate, but to set standards around how we move forward. I'll give you an example. Let's talk about the marketplace, okay? AI is making the development of deepfakes much easier, lowering talent and cost barriers. It's happening now. This is a real-time risk that sways an election, elects an authoritarian leader, causes social disruption. So in that scenario, there is incentive for democracies to sit together and preserve how things are running today, given we are satisfied. 

Speaker A: Right.

Speaker D: So I think there has been, maybe not efficient, but some movement around thinking about these questions. But you made a great point.
 

Speaker C: I have a quick anecdote. So I would suggest looking broader in historical context, because I think part of the AI salon is that it touches on so many parts of society. And I would suggest looking at the industrial revolution as the last time. A fundamental shift in the mode of production uprooted every aspect of society and actually challenged notions of personhood. And what I mean by that is, prior to that, people lived in small agrarian communities where they had ancestral rights to farm the land and ancestrally inherited obligations to pay a certain amount of tithe or taxation. It was the feudal mode of production. Industrialization comes along and things are mass produced. There's the enclosure act. People are evicted from their farms, they congregate in cities, they now have to engage in wage labor, they're renting their time, they live in rented homes, all this kind of stuff. We are grateful it happened today. We are grateful to not have lived through it, because it was a massively traumatic social upheaval and dislocation of established ways of living that led to massive losses of life and horrible working conditions, all this kind of stuff. And that really was an inability to think through what the second 3rd order impacts of this new mode of production would be and failure to anticipate those changes.  
Speaker A: Yeah.
Speaker G: I'm so glad you brought up the industrial revolution. My sort of historical anecdote I was thinking about was the printing press and the Protestant Reformation coming after that, and this big social upheaval in the western world. I think it's just the responsibility for policymakers to make it as much of like a soft transition as they can make it.  
Speaker E: I like to think of the fact that we had nuclear bombs for a very long time, and when they were new, it was very easy to think we would nuke the whole world and it'll all die. Same with AI now, right. It's going to be developing a lot faster than humans. I don't think that it is humans that are slow to evolve. I think we are quite quick at adapting new behaviors and changing our lifestyles, but our bureaucracy is very slow. But if you look at second world war, for example, you look at the difference in output 1949 versus 1944 in America. Those five years were extreme change with the war production board. And it seemed like if we decide to really tackle AI, it could go very fast. But the main thing is the short term things. I'm really worried about this year's election in terms of all that means for AI and data privacy, 1984 could happen in America. If America turns to dictatorship, what's the short term thing? Personal AI is like one short term thing. The other one is mass generated content. How can we regulate that AI and what that is as a person allowed enough to do? Can we give regulations on how companies what their algorithms can be?
 

Speaker B: Can I jump in here just for a moment? One thing that I find very interesting in the last half an hour conversation is we've moved away from personhood completely, right? We're now in an almost more real politique way of talking about AI, right? There are power dynamics. There are ways that it's going to influence our world, and what can we practically do to either constrain the influence of AI, constrain the perspective of different actors and maybe personhood. This concept is a privilege. It's a privilege to talk about when you've solved the main issues. And now you can think about personhood, lest we fall into the sometimes described as the guerrilla problem, which is gorillas were very strong, and they are nothing to us, right? They're nothing. And so we don't want to be gorillas to AI in the future. And so anyway, I guess that's just one aspect I'm really noticing here, how little interest, in some ways, there is in the personhood of the AI system. And then occasionally there's this appeal bill left, but to this transhumanism idea, which I think I'm relatively on board with, too. I just don't think that's temporarily going to compete with AI agents, their own evolution. I think the AI human hybrid will be more like the iPhone. Right? We can hope that we'll get to a positive future where we are augmented by AI tooling and we'll have plenty to deal with in trying to deal with our institutions growing for that world. But I doubt the AI in our brain, like the full symbiosis, is going to compete there. And just as one final point about institutional growth, in reinforcement learning, there's like a learning rate, right? You get new information from the world, and you use it to update your model of the world. How fast. If you're really fast in updating, every time you get new information, you completely change over. So, for instance, if you are like, doing what's called, like a multi arm bandit, all of the slot machines, which is a classic reinforcement learning task. 
Speaker C: The.
Speaker B: Very fast learning rate would be every time you pulled a slot thing, you got a reward. 100% of the time, I'm going to get rewarded there. You don't get a reward. 0% of the time, I'm going to get reward there. And so you end up like ping pong back and forth. But if the difference between the slot machines and you want to learn optimally are like 66% and 65%, you need to learn very slowly and integrate over time. But if the actual underlying changes, too. So it's not 66% of the time, this one gives me reward. 65%, this one gives me reward. But those probabilities are actually changing over time, over some time span, such that for these ten minutes, this one is going up, and for these ten minutes, I hope everyone's following this kind of analogy. The optimal learning rate is dependent on the volatility of the environment. And some people defend, like, our slow moving democracy. You say the congress goes so slowly, and some people say, that's as designed, right? It's good for it to move slowly. It's not a chaotic system, and it will slowly move towards correct kind of policies, which might be true, but if the world becomes more volatile, the world becomes more chaotic and changes faster. There is no way the same institution with the same learning rate could work for the. There's no way. And so I agree with you that the institutions need to evolve in some way. That's the only problem. There are so many other things people talk about in AI governance. This is the only problem. This is sometimes described as the pacing problem. The difference between our wisdom growth and our technological growth and wisdom is not self compounding, exponential growth. Technology is. And so I don't know if there's a solution to this, but this is like the only problem to me.
Speaker E: There's another positive thing here, is the difference between the industrial revolution that we couldn't think through the second Ferdor effect. Now, I think the wisdom is actually accelerating much faster than the problems, thanks to, like, we all probably spoke with chat gypsy about what we're talking about. Same. We could speak with the chat gypsy about how to solve our. I love bureaucracy, et cetera. 
Speaker B: So you're saying optimistically, actually, the tools we're building that might get out of are also the solution.
Speaker E: It's not the same as the industrialists, because now we have, what we're lacking is intelligence incorporated, but we have intelligence. 
Speaker B: I like this optimistic case.
 

Speaker A: I love what he said. Because when you were talking about how the last big AI was web two, where we got all addicted to all this stuff, right? The thing about humans is we're very practical, all right? If like, within the next ten years, food that tastes like sugar comes out, that actually enhances us and make us healthier and makes us smarter, we're going to eat that kind of food, all right? If like within the next five years, energy comes out, that's much more efficient for us and that's cheaper for us, we're going to do it. The only reason we don't do good stuff is because it's not convenient. Humans are lazy, we live in patterns and all this kind of stuff, right? So I do feel like exactly what you were saying about, first of all, with AI, there's enough people working on positive solutions in food in every arena, okay? So it's not even like trying to get people to become more ecology oriented and all that. They're just going to want to because it's cheaper and it's better. The same thing with food, the same thing with health stuff. There's probably going to be health stuff that's going to stimulate us and make us healthy without having to exercise very soon.

Speaker C: I noticed markedly that whenever I spent time talking to GPT four about this versus going and reading the most recent papers over the last three years. I got so much more valuable information from reading the papers than talking to these systems. I don't think it's just about that. Oh, we have these tools. Who will be wiser? Just because we have better tools doesn't make us wiser. We have safer, more efficient electricity. It's called nuclear, yet we still primarily use that carbon. I think we're fundamentally not rational. I think to your point, we are lazy. We like what's easy, and that is. But the economic incentives and personal lived experience incentives don't align with wisdom. They don't align with us saying, okay, to be like a better.  

Speaker A: So throwing wisdom into us was not appropriate. So thank you for catching that. From an economic and lazy point of view, I do feel like we're coming out with solutions that is going to make things more economical and make it accommodate our laziness.  

Speaker D: I thinking about the personhood scenarios, is there any example or a case where they were able to war game scenarios of AI in kind of incurriating some aspect of human like abilities? Because I think that would be very interesting, having some war gaming simulations on how would an AI be able to coexist in society. I know that there has been a lot of instances of war gaming, but like for wars and politics, et cetera, but for the very future of our society, I think that would be a fascinating thing to see because then we'd be able to put into context this conversation and see, okay, practically, how would that work in different scenarios?

Speaker B: What do you mean by war gaming?  

Speaker D: Here, wargaming is like a simulation of an AI that's trained to do something specifically. So think about. I'll give you another example of, I don't know, the South China sea escalations. There's a lot of wargaming that's happening on kind of what would happen in different scenarios.
 

Speaker B: Yeah. Are you saying like scenario planning for AI is of a certain form and incorporate? I'll give up one example, which isn't explicit scenario planning, but it's an example I like anyway, which is the future of Life Institute. A number like it was one of kind of an inspiration for this group did this world building contest where they requested people to just imagine positive futures with AI in 2045. It had to go well. What had to have happened for it to go well? And what is that world? And one of them had digital nations, and they actually started thinking about digital nations. This group of three people, I think they're mostly in the global south, these three people who came up with this thing. But since then, there are now some digital nations, as certain islands that are anticipating being washed away due to global warming are like, how can we preserve some aspect of our national identity? They create these digital nations. And what's interesting about the concept of that is that you could have these non embodied digital citizens and their AI. I forget why they chose to do this, but they imposed limitations on themselves so that they can engage with the other citizens of their nation, humans, on their timescale.  
Speaker D: It was actually a TED talk at AI this year. He was basically showing how you can simulate this behavior. And they were all given characters, they all had lives and friends and certain limitations that are only specific to each and one of them. And then you see what they do, what they would do, actually, if you just let them be that thing. But in essence, it's really like not each and one of them is not AI, but it's an instance of it. We are an instance of a human consciousness, each and one of us. So each character is an association of that AI capability with constraints, with history. I guess that's the way to go about it and see how that could look like. And I think people will be working on that because they're already working on it. And it's pretty good mentally right now, but it will get somewhere for sure. It will enable simulations for us to look at.  
Speaker C: I was going to comment briefly on this war gaming, the introduction of some new thing to society, and seeing how things evolve. We can't predict the weather more than one week in advance. 
Speaker D: There is a company that does that extremely well.
Speaker C: Let me finish with the point. Society is so impossibly complicated. How could we predict, simulate accurately war game accurately? So war games makes sense because it's a limited context, with known incentives, known capabilities in terms of real estate technology, very clearly defined goals and outcomes. This is a completely open world. I would think it's like the only way to approach this is actually through federalism, right? Which has historically been the case where what is the best way to self govern? And so United States has done well, I'm canadian, by the way. But it has 50 small experiments in self governance, right? Europe is another example where you have many small experiments in different national constitutions and so forth.   
Speaker D: But you can simulate people really well because people are so predictable. People are just so predictable. So much like alike each other in cohorts.
Speaker C: You think you can predict society, you.  
Speaker D: Can predict certain human behaviors and math.
Speaker C: Yes, society, not the whole evolution society. Think about what I'm asking. It's an easy answer.
Speaker E: It's a butterfly effect. It's a butterfly effect. Like Sam Alfman marries someone, and all of a sudden, like a hypothetical scenario, butterfly effect changes everything. Of COVID for example.
 

Speaker D: I think I maybe missed explaining. I don't mean using war gaming as a means for prediction, because even if you think about it in the military, war gaming, no one knows if there's going to be an invasion of Taiwan. It could happen in 100 of ways. It could happen tomorrow, it could happen in 20 years with different technology. What I mean is like, just using it as a means for readiness and preparedness, just as a way to say, okay, these are some scenarios that could occur, how can we prepare for that? But of course, it's not for prediction. And I totally agree with you. We cannot predict society. It's just a flush of diversity. To another point that I think we didn't really talk about is the point of culture is related to personhood, is like, are these AI systems going to embody culture? Can we say this AI is from Lebanon? How can you feed an AI a certain culture or are these AI agents going to have a culture of their own? Because I think a big part of personnel and humans is their cultural piece and their background. So I'm very curious about your thoughts on this culture piece. Where does it fit? I think something that I really want to add this conversation, I feel like it's kind of like a shift of paradigm as we were talking about this, is that I'm just going to present this as either a challenge or something that's like an inspiration. What about one person community? What about one person culture? What about the stuff that you fast forward into? I don't know how long into history, how long into future? But just like when Apollo talk about network state, he's talking about still something that's built online. It's still like a few persons are contributing to this concept. But what we have possibly we can just have a person, one AI agent, another AI agent, a few AI agents together with you, and then you're like, we can probably create a culture here, we can probably create a community here. And then the ultimate goal isn't just about human preservation anymore when we're talking about personhood, in order to define what persons should be justified to do towards other entities around the world, but in the sense like, what if I'm just creating a new species that are me, or a new culture that are just me, a few other AI agents? We're preserving this community as in this Sci-Fi world.
Speaker C: I don't know.
 Here is the edited transcript with the filler content removed:



Speaker B: One thing that's brought up sometimes is often in this biased perspective. You don't want an AI system that's like, this is the world, right? Any single answer is going to be biased, right? Because an answer to almost any interesting question has a million truths to it, and you can give many answers that are true. So how do you sort through all of those truths to be the ones that you want to reflect? When someone asks you some question and there's an insane amount of context that we bring to that question in a particular environment, and I'm going to call that set of context that determines how I act culture. And it's an effect on me. And so how are you going to get to a place where you want an AI system that doesn't just say, wow, there are many answers to this question, and then list 10,000. That's not useful at all. The way chat GBT tries to do it right now is kind of like that, but they list like four or five. Right? That's how they try to deal this. But still, there's always an implicit sorting of these kind of things. And it probably will be more relevant to just recognize that we have different cultural contexts. And when I say, how should we treat our grandparents or something, you probably want to know more about me than just that sentence. To be able to engage with me in the way that I actually want to be engaged. I'm Japanese, I'm 80 years old, or I'm jewish and I live in San Francisco. All these things are relevant, and a person would engage with me in that way, they'll kind of get some sense. That leads to information bubbles, and there are problems there where everyone has their own truth, but I think there's clear market incentives to just have a better experience for me, for the AI system to be able to understand my culture and therefore embody that. And why wouldn't we then create AI systems that are kind of. I can expect that they'll interact with me like a lebanese person would. And why wouldn't Lebanon want to make sure that such an AI power exists in the world that is an extension of lebanese agency and is interacting the world in a lebanese way? That must happen, I think.
Speaker D: Can I follow up with a question? So assuming that can happen, you would need data to train that AI? I feel like culture cannot encompass culture and data because culture is not only the food, the history, but it's the community, the relations of people, the way of life. So how would you include all of that essence that forms culture into an AI agent?
Speaker B: That it's just a simple example. Right now we have Internet ingested, and that's not all of human communication. Right. There's a lot of times that we interact with each other in person, we talk to each other like we're doing now. This is the information that I believe will move us towards now. Still not all of culture, but imagine everyone's Always conversations are just recorded everywhere. That is the kind of data that, whether consensual or not, is going to be somewhat recorded. And I feel like that's a future state that we can expect. I don't think we have the information right now, but we have the written work.
 

Speaker C: I think there's a danger when we're talking about culture to make it a model. Even this idea of these two essence of culture, we typically think of culture, food and art. But you ask an italian how to make a red sauce in Sicily, in Rome, Milan, you're going to get three totally different ideas. So then, maybe we have it regional. Okay, well, you talk to families in the neighborhood. It's just like this idea of culture and trying to build culture into an AI system. Until we're all doing story cores constantly, we are feeding this hyper localized, overflowing, ever growing stream of data into these consciousness. I think there's a danger for us to try and imbue this into it and then just kind of develop stereotypes. And there's also this danger of putting a culture or personification of a culture into one of these AI systems and then just letting that into the world, because then like, oh, this is my lebanese AI bot. I'm going to learn so many things about Lebanon. Okay, that's terrifying because just say you have prompt injections, like different attacks you can't do on current language models. That's not going away anytime soon.  
Speaker D: I think there is a lot of potential in it as well because then you get to enable cultures around the world be brought to people and they get more closer to them and understand them. And we can start learning and respecting each other more with our differences instead of creating separation because we just don't know, we don't understand.  
Speaker A: What these cultures are.
Speaker D: People have so many biases and predispositions because of lack of information.  
Speaker E: I'll join your side of the battle. I'll set it.  
Speaker A: I think we should not get into it.
Speaker E: We judge AI by what Chat GPT is today. And I think they just launched a few weeks ago that it's going to become more personalized. And if you add it that it can you also, which they turned off by default, but they have cross user data sharing and you can look at lebanese IP addresses. The output is always going to be like text or audio and that's what the input is like. It can learn from, say like Facebook GPT could learn from all the chat messages and it will learn how to do cultural text output or audio.  
Speaker A: Yeah, but it's going to do all the pictures and all the audios with IoT everywhere. Going everywhere. Definitely. Within five to ten years everything is going to become recorded. So it's inevitable.
Speaker E: Leila, I think one thing, when you were gone, I said the difference between switching topic completely, it's like the difference between the industrial revolution and its upheaval on society and you couldn't predict the second infertile consequences. Now we have instead basically an intelligence acceleration, right? And we could like, we judge AI by what chattivity is. And yes, it's probably better if you want wisdom or depth of knowledge to read a paper than to read chat GBT because it gives you the executive summary. But AI is much more like the next. We're going to have expert GPT, we're going to have wise GPT that looks at depths of the paper and gives you even deeper knowledge.   
Speaker B: To dystopia or personhood is not the most relevant of risks. Not every AI is the honest.
 

Speaker D: I think it's also interesting how there's always this power structure behind the development of AI. Who controls the direction of optimization? Who defines where these AI tools are optimized? There are a certain class within the society exclusively being responsible for guiding the directions of these AI agents. And it's not the bottom 20%, the bottom 50% defining these technologies. I think this power structure translates into more power imposition upon the community not defining AI.  

Speaker C: I think there's two frames about AI. We're in the midst of the AI revolution, like the agriculture and industrial revolutions. AI is a really powerful tool to write, copy, and learn faster than before. But AI also has the potential to evolve into consciousness in a way we've never considered with tools. We toggle between how to treat AI revolutionizing industry yet also the evolutionary implications.  

Speaker B: An anecdote about Adam Smith - he loved talking about pins because he observed pin factories. He said when you industrialize pin-making you can now iterate faster than humans learning over generations. Abstracting production away from slow human learning to fast building and iterating has led to exponential change in 300 years versus no change for hundreds of thousands of years before. We're now maybe further abstracting away from humans, but it's along the same exponential curve.

Speaker D: You can't separate it.  

Speaker B: This power abstracting the means of production and now thought from humans to something optimizable not by evolution has been the sea change from stability to exponential change.  

Speaker C: Free market capitalism developing powerful AGI, too. Don't we trust that process to find the best outcomes?
 

Speaker D: I just want to come back to the point that was made earlier on the IoT point of, in a few years, we might get to the point where the conversation at the dinner table and the conversations everywhere are collected. And I think this is a very big concern personally to me, because that relief infringes on surveillance. And I think that when we think about surveillance, I think this is where we lose a lot of the personhood part. We lose agency privacy, interconnectivity. And going back to the point of having an AI and body culture. I think that we also use the human connections that really define parts of who we are and parts of what our society has made. So I would like to push back on that point because I feel like this is where we should not reach. This should be. I think there's a quote that says it's taken in another context. My liberty ends when I'm infringing the liberty of another. So I think this is where I would personally draw the line between having my every move monitored and having that trained into an AI. I think this is where personhood gets a bit shaky, and this is where I think we lose a big part of our personhood as people.  
Speaker C: Cool. I think outsiders.
Speaker G: Oh, I was just going to say, I think that's an interesting point, and maybe this has come up before, but I wonder how much of that is to do with the systems we already have in play rather than the technology itself. Because I'm kind of afraid with the policy around AI causing degrowth and halting this innovation. And I don't know, just curious to get everybody's thoughts about that. 
Speaker C: There was a great phrase I heard a while ago, which is that AI is a tool in a lot of ways. And tools don't replace jobs, they replace tasks. And definition of a job is always a social construct, what we choose to call a job and choose what kind of roles we want to have in society. One note, those time I thought, let's couple minutes, everyone kind of reflect on a conversation and share other big questions that are open that could be topics for future salon discussions or cool takeaways or ideas that you had that were like, oh, I think about this differently now, or no, I'm even more convicted than my belief, sir, just like last kind of 1015 minutes here, we just kind of more reflective on that kind of stuff. Anyone?
Speaker D: I think something that also you mentioned about the intrusiveness when we're talking about this because it was kind of inspired by what I was looking at today. It's like a new product called Rewind AI. I'm not sure if people have heard of it, but it's basically like something that records what you do, basically everything that you do on your laptop. And it helps you in a way that it reflects your memory when you're trying to find something. Like you close the tab and then you're like, where is this thing? Like, I wanted to find it. So basically it's intrusive. But in a sense, I think going back to your point on human laziness, on the so called economic convenience that people are seeking over, I feel like just to also dwell on the concept of inevitability, I feel like 15 years later, 20 years later, when we're talking about the same concept of personhood, we might just have a very different view on this. Like, what agency would mean is probably not the same thing as we're talking about it right now. Is it good? Is it bad? We probably don't know.  
Speaker G: Just like how we're defining personhood right.
Speaker D: Now is different from how we define personhood a long time ago, but in the sense how intrusive technology is actively reshaping how we view our relationship with ourselves, how we view our relationship with other people. We have friends who use Chechuki per day. What can go wrong? You would describe your own personal decision making in very different ways than how we have been thinking about it all alone. When we're delegating our tasks out, when we're delegating certain our agencies out, we are potentially, like you said, we're potentially decreasing the amount of tasks that we think our personhood can deal with. But at the same time, are we going to come up with more tasks that our personhood can be dealing with? We never know. Do we have that optimism in our humanity? I feel like that's a good kind of distinction point where that people are going towards in the future direction.
Speaker E: Maybe you must touch on long term philosophical aspects of personhood and AI. Well, what I was more interested in when we started was the short term personal AI touching that personhood. But I think one insight I had I didn't share was even when we would merge with AI, I think before we would merge with AI, we'd marry AI, which means there would be the AI and there would be me, like the humans, there's like me, you, and us in a relationship, right. And there would always be that personhood. We would not merge with AI, would marry AI, rather.
 

Speaker B: Yeah. To me, these are like the optimistic worlds where, for instance, for surveillance, two things come to mind are, one, how different different cultures values are. It just shows that cultural value, like they are not objective, they're not inherent, even like privacy as a cultural norm. I'm sure the generations born today with AI will have a completely different view. At the same time, I believe there might be ways for us to have rewind AI or our own personal way of recording the world around us that helps create an AI that is an extension of me, or is to use the marriage thing is something that is. 

Is meaningfully talked about as this group with somewhat aligned incentives, somewhat shared knowledge, like me and a spouse, right? We're not, we're two people. But there's another real way of talking about us as a unit that engages with the world, that has incentives as a little group. And the more empowered we can make individuals to have access to their data or control their data, there's no inherent limitation. I see, for us each to feel massively empowered like gods compared to who we are today in our ability to synthesize, in our ability to engage with other people. And that might live on a platform of giving up some previously held values, but at least revealed preference so far shows that people don't give a shit about a lot of supposedly held values. Anyway, one of the things that I came in here with that we didn't talk about, and I'm not sure if it is actually that important, was the kind of like EA inspired idea of the moral patience of AI systems that can feel pain and are being put through reinforcement learning by the trillions and by training them are being subjected to torment that is absolutely beyond anything that has existed before. And so whatever society we create is resting upon this platform of just unknown terror that we are propagating against conscious. And the reason why I'm saying that maybe it's not so important is because I still think there's some importance there. But there's so many other components here that have been brought up that really push this question of at least AI personhood or moral patience independent of the legal relationship. Like how is society going to work with this shit down a bit? And I think I can appreciate that perspective more now.

Speaker D: I'm glad you brought it up because I think it's probably going to be the easiest practically way to create the ability to empathize with humans and also behave in ways that are more human like. If you actually expose an AI to that training data of feelings and pain and everything, and then actually heal. So that would be the cheapest easy possible.

Speaker B: You remember when I tortured you for 10,000 years?
 

```Speaker D: Don't do that. Exactly. I guess that sounds existing, that you are going to start treating it as a personal way. I think an interesting assumption that is kind of going with the moral patients, like the expending moral circle as it would be deemed by a lot of the EA philosophies, is like the moral standards. I mean, even if we talk about AI personhood, the moral standards that we're extending towards them are human moral standards. And how much of that is compatible with AI? How much of their moral standards, if they can actually come up with a moral standard, will be applied to themselves and also in their sense, like extending to us. I think that's an interesting kind of perspective just to think about what is the moral standard that we're talking about here. We're talking about human centered AI, because in a way we're extending our human values. But if we do in a sense give AI the very essence, the concept of personhood, as we think of them as a moral entity that is equal to ours, do we give the same moral treatment towards them as how we would treat another human, or do we give them the same moral treatment as we would just slowly develop a new moral standard for them? I think that's interesting for me to think, but I don't have an answer, obviously, in reflection to this conversation. I think in general, to me, what was highlighted was that as we think of AI in those two directions, of like AI in service of us and AI itself becoming a thing of its own and regardless of our own development, be the thing that is going to exist in self, the whole notion of personhood, then, should be also in those two different contexts. One is how it relates to us and deals with us and behaves with us. And the other is in what are the personas that it has in its own world, the world of ais, and where they're going to have their own rules and their own communities, let's call it. So it's a little bit of a crazy idea to think about, but it's likely to happen because people who work in the research of AI, they're divided into two directions, which is one, we have to have AI alignment, we have to make AI work for us. And that's the direction of where most society thinks like. But there's a lot of people who are actually thinking, no, we need to just develop AI for its sake, for the sake of its existence, just exploring that direction of ideas and thinking. And there's a market law that says anything that can happen will happen, so it's very likely that will happen as well.
Speaker C: There's this book called accelerando, and one of the plot points is that post technological singularity, these ais basically form just their own society, and humans are just totally uninteresting to them, and their society is incomprehensible. To us. And we're just kind of like, it's like there's a really cool party you're not invited to, and you're like, oh, yeah, okay. I guess they just kind of went off without us, and they'll go colonize the galaxy, and we'll be living on earth and wondering what it's like.  

Speaker C: Another reflection is similar to a short story called the lifecycle of software systems, written by Ted Chang, if y'all are familiar with him. Really great. And kind of the core conversation is around AI systems that develop consciousness for purely companion monetary purpose. That's the vein of this conversation we haven't explored too deeply.  

Speaker C: But, yeah, it's opened up a lot of questions. As always, more.  

Speaker A: Can I just say my final comment?  

Speaker C: Oh, sure. Sorry.
```

I've removed most of the filler words, repeated phrases, verbal tics, tangents, and excessive elaboration while preserving the core ideas and flow of the conversation. Let me know if you have any other suggestions for improving the transcript!
 

Speaker A: I feel like AI is going to help all of us to become superhuman because I feel like they're just going to have extraordinary intelligence on both, like, mental level and emotional level. So with that supporting me, I feel that's going to help me to become superhuman and our host society become superhuman. And I also feel like what's happened throughout history of humanity, there's the black hats and the white hats, and with the technology recently, there's all these people doing the bad actor stuff, and then there's people developing things to counteract that bad actor stuff. So I think that's what's going to happen also with AI, there has been a lot of bad stuff happening, but we're still here. The whole world hasn't blown up. So to me, that means my data has never been stolen or anything. So I feel like there's enough white hats that's going to keep everything okay. And I just think if most of humanity in both the developed world and developing nations now can become superhumans, to be able to create and create the kind of world that we want, I think it's going to be pretty amazing. And that's why I'm optimistic.

Speaker D: I find your point very interesting. I'm more of a skeptic around these issues. I come from the developing world, and I feel like we don't even have an understanding of what AI is, at least in my country and in a lot of countries in the developing world. So I feel like even the discussion around identity and personhood are still very big questions in society that we still have not solved. I think that the lingo and the impact of how AI would be integrated, if it would be integrated, and when, will vary from society to society. I'm still reflecting on how far we should go and how far we should push the needle.  

Speaker E: I summarized from our conversation today is that everyone's perspective differs. And I think it's largely on how we view humanity or whether we are more optimistic and personalistic about the humanity itself that determines our view on the AI. Because eventually it's kind of like in this new body problem that people diverge into hoping to salvage human beings or hoping for them to take over and dominate.

Speaker C: Any other, not responses to these comments, but reflections. Otherwise, I think I have to end the conversation. Okay, thanks, everyone. 

Speaker B: Thank you.

Speaker C: Yeah, totally super fun. And then last point of order is we host these every Sunday. I'm hosting now a conversation on Twitter spaces at 07:00 p.m. On Wednesdays. So it's like more people can join, but there's, I think we just have, like a few people will sign up to speakers and then the rest, the audience, because it's hard to have who wants to talk next.