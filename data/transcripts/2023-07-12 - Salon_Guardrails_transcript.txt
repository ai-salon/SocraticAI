Speaker A: ASA community a little bit more these days. So I've just been also thinking about the social impact of this, like, in different verticals and, like, different social problems.
Speaker B: So, yeah, that's that's just my starting point. So love to hear. I've been to that talk yesterday with.
Speaker A: Oh, yeah, the GitHub CEO.
Speaker B: Yeah, I saw the demo for Nvidia.
Speaker A: Yeah.
Speaker C: Awesome. So question for you and everyone all. But before I want to just let people know I record these conversations. The point of the recording of these conversations I'm starting to record right now after the introduction is to transcribe and share out not the takeaways in some kind of necessarily opinionated way, but to potentially share out the actual conversation that we have here. So just keep that in mind. That is something that's happening here. I will make sure that people are anonymized and I don't mention individual names and those kind of things, but just so you know, there's rules of engagement.
Speaker D: Of Chatham House, right? What it means that the rules of.
Speaker C: Engagement are oh, yeah, that's a good if people aren't familiar, Chatham House rules are you can talk outside of here, including with the writings about what is discussed, but not who discusses them. And yes, totally. But there's actually a recording going on, so I want you to be aware of that. Andrew. Sorry, what were you going to say?
Speaker E: I was just going to ask I don't know, how would we like to open the circle up to take up more of the space and we can all see each other? Or do we like the at the table kind of environment?
Speaker C: This is just I'm going to keep us here, do not just nice and cozy. Yeah, it's a little hard to get totally figured out, but we can talk about that later. So thank you for bringing all of your perspectives here. I've heard a few different themes, and I kind of want to start with one that you touched on, which is this idea of predictability versus empiricism when dealing with guardrails. So before we dive into specific guardrails or things that people are thinking about, I want us to bring to the surface some of our underlying assumptions about what we can deal with. Sometimes perspectives that people differ on are like, what is the timeline that I'm caring about? Safeguarding that's one perspective. Another one is, do we believe that we can predict the future in such a way that we can create policies that do what we want? Or do we have to take an empirical approach? And I think there's a lot of reason to. I mean, these are these are kind of fundamental in any other decision we make. So I want to start us there with just that general theme. How predictable do we actually think AI systems evolution is, or the effect of any intervention, whether that's policy or technical is going to be?
Speaker D: When you ask these kind of questions. One first we take is to turn to the past. See did we predict pretty much where we're going to be? My opinion is pretty much yes. The scaling laws have been out there for years. All attention is what you need. Paper 2017 it's just that the future was never, was never not evenly distributed and some people did not believe in it. But four years ago it was quite clear we're starting to compute at these things. So the pattern was established and from GPT-3 release we knew that GPT Four was going to be something quite exceptional. So not everything can be predicted, but there are certain things that in a way could. And so we can extrapolate that towards the future and ask the question what can we expect from Gemini, for example, and the brothers and sisters in terms of these beings like the market makers in some big techno industrial choices?
Speaker C: So I'll try and when people bring phrases that maybe other people aren't aware of, I'll try and just define them. So Gemini is a system that's being developed by Google and we expect them to basically connect the large language models that OpenAI has really been pushing for with the kind of reinforcement learning approaches that DeepMind has used for AlphaGo. And so we expect it to be more agentic in, I would think slightly more unpredictable ways than the scaling laws. But just to make sure everyone's on board with scaling laws, that's been a thesis that OpenAI has been pushing since basically 2016 or so. And they run with it. They're like, this is working. And there's a paper by Anthrop called Predictability and Surprise and LLMs and this kind of summarized the impact of this predictability. And one of the reasons why San Francisco is so hopping right now is because investors can be assured that if they put in some amount of money, they're going to get a system. So, anyway, Nico, you're bringing up that, for instance, when we think about video or we think about audio or we think about all of these other modalities that sometimes come up, it's like, oh, what if AI it's like it's going to of course the crank is going to be turned and applied to these other modalities. Those aspects are going to be brought in. And it wasn't difficult to predict this.
Speaker D: Except the margin, except what we call emergent capacities. These models are deep black boxes. We started to understand from GP three type model, size of model, that they were on the fringes, capabilities that could only be materialized by trying, prompting, testing, including the agendic nature of these agents. And so there is this duality between what could be predicted and the level of resolution of what we can predict and the level of what we cannot. And there is a debate over are these emerging capacities real or are they really like is this something that is part of the not the design but part of the inerrant ability of the system? Or is it that they are being developed or revealed by using the system and mobilizing the system in different ways? They're getting the question.
Speaker C: I think these are pretty important ground ground pieces of information. So in the same paper, predictability and surprise, there's this aspect of predictability but there are these emergent capabilities that seem to come out. Now, it's possible that these capabilities already exist in GBT Four, for instance, but are hard to extract. Or for GBT Three because the right prompt wasn't engineered or something like that. So it may be that these systems already have certain capabilities but it's hard to get them to represent it. Or it might be that they don't have the capabilities. Either way, we don't see certain capabilities and they sometimes emerge unexpectedly. And in the existential risk world, some capabilities that people are worried about are like self propagation, for instance, or deception, like intentional deception. There are certain kinds of capabilities that people are worried about. But anyway, this is an analogy.
Speaker E: Because.
Speaker C: I sometimes hear people who are less in the predictability side to not point to the, what did you call it? Like information isn't distributed but we were aware of it and could have had the capability of predicting. People who are less bullish on predictability will point to other analogies like the Industrial Revolution or other transformative technologies and how this is a big theme of San Francisco, right? Too much regulation will stop innovation and we have no idea where things will go. And so we have to keep guardrails off because we have no that. Does anyone want to represent that perspective here.
Speaker F: Just to build on this? One of the things that I think is hard about building out guardrails is the sociological kind of nexus of these artificial intelligence systems and what's predictable about them or their capabilities or their scale relative to how they'll come to contact with culture and human behavior and how those things will create more emergent and unpredicted outcomes. So it's hard for me to sort of be like well, if we presume everything stays static and the same, then we could predict a set of outcomes as a result of these technologies being in culture. But because culture is always also changing, then you need something that's actually a lot more like bamboo that's going to change and move as culture kind of adapts and ingests the nature of these products. It's interesting to think about how Chad JBT maybe we call him Chad in our house, chad for short. How Chad is maybe losing some steam in terms of usage and adoption because the novelty in some ways is both wearing off and the advantage of using Chad is not as good as just asking your colleague or cheating on a test normally or something. But then there are some people who will use Chad really iteratively and constantly gaining advantage in a way that the average number of people are not willing to put up with. Such that there's kind of like the Gartner hype cycle is kind of going all out of whack because language as an interface is so commonly understood by most humans that there's essentially a hype curve happening kind of on the one level that's super fast and then it drops off. And then the real work is happening with people who are building through these or working with props and things like that. So I guess what I'm saying is these things are all happening at once. Whereas before, because of like, for example, when the iPhone came out in 2007, you had to first get to distribution where everyone had an iPhone, but now everyone has had access to chad and as a result they've had the experience. And some people have already moved on to something else or nothing at all, and other people are continuing to work on it simultaneously. That feels fundamentally different, which will lead to generative outcomes that are less predictable than if you had kind of like a slow distribution of access to the platform or the product. Does that make sense?
Speaker C: Yeah. And I think you're also bringing up a difference in the predictability.
Speaker F: I'm inviting you to unpack what predictability might mean as a technical term, what you guys are talking about.
Speaker C: I think these are both tell me if I'm summarizing correctly, but basically if GPT Four was frozen in time, this is the type society would still evolve, right? We'd figure out new ways to use GPT Four. It would encode itself, it would transform certain industries in certain ways and other industries in different ways and some not at all. It might take a long time to get static technology and that's maybe a little hard to predict how it's going to work out and it might take a long time. Well, Nico is bringing up the technology itself is evolving potentially in its capacity and there's an intersection between those two which might be difficult, but some aspects might be predictable, which is what the models might be able to do in three years or so.
Speaker F: Okay, so just to build sorry. And I think what you're alluding to or bringing up is that one of the challenges of this conversation is that there may be a set of things that are predictable and which we will fixate on or over fixate on because they are predictable and understandable, and we can have a conversation about them. And then we will create rules around those things or guardrails around those while the unpredictable or spontaneous kind of things happen that were not accounted for or predicted. And so there might be a sense that we're safe and we've got the guardrails in place and we're all good. Meanwhile, these other kind of random, let's call them like mutations or sort of meeting situations will occur that weren't predicted and then cause people to be scared. I just watched Nuclear Now, which is a new Oliver Stone documentary about nuclear power. And essentially there was a whole set of things that caused people to freak out about nuclear power back in the that turned us off of that type of power generation that actually could be very valuable or useful now. And so in a similar way, we may create the guardrails around certain types of GPT-3 five and four level LLMs, but actually miss a whole set of other things that might happen that then cause people to freak out and then either overreact or whatever. So we might repeat that same pattern if we don't find the right way to describe what is predictable and what is not and what is a normal distribution of surprises.
Speaker G: I think there's like two types of Godrails we suck at, right? The short term God Rails, which we see it all the time with my product. It's like I ask you to send an email to someone and sends an email to maintain our price book. And I'm like, no short term technical guardrails like, hey, this is like an engineering problem and we need to fix it. And then there's like the long term guardrails, which certainly we've been talking a lot about lately, like the existential risk guardrails, which this is more like a policy conversation before it's even a technical conversation, right? I think you're making an excellent point that we may risk killing the good that lays the golden eggs by prematurely imposing guardrails. And certainly that's been the trend of technological innovation over the last few decades, which has led to the secular stagnation and all of that stuff. I would say I am usually identified as a vibata and I am usually like a huge technophimist anti regulation, all of that stuff. I used to work at Uber, so not a huge fan of regulation, just.
Speaker E: Something in the waddle there.
Speaker G: But hey, this time I would say hello super. You know, I think we are talking about existential risk. I think the case for existential risk is actually quite convincing. Namely, we can't really predict exactly what's going to happen, but we can predict the outline of what's going to happen. Namely, any sufficiently intelligent agent ends up pursuing three things. It ends up pursuing whatever you're good. If you are an intelligent agent, you are going to pursue three things. Number one, you don't want to die. Number two, you want a ton of resources. Number three, you don't want anyone to change your goal. Right? That's called instrumental. That's super dangerous, right? Because if we have like an EGI that's way more powerful and intelligent than us, then it will not want us to kill it and it won't want us to change its goals and it will want to accumulate as many resources as possible and maybe cover the entire earth with solar panels in. This is actually one of the topics. Well, I would actually for once make an exception. Be like, yeah, we need serious, serious, serious policy guardrails in place even before we know what's going to happen. Because we can't wait to see what's going to happen.
Speaker C: I'm going to intercede here for 1 second to say two things. One, can you introduce yourself?
Speaker H: Yes. Hi. I'm Mehta. I'm so sorry I'm late. What are we doing as part of the introduction?
Speaker C: We are just letting people know what brought you to this theme. Like why are you were you interested in Guardrails?
Speaker H: Awesome. Yeah. So I am. Right now I'm working with McKinsey and a lot of my work for the past two years has been helping companies thinking through their AI strategy. And more recently with gen AI, I've been speaking with more retail and law clients on like, okay, how do we set our responsible AI principles? And that has been interesting for me because I've had lot of academic sort of learning in the past few months just like everyone else, but also seeing how companies are, what discussions they're having in the room when they're thinking about implementing it. And I thought this would be a great opportunity to really meet people from across the board and get their perspective on the topic.
Speaker C: Awesome. Second thing, the bathroom is down the hall on the left. Okay. And there's also glasses over there if you would like water. Sorry.
Speaker E: And Chatham House rules.
Speaker C: Recording oh, yes, yes.
Speaker E: Sorry.
Speaker C: We are recording this conversation. We're following Chatham House rules. So you're free to talk about what we talk about here. I might even publish what we talk about here. But no one specifically will be named or attribution will likely be given. Were you going to say something?
Speaker B: I was just going to jump in and at least I don't know what everyone's familiarity is with the proposed AI act in the EU, but there's been a lot of discussions around some of these things. Right now for non high risk AI, there's the self governance recommendation and then for what they're deeming high risk AI, they're still figuring out what our general purpose AI kind of these large language models, where do they fit in? But they're starting to discuss pre market requirements or what they're calling in the legal realm foreseeability, like things that are foreseeable risks and asking companies beforehand at a minimum to go through and basically predict things that might go wrong. And then if they're too high risk, either fix them or whatever I do. Question number one, what falls into the scope of foreseeability? Whether we're going to actually be able to foresee the things that are existential? It's easy to see things that are close to us, but if it's some proximate cause that's way down the line that we're unable to see, unable to guess. Or maybe it's an exercise of just maybe we just don't put that existential risk down because it doesn't occur to us or it seems too attenuated. I think that's a big question that's kind of open in the EU act. I think it's an important act for folks, even the US. Because much like the GDPR, which has the clause that says, hey, if you're going to participate with EU data subjects, same thing with this new law. They're proposing that if you're going to interact with the EU market, you better have this certification in theory, trustworthy certification. So enough to say that these are going to happen as drafted or these rules and regulations are the things that should be. But it is bringing some questions to mind about foreseeability and what exactly this risk assessment is going to involve and whether or not it's going to be just an exercise that adds more to the process or it's actually going to be something that's constructive. And I'm not sure at this point.
Speaker C: Yeah, it seems like oh, Andrew, please.
Speaker E: I was going to jump in because I think there's a couple of emerging threads and I want to hope we can clarify what you just brought up too. So I see it's like there's the engineering capabilities, which have uncertainty, right, and have maybe more known practices and guardrails. And then there's the socially emergent complexities, which are much more unknown. I think one thing, and this is a term I'm just going to make up right now, but if it makes sense, you can tell me not. So when the pace of capabilities outstrips the market saturation for what those capabilities imply at a fixed point in time, I think it can develop what I would say is the frontier ecosystem, which is where the number of new available opportunities is much greater than the number of opportunity seekers. We see this usually in the saturation of new market developments, and things become commoditized and penetrated into their respective use cases. So we've kind of seen that in Internet technology, web 2.0 stuff, okay, like more Twitter clowns, great. But this, it's like there's so many opportunities now and there's only more increasing. And I would suggest so thinking about guardrails, right? There's kind of two lessons from engineering sorry, nuclear energy world. There's engineering controls, which are built into the technology, and then there's social controls, which are built into the laws and social practices regarding the use of that technology. And I would suggest that in a frontier ecosystem, it's almost impossible to regulate social controls via case law or case by case because the space of possible cases is exploding. It's faster than anyone can track. But we do have a lot more grips on the engineering controls because they follow a more known performance improvement curve based on infrastructure and capitalism. I don't know if those ideas resonate.
Speaker C: With yeah, they definitely resonate. And it connects with so there's professor at Stanford, Rob Reich, who articulates one aspect of what Andrew's talking about, which is his whole thing is that AI practitioners have to develop a culture of responsibility because any law, any standard I mean, in some ways they have their proximal effect, which is what they explicitly regulate. But hopefully they also engender a broader culture of responsibility setting, which is the more robust kind of change the issue there is. AI is born out of the tech culture or now is being co opted by tech culture at the very least. And tech culture is a uber mindset innovation first. Sometimes when he brings up these kind of culture of safety, we'll point to bio technology, the kind of safety mindset that seemed to have surrounded CRISPR and then the gap between that and where we are now. So I like this point that you're kind of calling out that through either law or culture there's a lot more uncertainty on social safeguards and social evolution. This idea of kind of opportunities outstripping opportunity seekers is an interesting one and so technical or engineering safeguards seem it sounds engineering.
Speaker E: Maybe my pitch is really we want to get the net effect of guardrails to preserve this frontier ecosystem of new emerging capabilities and tons of opportunities. Yeah, but then I think this other point that got brought up is very true resonated with me, which is like a few bad publicity events in the social space can hamstring the whole industry and clamp down on that improvement in capability.
Speaker C: So I wanted to bring that up. There's a friend of mine who works in the governance space and his motivation is because he actually believes in AI's capabilities a lot, its benefit for humanity. And he believes that that will pay out or he wants it to be able to pay out on the not centuries timescale. It's not like a long termism kind of thing, but on the five to ten to 20 year timescale and is worried that a few terrible kind of or near misses or just bad publicity will lead to a complete rejection of certain kinds of technologies. So for instance, we've already seen this with facial recognition. Facial recognition was there's a famous paper called Gender Shades which showed facial recognition systems were biased or Amazon's facial recognition system was biased. And regardless of where it's a dual use technology like any other, probably has certain uses that are like pro all of our values and some that are anti our values. But that's not the kind of conversation we have about facial recognition anymore. Instead it's kind of a demonized technology. And that's one aspect of that's a pro innovation, pro governance perspective where you don't want to end up with that social backlash.
Speaker H: I feel like if you're the developer of these models though, like say you're OpenAI or Google, can't you put in place some policy or some systems as you're building it to be like, oh, I have a committee or something where if you see a potential harmful use of this, you can submit it to us?
Speaker C: So they definitely do like OpenAI, for instance, and DeepMind. These have very they put a lot of investment into evaluations and guardrails and governance and a whole bunch of different things, potentially not enough. And potentially we just don't want guardrails to be dictated by the people building the model. You might want it to be a more democratized approach, but they put a lot of investment into how are they going to evaluate, find out unknown, unknowns and a whole bunch of different things.
Speaker G: Which I think to your point about engineering guardrails, like the really tricky part here isn't the Googles and Open AIS, which are actually very well intentioned. It is the fact, and Marcus and Elizabeth talk about that it's Mosla. Like the march of technological history is not on our side here. It is becoming exponentially cheaper and actually even faster than Mosla to train these models such that a GPT-3, that cost of the order of, call it $100 to $200 million to train three years ago, because today probably of the order of a million dollars, if that, it's really not that much money. And five years from now you're going to be able to train a GPT-3 with perhaps $10,000, right? I think you're actually going to get these things to run on your Apple Watch, right? And so the case to be made against regulation is kind of reminiscent of the case to be made against a gun regulation. But this time it's actually a good case. It's like, hey, if you're regulated, only the bad guys are going to have access to these capabilities, right? Because the Open AIS and the Microsoft are going to kneecap these guys and really make their research slow down to a crawl. But then any script kidding that arise and you terrorist, any ill intentioned person will be able to train these models, whether you want it or not, with a laptop. And there's no way you can regulate that without regulating laptops out of existence. And so that doesn't mean we can't do it, but that just means insofar as we want to regulate these things, that is the cost. If you really believe there is an existential risk, the cost is like, hey, moratorium on both law effectively.
Speaker D: Well, I think one important aspect to be able to lay the foundations over the appropriate, appropriate, desirable balance between proactionary and precautionary is to understand the value chains to build upon your example. I, for one don't believe that in three years from now you're going to be able to train a large language model on a laptop, you're going to be able to run inferences and you are going to be able to fine tune, which is a very, very high potential. And I agree what you said in terms of the downstream, to your point, we've been very active on the UA Act. I really encourage you to read Article 20 B of the current text, voted by the European Parliament because it serves as the point of encounter. This is the article focusing on the guardrails for foundation models. It's been a tough fight to get this in. This is now voted and now we have entered into a trilogy. This and the code of conduct been developed by the Trend Technology Council between the US and the EU. I think revolving around the article 28 B serves as an interesting point to look at in terms of what could be a balance between proactionary and precautionary in terms of common sense, pre market deployment and post market development deployments. Things to watch and look at, because when you look at it, I really encourage you to read the text. I can share it with you when you are pass it down to you Nick, and you can share it. I believe I belong to group releases this common sense because that's the kind of thing that open air entropy and others are already a bit doing, but without accountability and without necessarily the right amount of investments inside the company and towards its customers and suppliers around that. But for us to be able to calibrate those points of friction and not over regulate to engender a good balance visa vis innovation is to understand the value chain. And right now we don't have a good base of evidence over what is slash LLM value chain. What is it? It's not clear. And the more time we spend not having it through science, through policy, the more we are bound to make mistakes. Not even talking about the Global South, which is demanding more innovation to accelerate access to basic services at a limited cost. So this value chain thing is for us like an important flashpoint.
Speaker C: I just wanted to actually make sure that everyone's on the same page about the UA act for a second and then we'll come back. As Allison was bringing up before, the way that this act is approaching regulation is to say there are high risk systems and not high risk. This is how it was actually how it was high risk systems and not high risk systems. And those high risk systems are systems that are kind of closer to human flourishing, like health care and jobs and these kind of things. And they have additional requirements. And those requirements, a lot of them are transparency requirements, like document how you did you do this evaluation, document your training data, a whole bunch of things that seem relatively sensible. And there was a gap for these generative AI systems. How do you deal with general purpose systems within these high risks, which is what we're talking about right now, but there's still an active. While the generative AI general purpose AI has definitely captured a lot of our focus, nico had brought up the difference between or the sometimes contentious conversations between AI safety folks and AI ethics folks. One of the causes of that contention is because the AI ethics folks feel even. More that actually these conversations should be focused on guardrails about things, systems that are deployed and we want to make sure that they're not biased against black people or something like that. And so even in the prioritization of our conversations do we sometimes focus on different kinds of guardrails? But the Euai Act and other conversations have evolved to say hey, we can both have specific guardrails transparency requirements for high risk systems or in the US particular entities responsible for regulating housing or something might take that kind of perspective. But there has been a focus for more general purpose AI system regulation. Just wanted to have that, please.
Speaker B: I know that you left off talking with the future society focusing a little bit more on doing, I don't know, is there any research being done on the value chains?
Speaker D: There is a mounting set of elements that has been now consolidated at the level of the OECD, which is the main international coordination body for policy and evidence. But it's still quite shallow, not only for bad reasons, it's quite shallow because it's moving so fast. What we were discussing before, where's the frontier between training and fine tuning. This frontier is moving for good reasons. So that's why the jury is still out there. That's why we have been proposing in our own work to frame the question of governance to include the most difficult thing to govern, which is research and development governing research. What do you mean you want a stiffer innovation? No, which is that the underlying technoscientic cocktail is so potent and complex that we may have to look at that too.
Speaker B: There is a part of the for those who are not familiar, the proposes regulatory sandboxes for playing around with some AI that may be of higher risk. It sounds like each member state can have their own regulatory box. I wonder how efficient it will be to oversee all the various sandboxes. And honestly, certain member states can probably fund different size member boxes or sandboxes. But I do think that it's an interesting thought. It's pretty rare for a company to release a system that is only meant for one particular country. And it's a difficulty just in law in general these days that each country has a different way of doing things. In the US. Different states have doing things and I just seeing the law. I imagine that California will probably be the first state that will probably try to model something similar to the EU's AI Act. And if they follow the privacy track it's going to be a long process of discussions. And I don't know, I do see the AI Act pushing a lot of its frameworks to us just because we are a little bit slower in the US. And we are reactionary. So we might see things on the military side. We might see things that are existential threats being covered from the FDC standpoint or some sort of federal law. But from the publications I've seen, I'm just not seeing a lot of frameworks that are actually being proposed. The AI Bill of Rights that was proposed, bill of Rights that was sent out, it was 70 page paper from the White House's Technology Group and it was just kind of these are things we should think about for 70 pages on. That's why I have been pushing for discussions around the EU AI Act because I do think that it's something that we won't realize until later, that it's something that affects Americans.
Speaker C: So we've moved into this kind of policy discussion just to bring it back to how it intersects with predictability and surprise and is one of the conversations that seems common amongst us and also in most government policy conversations I've had. It's a recognition that this technology is developing so quickly that policymakers are aware they do not want to create brittle policy. They want something that can adapt and evolve. And that seems fundamentally difficult. It seems like the technology of governance itself needs this, I guess, needs to evolve to be able to maintain because this this is like precautionary and reactive. I mean, it needs to be both. I mean, this isn't a there are certain risks. It seems like we can anticipate some common sense things. We can track certain kind of information which will allow us to better act in the future, record AI incidents, do some compute governance and have some understanding of who is training large frontier models. These seem like sensible kind of systems to set us up better. But it seems fundamentally difficult to or part of the solution would need to be creating this meta system to be able to react appropriately over time.
Speaker A: Yeah, I kind of had a quick question since you're on the subject of EUA Act and what you just mentioned, the gaps that policy kind of tend to create. Is there an initial kind of consensus around what's missing in the EUA Act in terms of guardrails that I know you kind of mentioned one that research perhaps is ineffective with that one. But what's missing in terms of guardrails?
Speaker B: Well, you mentioned foundational models, right? I mean, that's something.
Speaker D: It's addressed through 28 B, so there are a number of it would take a lot of time to cover that, but basically a lot of constitutional enforcement to have a right level of coordination across the EU with the CI office, which is now predicted in a way that is non regulatory but coordinates EVAs regulatory sandbox and others. And then how do we calibrate if we put a threshold? Because one of the key things which is not yet sorted in the US is a key notion for French cartesian is the question of definition. And the definition drives where the friction point is. And this question of definition is not solved at all. There is a definition for AI packed with these pyramids of risk, a definition of general publication systems, foundation models and generative AI, which is arguably one of the functions potentially of foundational models and that is still in flux. Now we are at a moment where the trilogue is happening, where the three branches of government are getting together behind closed doors to sort that out. Expect some interesting surprises across these. These are some of the elements still in flux that we could go on for. Yeah, probably you should get that stuff there if you want.
Speaker I: Does the EU act define what is I risk?
Speaker D: Of course one of the best things to define is risk by application layers. That's one of the strongest part. One of the weakest part is the underlying technological definition where we don't want to regulate technology and we won't define it. And yet we have problems with XYZ, right?
Speaker I: So high risk is just like specific sectors applications.
Speaker B: There's annexes that will make it very clear which ones are I mean, in know this is still proposed. I do think that it will be a question that lawyers will get here in the US. They'll ask is this something that's high risk? Is it something that is prohibited? And making that call can be difficult potentially if these definitions aren't clear. But the things that are considered not high risk there is a strong recommendation still in the proposed act to have a self governance have a code of conduct of some sort. It's a very short article, sexy article 69 that says that recommends that non high risk AI should have be self governance program. But there's not much there.
Speaker A: There's some examples of codes of conduct.
Speaker B: That I think that some groups are working on. But if there's no code of conduct, I worry that folks will start saying well, I might be considered high risk at one point because they can change their mind. Maybe I should just do what is required under the high risk AI type of things or do nothing at all because they just don't know what to do.
Speaker C: Let me ask you a question for the people who are like because even though I work a decent amount in AI governance, I think that's like a relatively small part of the overall risk mitigation. I think it's an important part, but it's just one I'm quite bearish on code of conduct impacting many things. I'm curious whether people think one aspect is or one focus of a lot of these policies are transparency requirements and one goal there is to allow the broad to make it less brittle. Right? You say, okay, you have to report on some things and allows broader society to make educated decisions about what products they use, whether they demonize certain things. It can allow researchers to to come in there's like a hope that sunlight is the best disinfectant, that kind of thing. Are people bullish on transparency?
Speaker F: I've been sort of trying to apply some of this part of the conversation. Back when we were first building the social web, and the fact that the Facebook platform was available and it was documented and the API was out there, there was a level of transparency, at least, on how to interact. With the system, what methods were available, and there know. Maybe you didn't know exactly what was happening behind the scenes in terms of their ad platform or the optimization of the newsfeed. And that's constantly on all these platforms. A bit of an arms race, but there was ostensibly a great deal of sunlight in that moment. If you just look at the developer documentation that was available. But the ability for the population to understand what types of products or tools could be built, whether Cambridge Analytica is real or not, that type of application was obvious. I mean, many of us actually were trying to build a way for you to move your data out of Facebook onto other platforms and that was actually a goal in order to give people more freedom over their social web experiences. And yet the sort of result of that was again like in nuclear, this explosion that then caused everyone to look at and fixate on that one thing, which then caused the more or less destruction of the Facebook platform and other people having the ability to build on it. So the point about transparency is one where it's like, well if the population isn't educated about how to actually make sense of the type of information and data that is made available through transparency initiatives, then you actually haven't like it's not sunlight that actually sort of shines on all things equally. It's only for those people who are educated and who understand how to evaluate what they're looking at that it actually creates a type of engagement or participation vector for people. So I don't know how you bridge that gap, where on the one hand, you may have transparency, but the deluge of information is so great that unless you're only living to read all these things that are coming out, there is no possibility for actually ingesting and making sense of and rationalizing and then discussing and then asking for adjustments or changes to the way in which any of these companies or organizations or individuals or open source projects are operating. So I guess I'm just concerned that there is so much information currently being produced that's hard to keep track of.
Speaker H: It's an info dump.
Speaker B: In order to stay transparent or see the light, basically even these laws are saying like, you need to inform people through a user guide, an AI user guide. So it's like, okay, so when you start you're going to first read the terms of service, then you're going to.
Speaker H: Read the piracy policy and then you're.
Speaker B: Going to read the user guide, right? So it makes it again, I wonder putting so much transparent if you're just.
Speaker F: Being thrown burden on the individual, but it doesn't seem like it has anything to do with individual autonomy or the ability to choose without a commensurate educational program where education itself has to sort of modernize and change and adjust and adapt. You know what I mean? It's not like there was, at least as far as I know, to your point, an education campaign around the GDPR like cookie banners or how to make sense of them.
Speaker C: It doesn't have people.
Speaker F: I guess my question. Yeah. What type of mechanic is the transparency initiative and how are people supposed to make use of this information to self inform and make different decisions?
Speaker C: Are you going to respond directly?
Speaker E: Not directly. Build on that?
Speaker A: Yeah, directly to the extent I was going to say plus one.
Speaker H: Plus.
Speaker A: I completely agree. I think you're absolutely right. Transparency, it's hard to argue against it. Surely we could all make use of it, but who are you empowering in this process? So, for instance, a lot of calls currently around transparency of the components, if you will, the data used or the compute used, or the documentation process, or.
Speaker C: The weight more interesting.
Speaker A: And I have a question about could we think about transparency around usage? How are these models being currently used? Which goes to your question of the value chain. We still don't understand how they're being commercialized.
Speaker B: Let me finish.
Speaker E: Great. Segue?
Speaker A: Yes, I think there's a question. I mean, I think even as we think about companies kind of commitment to transparency, there should be a commitment to translate for different audiences. I think otherwise the default around transparency is centered around needs of machine learning scientists and the kind of transparency they require to examine these systems, which are no doubt important. I think that can be solved at the cultural level. But I think taking those talking points and kind of baking it in policy like give us transparency around compute use or training data used, I don't know.
Speaker B: If that's the stuff that's going to help the end user, but yeah, it's interesting hearing.
Speaker C: It it's interesting hearing. I rarely thought of transparency being primarily for the end user. That seems like a hopeless like, I would like companies to be transparent such that watchdog groups or government agencies or academia, the other representatives who are there to have different incentives to reflect different goals, some of them are explicitly meant to be my representative. Some of them just have a different view of the world that they have the ability to actively participate. And one of those might be our goal is to take these difficult transparent reports and make it possible for consumers to understand them because that's my niche in the ecosystem. I want the information to exist, to allow that ecosystem to develop rather than necessarily need the one company to do all of those things. But I think that's like a necessary precondition for all of these other actors to profitably interact. Sorry, Andrew. You've been delayed. Number of times, please.
Speaker E: Interesting. And I'm kind of bringing an idea up which made people more informed to comment on. Is this something that people acknowledge? So there's this transparency to outside experts. That's good, right? Transparency enables one other thing, too, which is attribution and reputational risk for foundation model providers. I think the automotive industry versus nuclear industry could be illustrative. So the thing that Christy brought up on nuclear, it's the liability, it's the companies being liable for the downstream possible bad effects, which kills the industry, and that increases the insurance overhead and everything is terrible. Automotive industry, I mean, cars started off super dangerous, right? But car companies compete by reputation on.
Speaker C: Safety.
Speaker E: In answer to this point, which is, look at these cheap open source models. And maybe there is an arms race dynamic between privately funded large corporate models, but that are beholden to either shareholders or regulatory boards versus open source hacker laptop guys, is that to win that arms race, you need to have the people under scrutiny and subject to reputational risk be technology frontier leaders.
Speaker D: Right.
Speaker E: And so hamstring them with liability is a good way to put them not at the frontier, in which case, in an adversarial mindset, they can't outcompete the bad actors.
Speaker C: Are you saying you're advocating against liability?
Speaker E: I'm saying liability is how you crush the industry, because the liability of potential things downstream is so infinite. But acknowledging that transparency and attributional transparency, who did what, is then opens companies up to reputational risk, which is a more positive reinforcement.
Speaker F: Isn't this why Google didn't really launch any of their LLM stuff for so long?
Speaker E: I think that's also related to their.
Speaker C: Business model, but yeah, sure. For research.
Speaker D: Yeah, arguably. You say arguably because of that.
Speaker F: Yeah, I'm saying that the reputational risk to Google was enormous relative to yeah.
Speaker C: Reputational risk is a way to say there is actually a court of public opinion that is incredibly meaningful and potentially more quickly evolving.
Speaker E: Maybe consumer opinion.
Speaker D: Right.
Speaker E: So if I'm an application layer company, and I'm going to choose Claude or Chad, is it Chad, D or T?
Speaker F: D. We use d. I like that.
Speaker E: I love that. I hate names. Chad. Chad versus Claude.
Speaker D: Right.
Speaker C: It just shows that Chad were not intending for it to be such a product. It was just like they were like, here's the next version of GBT.
Speaker E: Yeah, it's terrible, but exactly like, I as a consumer, vote with my money. I buy a Volvo because they invented Seatbelts or Tesla because they're in higher stage, whatever. Right. So vote with your money too, is powerful for companies.
Speaker C: Yeah. I saw you raising your hand.
Speaker B: Yeah.
Speaker H: I wanted to respond to your point on transparency, where you were saying that when we're thinking about transparency alternates for watchdogs and regulators and things like that. And I think that is a component of transparency to the earlier point that you made, which is about, okay, now that we're thinking about, when we start thinking about the value chain and we start thinking about enterprises that are thinking about fine tuning or building an application layer, there has to be an element of transparency to these enterprises. So if I am a bank or a retail company, what I've seen often is companies struggle with understanding what information is not available to them because they're not AI native companies. So it is to say that if I am going to be using a GPT Four, and this is what has been given to me as part of how the model was trained, whatever the disclosures were, then what is it that's not available to me? I'm not empowered to ask those questions. And at some level, when companies are thinking about their own principles or their own independent guardrails, the question that they have is that there are certain things that you can and cannot do. And sometimes those things are clear. But there are certain things that I would not do because I am an X Company, which are my personal organizational values. And how do I know if this model was built in conflict with those values? Because to your point that there is certain information available, there's certain information which is not. So, for instance, if for me as an organization, the value is that my team has to have demographic representation of everyone that I impact, while GBD Four doesn't tell me who holds people in the room or whatever, right? This is an example. So that in itself becomes a question of like, I will have to use it because everyone else is, and if I don't, then I'll be left behind in some ways. But how do I shape my own personal narrative? And what is within the boundaries of transparency? What is not available to me as a question?
Speaker D: Can I just put a point in response that a lot of how the Europeans have looked at that and how the transatlantic conversation has been shaped around that is transparency as a purveyor of appropriate sharing of the risk and liabilities along the value chain. At the point where those foundational models are created, there is capital, knowledge and a lot of problems to create that transparency, but where a lot of information can be created so that when you pass on the liability down the value chain, you can pass it on in a way which is reasonable. It was not the case a year ago where because of, okay, lobbying, some companies, I won't name them, were pushing the liability at the very edge of the market, say, oh, you're deploying my model, you should be liable. While these agents did not have the right kind of information to exercise their risk, basically. And so transparency is also a way to apportion the risk in a way that that is, in my opinion, long term innovation. Without that, at the end of the day, some systems won't be. So the way in which Europeans are looking at that is by through the US. Creating a system of fines, finding companies whether or not honoring and complying with their obligations and reforming the product safely liability regime to ensure that if you're putting on the market something that you can be liable for it, but in a way which is not criminal. There is another set of opportunities down the line according to sectoral regulation, where you didn't mention aviation, but aviation you exhibit criminal liabilities too. Like if I am the project manager and I'm building the aircraft and project manager for the A 320 program, for example, there are certain cases where I am going to be criminally. I could go to jail, basically. And those intertwining regime of liability and fines, which is not liability were expected in Europe to push the right kind of behaviors by developers deployers of AI systems. If you have only fines and not liability, you get to what we've seen on privacy whereby Big Tech has said, you know what, 200 million don't care with provision, 1 billion don't care about provision. So fines were seen not to be sufficient, but a web of incentives, including liability, including in some case criminal, but it's not the case yet in the UAI regulatory regime were seen as the right web of incentives along the value chain and putting the obligation where it belongs.
Speaker H: When you say value chain, what do you mean over here?
Speaker D: Well, if OPDI is developing GPT Five, you cannot expect a small SME in Germany or in Alabama that has no clue as to how it was developed to carry the liability without it receiving the package of information in terms of what is in there into the system.
Speaker C: I think he's talking about what you're talking about, which is there is a streamline. It's not just even foundation model and then applyer anymore.
Speaker D: Right?
Speaker C: There's a foundation model, then there's maybe fine tuner, fine tuning system, some other system that gets the data for the fine tuning. It's a vast web that comes together. And I think your point, which is there are sometimes where where safeguards have a veil of being solved. For instance, in San Francisco, every company will require another company to be SoC two compliant, which is a cybersecurity standard. And that is the high mark of self governance. Right? It's not required by anyone. It's not a standard, but market forces because every company needs cybersecurity, requires it. So everyone goes through that exercise. I have not yet read a great academic paper that shows how effective has Sock Two been. Has that tick mark checking exercise helped? I imagine to some degree, right? I'm sure companies are generally more cybersecure thanks to Sock Two. But how quickly does sock two evolve? How quickly does it adapt? They're already market leaders. Do they have any incentive to continue evolving? And my hope for transparency is that the kind of customer incentives that you're saying, like pass down companies, buying another company is like a powerful, powerful law.
Speaker D: That's because it is incentive.
Speaker E: I wanted to ask, maybe clarify for myself, because I'm also not an expert in this field, but when you say value chain and apportioning risk liability, I was thinking of an example, and I think it illustrates something about this topic, too. So for a car rental company, car manufacturer, the dealership, the person that operates the car, what goes wrong? You can apportion liability along that value chain, right? Someone rented the car and then went.
Speaker C: On a rampage, obviously.
Speaker E: Is the manufacturer not liable?
Speaker C: Right.
Speaker E: This is bad use. But if something faulty in the car failed right. Breaks or something is at the dealership for selling a faulty part, the car.
Speaker C: Company didn't maintain it properly.
Speaker E: But if they follow the maintenance schedule.
Speaker D: Then it goes to manufacturer, so forth.
Speaker E: I think there's an example there, but I think suppose what you said earlier is that these things are so black boxed where that thing went wrong. You can't point to the brake pads that failed because the brake pads are these weights in some obtuse space that no one understands. So there's transparency in two senses. One that I think is well formed and one that's not. The well formed is company operations and intent and operating procedures. The other transparency is where is the factor of safety for this brake pad equivalent in the parameter space of this model? Where the hell is that the question.
Speaker D: To mother, which is a real question, which is very contentious. But that's why I'm putting on the table over transparency over the weights, which is contentious for many good reasons and a few bad reasons. That's why I think that we need to be able to elevate the conversation at the level of weights because of the underlying scientific the weights are the.
Speaker E: Equivalent to the factor of safety on the elevator cable. Right? When you design an elevator, you design an airplane, you're criminally liable if you make bad engineering decisions.
Speaker C: Right?
Speaker E: That's it. But we don't know what those decisions are.
Speaker B: Some people would argue that the weights are the trade secrets, the source code.
Speaker C: Yeah, of course. Other people, even safety minded people, will also point out that some of these things, especially are information hazards over some evaluations. Some people even demonize some X, risk focused people as intentionally hyping their systems by being like, I'm really concerned about the risk here, and others being like, you're just trying to make it seem really cool.
Speaker F: We know you.
Speaker G: And it's like, no, I really am.
Speaker D: Intertwined. That's a great point. We have an epistemic crisis around that very question for many good reasons, too. It's the first time on that trip I hear another layer, which is Espanage. Finally, the word is out now, commercial interest, but also now, because it becomes so nationalistic. Yeah, espionage. I'm like, okay, we won't give a weight to the EUA office because we're afraid that they are going to be passed on to whatever nation which is developing international level. That's the level of that's why I think it's contentious.
Speaker C: Yeah. I think the idea of this is where transparency and safety this is also why sometimes safety minded people and open technology people are sometimes being divided now, too. Because previously, most people who are, like me, super into open sourcing things, love the idea of an open web, but now also am worried about the reality that flow you brought up a while ago of what happens when the open source market is not just like barely delayed versus the frontier months and then Alpaca is trained for $600 or whatever. How do you kind of push on?
Speaker G: And I think that there's a metal to the layer of this conversation that we're starting to touch on, which is, to your point, the quality of the conversation.
Speaker C: Right.
Speaker G: And it's like we've lost goodwill in the conversation. Like both sides are now attacking the other side's intentions. We don't even get to the substance of the conversation. It's like you're saying that because you want to be saying that because you're anti power. Can we just get into the substance of the conversation? And I think even more worrisomely and Jeff Hinton, who quit Google recently and is a very prominent AI researcher who is starting to be himself extremely concerned about the possibilities of this technology. He put it on the head recently. He said there's two questions that the research community needs to come to a consensus about because the research community itself right now is split on this line. And if the research community is confused, like, what are the ads that the regulator wouldn't understand anything like the researchers themselves don't understand?
Speaker C: Right.
Speaker G: And there is a building need for consensus in the research community on two questions. One, is all these large mortgage models actually understanding what they're doing? Or are they just stochastic parrot? Right? And the second question is, what are the risks? Like, exactly? What do we risk? All the existential risks actually real? I think the emerging conscious system. Question one is, yes, there is something.
Speaker D: That'S akin to understanding probably not yet scientific. We're far away from the beauty and the laboring outcome of a scientific consensus. This is why they were asking for Andrew Ng and him in that conversation, an IPCC for it that we've been putting on the tape, but we are.
Speaker G: Far from it, definitely. But I think the intuitions of the researchers in that field book, because this is the thing books are like, yes, we lack the rigorous understanding that we need to understand these systems.
Speaker C: But your gloss, I agree with your gloss. That seems like a reasonable summary of that.
Speaker G: When you talk to these things, I kind of understand it's hard to say there is no understanding going on in these things.
Speaker E: And.
Speaker G: I think, again, the second point, there is no consensus whatsoever, even in the research community on this. So I think it behooves us at the very least to hold the bar on that conversation where it's like, I don't tolerate whenever I'm in these conversations and people start to attack each other's attention, can we just stay on the substance level of the conversation?
Speaker C: So one place where attacking intentions comes up and probably is true sometimes is like large companies, typically when they become pro regulatory, you can start to understand that space because it squashes the little guy. Regulators themselves have ways to deal with that by having triggers where regulation becomes arduous. When you have a certain amount of arr or size. But that's like one thing that's brought up. But I agree with you that there are so many substantive reasons to bring up any of these conversation points that it's troubling when it gets disregarded. And on the researcher front, I feel like I don't see it changing honestly for a long time that there's going to be consensus. The only way there will become consensus on literally any dimension is if we're on an escrow that flattens out so that there's time to gather, okay, where are we now? And I'm just not that optimistic that we're going to get there into some flattening. I think we're going to constantly be in this evolving state. And so I think we need some way to be able to act without consensus with lower probabilities, a lower certaintude. I think we should be proactively trying things, regulation and not regulatory innovations and then empirically tracking. I want better incidents reporting. I want to know, I think you brought up your OpenAI. What are people using your system for? Like, OpenAI will come to talks and it'll be like, we spent a lot of time trying to understand how our models are being used, and we have a bunch of safeguards and I believe them. That's awesome, that's great. But that seems like a rich, important piece of information that unfortunately, I don't think these models, we can protect that information to protect OpenAI's intellectual property with their intellectual property needs and society's needs to know how these systems I'm more like, okay, you need to report on how your systems are being used.
Speaker D: An additional level of transparency, which is underlying here, which you can put down, is over corporate governance. Very important. Open air claims. XYZ, where I run by a nonprofit. Show me the proofs. Yeah, this 501 C three governance regime is made towards transparency. You file every year 990, where you say a bunch of number of things on what is there on your asset. You are really like, constrained on a number of levels. And I think that one of the underlying reason why people have lost trust in the claims that are being made in terms of what we stand for, in terms of the public visa, visa, private is that no one has really understood, and I'm supposed to be an expert on that. I've raised questions to Entropic as well. They are a b corp. They had promised that they would follow a wave of transparency over corporate governance. This wave is not yet there. I for one believe it would help tremendously. But I'm afraid the underlying dynamics of the capitalistic race is moving us further away from that desirable state of corporate governance transparency, which is key for this hybrid configuration, particularly when you claim to be governed by a.
Speaker C: Credo AI is a for profit corporate governance software company. Right. And over time this has changed a little bit. The main reason why people, at least early on, would want to work with a company like Credo AI is for a halo effect. Right. They want to look like they are responsible, that they're doing a good thing. That's an important part of brand. People recognize that part of brand. That was an important brand statement pretty early on. But that needed be married with actual governance, necessarily to get that halo effect. Can I call on people who have spoken less to we can continue on this whatever theme. There are other themes that we could talk about, either going more about into kind of engineering guardrails culture of AI timelines of concern or other themes. But I would love to hear yeah, I'd like to pose a question. Sure. Does anyone think that OpenAI should release their weights?
Speaker D: Wouldn't that be dangerous for all modes.
Speaker C: Just to be quite 3.5 and four?
Speaker E: And when you say release it's not to a government agency that has privilege.
Speaker C: But open source public? Well, either of those questions. Yeah, if you want to say yes and one no. Tiffany?
Speaker H: Yeah, I'm curious. So you want it to be open to the public? I feel like wouldn't that be dangerous?
Speaker C: I didn't say I do.
Speaker I: What's the question?
Speaker H: No, I think my concerns are like, oh, if you did that, wouldn't I don't know, that be bad in the sense that everyone has access to these weights and they can use that as a starting point and then just train really dangerous models very quickly.
Speaker E: 100%.
Speaker C: Yeah.
Speaker A: I have more of a clarifying question. So when you say release weights, is that what Meta did with Llama?
Speaker D: Well, possibility.
Speaker C: Meta released them to researchers and they promptly leaked. Which is why I would say that I don't really differentiate between releasing them to a government agency or academic researchers and releasing them to the public. They would be leaked promptly. You should be aware of that. If you release to anyone, what will happen? Llama is a case positive of that. So you're bringing up concerns. So Llama was released and Llama has led to a huge number of downstream models that have been allowed for different kinds of post training, including reinforcement, learning from human feedback. That has led to greater capabilities and sometimes done very cheaply. There's a new model called Orca made by Microsoft which tried to use a more robust way of training. So a lot of things and so mean, it seems like the open source community. That's been a huge boost to the open source community. I guess we'll see its first real.
Speaker G: Hey, Flo, thanks for having me.
Speaker E: So stop out here before you go. Is there anyone object to being tagged on Twitter in some yeah, if you don't want to, we don't have to mention people.
Speaker D: Why don't you give an answer to that question, at least before you take off? That would be useful given what you said before.
Speaker G: I would default to no, because really it can be very dangerous. The steelman for Open sourcing the weights is that open source software is the.
Speaker D: Safest software is the safest over the time, yeah, over time.
Speaker C: I would counter that you can do a lot of like, if they open source, the weights, okay, great. So I can evaluate them, but I'm not like looking at the individual floating point values. I'm doing some sort of evaluation and paying for my own hardware to do that, where I can basically do that just by using OpenAI as service. Now I do lose some insight and I can't fine tune things and all sorts of other downstream things.
Speaker B: Just because you have the weight doesn't mean that you have if they have some hard coded things within it, you won't be able to see everything. Right. The weight is just to the AI elements. So I don't know if you'd be getting the same value, but if you had them on your own, you'd be able to build your own product in some sense and maybe adjust the weights a little bit, make it more. And that's why, from my perspective, I think it is an IP issue. And when you're working with contractors to help you fine tune, it's probably something you should be careful about. But maybe I'm just coming in from it from a pure IP standpoint.
Speaker C: I sometimes wonder if by releasing the weights or a lot of the things we do, we make it very easy to to use these systems, to build inference into these systems. And like, there's, I think, growing recognition that, you know, our evaluations are lagging like the kind of safeguards that we would want in place. Technically, we don't have academic consensus on them, but we also don't have they're not easy to do. It's not like if you got Llama, which is hard enough to get running on your own, that you're like, oh, just run this safety suite that you can make sure that you use. Some consumers, not all there are bad actors. But if that was trivial, trivially accessible, they would be like, yeah, I definitely want there's some that would want those kind of safeguards. But it's not trivial, it's not trivially accessible, it's arduous. And so it takes away tremendously from quickly getting going and just setting something out.
Speaker D: One of the reason why people in my faction want to anyone to vote?
Speaker C: No.
Speaker B: I just had a quick question again.
Speaker A: In the vein of clarifying. I had read something about OpenAI coming out with an Open release. I don't know if that's I read it on a website called Artisanal AI. I don't know if that's correctly, but that to me was interesting as well. I thought Open and Closed was hard coded into these companies'identities, but it sounds like now OpenAI wants to get into that race because I imagine that the Open release is going to lead to wider penetration and create folks to develop on it downstream. So I wonder if it's even a philosophical difference or is it even just really based on commercial incentives?
Speaker C: I mean, it's probably neither one nor the other, right? There's a new French company, right? And they are a big time open.
Speaker D: Source, those big more large model in the range of 15 billion parameters.
Speaker C: But they like stability. They come out and they say, like, our niche in the ecosystem is to bring these models to you see the pattern.
Speaker D: Most European stability, ti, Mistrala, Aleph, Alpha, Germany and Eluther are trying to position themselves versus the Americans by saying we're going to accelerate this, accelerate intake and traction by being open source, which for the people like me is like a source of major concern. Reason that you mentioned, like if the race is established like that oh my God.
Speaker C: Yeah.
Speaker E: Is that because they're looking to avoid regulatory overhead?
Speaker D: No, as an entity site, mostly because they want to catch up and their sense of how do we create a platform with maximum traction would get from that where we can get our system adopted and red teamed in a way which yields interesting passes in the end. That's why for me, the question of will for people my fashion trying to bridge the gap, I primarily see the need to go at that contentious question as a way to create a wave of scientific research towards mechanistic interpretability and other sources of alignment and control solutions because it remains an immensely hard problem scientifically. If you ask entropics interpretability, which is one of the best in the world, how far along they are, you'll see the answer we are so far away. But some others say that no enabling that itself is going to generate more raw capacities. So it's a never ending story, not even touching the real question of commercial interests IP and now it's pioneer.
Speaker E: But that does open up this question of attribution, of responsibility in the value chain. If you have mechanistic interpretability. Is that true?
Speaker D: Come again?
Speaker E: When you say mechanistic interpretability, it means we know what parts the model can.
Speaker D: Produce, what kinds of behavior. Yeah, not in black and white. It's like a mountain we're going to climb Mount Everest and we're a campaign of understanding a bit more. Okay, why did that decision, why did that hallucination happen?
Speaker E: And does that make it easier to assign responsibility in the value chain?
Speaker D: Arguably. Arguably. Potentially, yes. But again, there is this catch 22 between safety and gendering capacities and raw capabilities. And what it shows that the more you understand the model, the more you can plug it back to more raw capacities.
Speaker C: Do we need interfaces? Let's say we understood generally how these systems were being built, and that's an updated thing that we do. We monitor how these systems OpenAI monitors it, and it's required to publish this somewhere else like they already do, and they're discovering new use cases all the time. Right. We also have an iterative potentially automated process of developing new evaluations. Anthropic has scaled supervision.
Speaker D: What scale supervision?
Speaker F: Yeah.
Speaker C: Anthropic has this perspective on, like you can build automated evaluations for new kinds of uses so you can discover a new use case. This is how people are using it. We didn't anticipate it before. It's not part of our evaluation suite. We've discovered it, though. We've built the evaluations, which can be done quickly, let's say, in this future. And you, as open foundation model provider, have some requirements to have some coverage over that through an evaluation suite. You don't need interpretability. You're just like we haven't probed it everywhere, but we probed it on 10,000 dimensions, which reflect the constantly evolving space of security. And if you're going to do well on a bunch of different dimensions, then Goodhart's law, which is a way that these measurements become worse over time because people hyper optimize for is less of a concern the more measures you have. It's easier to do well on a billion measures by just building a good system than screwing up in each partial one. And that will allow us to at least have the beginning of saying, what part of this evaluation suite is your responsibility foundation model versus what part is the responsibility of the next layer or the next layer? To me, interpretability is one avenue towards measurement. It's a way of understanding the system. It's not as robust deceitfulness from the model. So people in the existential risk are worried about these evaluation suites being subverted by a model that understands it's in a testing regime. But unless we're in that area, it seems like we can get a lot from focus on evaluations requirements around them and an understanding of the kinds of systems that these are actually being used for prompt space.
Speaker D: Just to be clear what you mean here, what you mean by evaluate. No access to weight just based on prompt engineering, correct?
Speaker C: Yeah.
Speaker D: Okay.
Speaker C: Yeah, exactly. These systems, they're black box. I'm going to treat them as black box. There are prompt responses that are the fundamental building block of how these systems interact with the world in different kinds of ways, whether those get interpreted as like, API calls or whatever. This is the thing that it does. And we should create a robust security harness for that which is potentially application specific or can be generalized into foundation models for a broader range of foundation models. And that's what they can be liable for.
Speaker B: I don't know.
Speaker C: I feel like this is possible.
Speaker D: I think we need to go one step beyond to advance the science of evaluation. But these two are not mutually exclusive.
Speaker C: Yeah, for sure. Anthropics automated evaluations is at the tip of the iceberg of what needs to be evolved here. And there are recent fun papers I'm very bullish on evaluations, but papers both on how can we do a better job in evaluating or even defining the kinds of risks that Flo was saying. Like, we don't have a good taxonomy risk. He's right. But there are people that are trying to define this in terms of both critical, critical risks, existential risks. There's another theme of evaluations which are in the ethical space, and that group which is evaluating bias. And these kind of things has pushed out the kind of innovation in that space. Is thinking about evaluations on societal scale, like, how are people, how is society interacting with AI systems right now? Your AI system, did you do stakeholder interviews in some way that then can survey them? That's not about the system, but it's about that system's interaction with some space of people. You're thinking about this quite a lot.
Speaker B: Do you have kind of perspectives here for more context?
Speaker A: So, as I said, I work at this organization called Partnership on AI. So we get to bring our partners together through working groups, essentially to try.
Speaker B: And see if we can kind of.
Speaker A: Arrive at a set of norms through a participatory process that everyone commits to. So the process that you're referring to is one that we've been running for a couple of months now, is to try and get model providers to agree to a set of norms around transparency, primarily.
Speaker F: Transparency of what?
Speaker A: One of the things that we're thinking about is transparency of model usage. There's folks within our working group who are asking for calling for transparency around the components for your training data or compute. And what's been interesting is that risk taxonomy that you kind of spoke to, I think part of the conversation now is people are talking about malicious uses models, kind of engaging in power seeking behavior, potentially using these models in safety critical infrastructure. And then there's the ethical and societal risk. And what we're kind of discovering is we can't solve all these problems at once. So part of trying to make progress now could be having some sort of surface area. Let's agree on addressing some risks as of now, and then let's think about updating these norms over time so we can think about perhaps some of the risks, which are kind of dubbed as more speculative because a lot of the kind of evidence around power seeking behavior.
Speaker B: That these models can engage in.
Speaker A: As we've been talking about, I think there's this question of scientific consent that's partially missing. So what we're trying to test out is can even operating within the Oaton window, right, things which people are generally in agreement about, can that be powerful? Having that as a starting point where everyone agrees to the same kind of set of norms and then trying to push that overton window as the conversation kind of evolves, which perhaps regulation can't.
Speaker C: Afford potentially if it's hard coded, we.
Speaker E: Just don't share the same window. Such a span of windows. Some people don't want solar power because they know they like coal or something.
Speaker C: Like.
Speaker D: There is a consensus on one catastrophic risk. One most is the bioweapon capacity. This one, a lot of people have come around. It is being discussed. But apart from this, yes, there's a lot of distance.
Speaker H: I think one thing on societal impact which is interesting for me is aspects which can be measured and not measured. Because a lot of times when we're thinking about societal impact well, I was thinking about this deeply, which is a little different from when we're thinking about measuring AI systems is before I started working more deeply with tech, I was working in education. I used to work with lots of governments and we used to do all of these randomized control trials where you're kind of evaluating the impact of a program. And I was thinking what are the parallels between evaluating a randomized control trial which a government is doing for like, I don't know, vaccine distribution, pharma or pharma or Pharma. Sure, that's dangerous. And what are the parallels that you can draw in terms of evaluation of a technical system? And can we learn something from non technical evaluations for technical evaluation? And one of the things which kind of has been top of mind, which is not an evolved thought for me is there are things that you cannot measure or you cannot quantify. And so to that extent so for instance, an outcome of that would be like if you're thinking in education, there are learning outcomes which cannot be quantified. And so, to your point, when we're thinking about societal impact, if you're thinking about mistrust or if you I mean, there are I don't want to be quoted on this, but there are instances where in India some of these models are being used for misinformation, blatant misinformation. And so the impact on trust, what it does that cannot be quantified or evaluated. So I think part of the question is also thinking through what are the dimensions which will probably often be qualitative but have to still be taken into account in some sort of like report card of what's happening. And this is a lot of it is more at the application level than at the model level.
Speaker C: I'm maybe more bullish on quantification in that, I think in social sciences, when you operationalize something so trust in AI systems, you come up with some measurement strategy which is not going to be perfect, it's not going to capture all aspects, but that's the job of some organization, a government or whatever to monitor these kinds of aspects. Some nonprofit group and I guess this is a San Francisco. I do not think we should let the perfect be the enemy, the good here. There's like a lot of the first component of signal that I think we could be advantageous. And I believe that we could have a faster moving measurement regime than we currently have. When I think about one of the ways to deal with the meta problem of how fast technology is going to change, I think about capacity building. And capacity building is, at every scale, institutional organization, future societies, a part of capacity building. The humans that are being brought into ethical, the people who got involved in responsible AI because of fairness concerns are sometimes a true boon to this. We're in a different world than we were a few years ago, but it's great that we had people who got involved a few years ago. And I think about measurement and evaluation in this kind of same theme.
Speaker H: I think I say the same thing. I think what I'm trying to say is that to your point, yes, not making perfect the enemy of good, but also recognizing that can we be exhaustive in what we're measuring, but realize that certain things we can only measure directionally sure. And not attribute a number?
Speaker C: Whatever measurements we come up with, we shouldn't fetishize them. And that's, like, historically has happened many times over. Right. Someone in a well intentioned way said, I think this is like a decent way of measuring gross domestic product, and it's like GDP is what? It could metastasize a little bit.
Speaker H: Exactly.
Speaker B: That's the risk we have with regulation, though, or guidance, for sure. To do that, you'll have to have something to hit right in order to say, hey, we've done this analysis. This is how we do the analysis. If you don't have that hard coded in law or regulation or guidance or something, what else are you shooting for? Unless it's this kind of it's aspirational that you should do this. And then it leaves people kind of.
Speaker E: Wondering what to do, why?
Speaker D: I referred to clinical trials because one of the way in which we, the humans, have handled complex social technical systems, when they put at the center deep black boxes like the human body, where the first principle is do no harm. So precautionary is a regime of clinical trials. If you look at what the FDA does, it's what we're doing here with those self driving cars. It's a series of clinical trials at scale to expose those systems to a number of circumstances, both quantitative at heart, but also qualitative and see how they behaved. And I think that the regulatory regime of the FDA is an interesting example. With its problems, it takes billions and a decade to bring a new drug to the market. Can we afford that? Do we want to afford that? We're not sure in all cases. But in cases high risk. For example, the case where we would push this, we are already pushing these systems. Like in self driving cars, we are not deploying Avs without XYZ. That's why I use that analogy of clinical trials.
Speaker C: Are you saying like, there's a risk reward trade off? So the FDA's strategy may not be the highest I'll use expected value, but it's the most anti fragile to it lowers the chance of atrocious errors while potentially stopping us from benefiting from certain things.
Speaker D: And remember, we society, when I look at FDA, are trusting FDA to help us solve one of the biggest problems cancer. Is there a strong societal consensus on solving cancer? Yes, of course, I think. And yet so we would want there would be good incentives to jump and say let's try, let's break it. But because we have these human bodies at the center and do no harm, we are going through the system which is creating moods, entrenched position. Let's be clear here too. It's not all rosy, but it's the kind of calibration to try and move not so fast, break not so many things, especially to the human body, and yet deliver a technoscientific revolution. And I use this analogy because of the core black box at the center. Like, do we have a good understanding in how the chemistry and the biology of these active agents operate in our bodies? No, we don't. This is why we move from categories of tests like they are right now with those.
Speaker F: Problem with that though is like the atomic unit of harm is at the human level. And presumably after millions and billions of years of evolution, we've arrived at a point where human body sort of is self corrected, or what I mean by that is it's just a stable point?
Speaker C: Yes.
Speaker F: So essentially humans reproduce and then they keep reproducing. And as long as there's not a bunch of environmental factors, presumably humans could continue sticking around for a while. Whereas in the case of these AI systems, we don't know what the atomic unit of health is. And so therefore it's hard to create tests.
Speaker D: Absolutely.
Speaker F: Because you can't isolate and have different populations that you're testing these technologies against.
Speaker I: I'll chime in on it.
Speaker F: So I guess I wanted to bring up two thoughts that I was having just about this conversation, bringing it back to the idea of guardrails. I guess I'm thinking more automotive. But in the case where you're driving along a highway and there are literally like guardrails, we also assume that eventually there will be accidents and collisions and things like that that just happen as a result of operating these types of machines where there is variability in the functioning of those systems as well as environmental factors. And we have an apparatus of repairing when those situations occur, whether it's police going out or tow trucks or any number of other systems that have been created to address the downstream negative consequences in a rather orderly way. It doesn't sort of stop all traffic for all of time. We're sort of like okay there's an accident, let's sweep it off to deal with that. So I guess I wonder if there is a similar like if we're thinking again about guardrails the point is to continue motion happening like the locomotion of the system should continue going but then if there is something that breaks presuming the risk is at the right level. We can then bring in whatever ambulances or something to fix the thing to get it back into healthy functioning. So that's one concept. The second concept which is related and similar and this comes out a lot of my work in open standards and open source is a question about creating a new sport versus creating teams that play that sport. And so when you create a set of standards and interoperable standards you're essentially saying this is how we want people to compete. Because these core concepts, kicking a ball into a net is a way to score a goal and there are essentially efficiencies that are in that sport that then govern the way in which the different teams play that sport. So when you're designing an economy, you're designing a way, let's say for different companies to then compete within that sport. And what is a red card and a yellow card and what is okay and not okay. But you have to have an enforcement mechanism that whether it's fines or other types of regulatory regimes that cause people to come back to play the core game. And so I'm just looking at these two kind of analogous spaces. One where you have vehicles and travel and you sort of assume that there will be breakage along the way that needs to be addressed. And then other cases where you're designing a sport for people to play in which is building these AI systems. Feels like maybe two other worlds where we've worked through these systems. I guess again, the perfect is not what we're aiming for but we're creating structures or spaces where people can compete to create better and better systems that serve more and more people. You're looking to be like you don't have any idea what I'm talking about. No, which is fair, which is fine.
Speaker C: I think it's just it comes down to when there's a misstep like the existential risk people, they're like so huge that it's not going to just can be swept up. But for Britzio yeah, I think I'll.
Speaker I: Ramble for a bit. I think I find it interesting that whenever as humans we try to make predictions based on our prior, all the priors that we brought up in. Terms of this kind of technologies like big industries, we got nuclear industries like airplanes, cars. What I feel is that all these industries have one thing in common which is like building whatever their purpose is to build is hard in that you need specific equipment, you need a lot of materials, you need a lot of expertise, you need to gather these things and build things that exist in real world. Like real in a sense, like in the physical tangible world, these systems are different and the availability of information and the speed at which we're progressing in developing these technologies is unlike anything that we have ever seen. And, for example, how would you apply the protocols that we have in place to ensure that medicines and drugs are safe to these kind of systems if anybody can develop their own?
Speaker C: Just think about how poorly, I guess this is, how hard it was to adapt the FDA to COVID and just as an example of a different contextual environment potentially requiring changes in the yeah.
Speaker D: And whether they would be suited, whether FDA in its current setting will help us cure cancer quickly. The balance between fractional and precautionary.
Speaker I: What I'm saying is I find it extremely hard to conceive even like an organization that can enforce this kind of level of control across the entire human population.
Speaker E: I don't know if you're finished, sir.
Speaker I: No, I can pause for that.
Speaker G: I was just going to say so.
Speaker E: This example keeps coming to mind. I'm like no one talks about it, but for cars and airplanes it's industry manufacturer associations which are voluntary associations of industry leading parts producers and that's how they enforce standards and it's like.
Speaker C: But it is a company, it's associations of.
Speaker E: Manufacturers like wireless internet is a great one. Like this is company led association where they are all agreeing on the next standards to guide technology because there's positive sum games you play.
Speaker I: Yeah, but these are companies that are willing to obtain or enforce or afford to obtain by law. So what I'm saying is if we are created a technology that individuals or small groups of individual can bring forward and develop in the garage or with a set of laptops or whatever and even if there is no dramatic sort of technological improvements run by single individuals. The actual usage, the adversarial part that I think you mentioned at the beginning of our chat, damages that derive from that kind of behaviors are still very high, or it's potentially dangerous behaviors that can arise from that kind of application of these technologies, even if there's no effective technological improvement.
Speaker C: We're going to see. I think just like the 2024 election, here is a good focus area of but politicians in the United States, one of the things that they just like each area we focus like artists care about copyright and art governments are interested in how is this going to destroy or corrupt the democratic process?
Speaker I: Which I mean, that's actually like something you said. It's not existential or is it.
Speaker C: Problematic?
Speaker I: A question that I would like to for us to go around is like, what do you think is existential risk? Because everybody has used it in catastrophic risk.
Speaker D: Extreme now?
Speaker I: Yeah, I think we should do around Robin. It's like, what's your catastrophic risk? Or even extreme?
Speaker D: Extreme is not a new point of calibration, which I find personally useful. We can discuss more. Like, if we want to politicize around extreme, I think it's easier.
Speaker C: Than getting into it's. Like, can you think about something that's like a fucking big deal and problematic? Yeah, that's what we're talking about.
Speaker E: Asteroid level.
Speaker D: Yeah.
Speaker C: Not even one person for the trust thing. He used the phrase epistemic apocalypse. Epistemic apocalypse. But yeah.
Speaker I: No, that was my question. Maybe with the last question we can go through. I just wanted to bring another point to the idea of would you open sources the weight of the models or whatever you make in public? I think if we do the parallel with nuclear industry, would you open source, say, the plans for a bomb, or how would you compare it with open sourcing, like a specific model? In my mind, I wouldn't be as worried if you open source the plants for a bomb, because the constraint on the bomb is not the technology, it is the availability of feasible material. And that is a fucking big problem, too. If you want to create your friendly neighbor nuclear reactor, you would have still quite some issues of finding uranium. It's not the same.
Speaker C: Just like AQ khan, though, had he not open sourced stuff.
Speaker D: He doesn't open source necessarily. He transmitted.
Speaker C: Yeah, exactly. Kim Jong UN might have fewer nukes right now, had he done that. So this comes up in a lot of these conversations. There's a paper called The Vulnerable World Hypothesis, which is about this topic, which is like, what if it was easy to make nukes? And what would be examples of that? And Boston is the one who wrote it. He talks about engineered pandemics being a potential vector of these easy nukes. AI is another one, and he tries to go through what would you do? Maybe this is like a security versus privacy trade off. And if there's such existential risk from how easy it is to do these, maybe you need a ridiculous security regime. Who knows? The way we organize our society, the optimal way to do it is going to depend on the challenges we face. The challenges we face, as we all know, are changing relatively quickly, both because of lagging recognition of things that have been happening for centuries, like climate change, but also exponential technological change. And AI is a good conversation topic because it's one of those things. But there are people who are still interested in engineered pandemics because these are other technologies that can be incredibly problematic. So I want to throw a question in, which is when I see the challenges for guardrails, for governance, for keeping up with this rapid changing landscape, it seems nest necessary to me that AI is part of the solution. Like AI must be part of how we evaluate AI systems, how we potentially govern them, how we enforce. I don't know exactly how this will how this relates to human in the loop, because at Credo we think a lot about how to know empower people still have humans be the governors, the responsible people who are responsible, but support them with AI as often as we can in this kind of iterated amplification display, whatever. There are a bunch of different ideas of this, but I definitely feel like that's just necessary. It seems hopeless otherwise. I'm curious about others. What is your take on AI systems being part of the solution here? Or do you think we fundamentally do need to maintain it to be like a human only kind of system?
Speaker H: I don't know if I'm completely answering your question, but something came to my mind initially when you said that I was listening to this podcast with Brian Stevenson and he was talking about basically mass incarceration and how a lot of these models basically targeted more black people, et cetera, right? Like we know about that problem. And his response was that the same system that was used to incarcerate the wrong people, you can use the same system if used in the right way to identify what were the wrong incarcerations. You can use the same system to identify which of these people need more mental health support. And I think that shift of perspective was just a little bit of a light bulb moment. It kind of is obvious what he's saying. But the root of that was the same thing as what you're saying in that you can use the same AI system. Not in a godreal sense, but the problem that was created by AI can.
Speaker B: Also be solved like AI or checking type of a thing built within them.
Speaker A: Would it make you feel better to.
Speaker B: Have a third party AI do that for you?
Speaker D: It's not that we are unbiased like judge has been proven that in certain circumstances we the humans, the coincidence have not demonstrated the level of bias we find ourself into. And so it cannot be fundamentally one or the other. We need good sociotechnical system to be less wrong about ourselves, be better at what we do, but control the technology and how we do that. It's a question of always of balance. So I think it's dangerous to be fundamentalist. We will need but we love it at the right pace, we have it with the right control. Can we sort of alignment and control at the same time? And as humans we tend to it's a pattern. When we delegate to technologies, if the conditions are not gathered for good decision making to happen pretty quickly. We get into what we call rubber stamping where we the humans are said to be. It's not an automatic decision making system. It's a supported decision making system. And yet it becomes de facto an adm recipe for disaster. So as we do that we need to elevate a wall of dignity to ensure that something we specify as a support to decision making system this never becomes an adm and some criminal justice support decision making.
Speaker C: This person is 99% chance going to commit another crime. Are you going to let them go? What are you going to do? That kind of is not support that's just like what liability would I have if I did it?
Speaker I: I have to say yes.
Speaker G: I was going to say it's an.
Speaker E: Interesting counter perspective here and this is a general trend I'll pause it too, is that if you have a new capability that gives an asymmetric threat advantage to small actors then people in general will be more willing to surrender their civil liberties and form more intensely monitored security federation. Right?
Speaker C: Yeah.
Speaker E: And we saw that with 911 last 20 years. And so this is so there's a it's not just Minority Report for good. It's what's the downside of Minority Report world which is that everyone is now surveilled and monitored. Exactly right. So there's an argument against it which is kind of like wait, what's happening to our society? Do we want to surrender? How much do we trust centralized authority today?
Speaker C: I guess rather than push it in this, I'm making a more modest claim which is I think our governments, I mean lots of people put us out. Our governments, the way we understand our world, the way we empirically create policies have not evolved very quickly. And I think we're going to hit a struggles that will be enough of a pressure that the governments that start to adopt the AI systems to support their function will outcompete governments that don't because those governments will be able to get services to more people. They will be able to understand like we talk sometimes about. I'm sure hopefully most of you use chat TV teachers do one of the things it's best at, which is to say I am this kind of person. I'm trying to understand this thing. Can you explain that to me in the way that I will understand? And there's hallucination problems and it's not perfect but it does a remarkable job in being able to get it to.
Speaker D: You and if you don't get it.
Speaker C: You'Re like can you dumb that down a little bit or actually I can understand that better. Can you make it more complex? Even that component in supporting governments to understand the many things that they have to interact with seems critical. Not to mention the automated evaluations and everything else. I just can't see us actually maintaining even as corporations who are trying to govern themselves dealing with the rapid change without AI being part of the solution.
Speaker F: Just to pick up something. You said, again, triggered this reference back to healing or fixing or adjusting. In the corporate setting. You're trying to limit your liability in sort of absolute terms, as opposed to designing systems that are more resilient or antifragile, where there's an assumption that things are going to go wrong. But that the way in which you recover and then address or fix whatever went sideways actually is the function of the system or the purpose of the system. So in other words, when you're talking about government services, I can imagine that a very naive and stupid implementation of an LLM style chat bot for government services is one where you sort of end up in these cul de sacs of nonsense where the system hasn't been tested.
Speaker C: Cul de sacs of nonsense is a great phrase.
Speaker F: Seems like it's a very relatable experience. And as a result of not having tested all the way down to that sort know, cul de sac of nonsense, you end up in, like, Kafka esque sort of experience that's horrible. As opposed to almost like designing these. Systems to be somewhat more improvisational, such that you're like, oh, it seems like we've kind of, like, hit a cul de sac of nonsense. Let's try some sort of let me give you a ladder to some other roads of sense making that we can arrive at an outcome that's closer to what you want because you have no idea what the words are to the magic incantations or the prompts to get the service that you want. Because the language that government uses is esoteric and bizarre and very specific and precise law.
Speaker C: But we might be able similar to APIs that are some like there's an example I love to bring up, which is one company we were working with was a hiring company and they basically had a database. They said they were naya company, but they weren't. They were just database. And the way people interacted with it was through complicated Boolean expressions. Most people, if you were naive, you would just add in one or two. But recruiters have become experts in this Boolean language where they're like this and this or this. They can craft ways to find black applicants by just doing long lists of historically black colleges. They have interacted with the system through this crazy shitty interaction and they have said the way we're using LLMs is now you can write what you want and rather than it finding it, it creates the Boolean expression for you, which is a good I think there's a great use of it. It both lets the interface be more accessible. It also puts a safeguard in that it can only interact with a system in the way that it's already been designed. And that's an example of I mean, a lot of people have pointed this out that one of the potential things that chat GBT allows us to do isn't all of its functionality, its reasoning, but as a user interface improvement to a huge set of things that are jargon heavy or not approachable. And maybe government services are one of these as well.
Speaker F: Just to build on what you said, it occurs to me that so many government services and again, government is just sort of like a shorthand for where this will ultimately end up, I think, which is the system that rules and determines how each of us gets benefits from our pooled resources.
Speaker D: Collectively.
Speaker F: That's what the government is for, essentially solving the common actor problem or whatever. But bureaucracy comes from bureau, which comes from desk, which comes from sitting across from someone else and asking for a plot of land or asking for healthcare services or asking for something else. But the government then has a set of rules that ostensibly a set of people, whether they're elected or not, decided upon for later people then to come into interaction with so that there's some fair adjudication of the provisioning of resources. I know I'm using a lot of words, but thought is that instead of having a bureaucracy where you're sitting in front of an officer that is executing the set of rules that were previously decided upon if an AI. Allows you to sit side by side and sort of look at the problem together, to collaborate on the outcome that completely changes the way in which governance can be thought of and construed.
Speaker B: I would love there to be some sort of TurboTax or just like interpreting local governance that people just blindly accept and say like, yes, you're an average person, you need to know how to do something, you go on it and it's been vetted and approved by some.
Speaker E: I'm actually building this. It's a company I have with some friends. It's for government services. If you want to talk later.
Speaker C: We'Ve fallen into government services a little bit.
Speaker F: I'm thinking about guardrails.
Speaker C: Yeah.
Speaker F: Again, I am trying to keep this and I'm trying to both think about this from a tactical, tangible sort of context while also thinking about the guardrails that should be in place, that allows for there to be it's like kids playing in a sandbox environment. You talked about regulatory sandboxes before. What are those sandboxes that are created for different companies, providers, services, government, whatever the entity happens to be such that the outcome allows for some spillage, some breakage, some crashes on the highway, but then can get fixed and remediated while you're maybe in the same conversation or the same context.
Speaker C: And people talk about this in terms of monitoring regime, like what kind of monitoring do you need? Which all immediately comes to what kind of evaluations are you doing and how do you approach them. We are at 535, so I'm going to start us. It's going to take a while, I'm sure, to wrap us up. If you need to go, feel free. But I would love if we could go around and if there was an idea or a thought that you thought was particularly interesting during this conversation or even had, but haven't yet mentioned or wish we had spoken about, whatever is your last kind of perspective, we'd love to hear it and take a minute. If anyone wants to start us off, because they already know. They're like, yeah, this was cool.
Speaker D: I take off. So maybe I'll throw one last idea and coming in I was hoping that one helpful way and it's far from imperfect we can delineate the question of the good calibration of guardrails is this notion of industrial. It's not at all the same to handle ten customers as a startup or 100 million customers. The prospect for profit, the prospect for risk and damage are like all there's a magnitude difference and oftentimes in the world of governance we have failed to see that no, you're not a startup anymore, not even a scale. And forget about that. You are very much industrial. I think that's one thing that it's not silver bullets but it's one of the things that over the past 20 years of attempt to digitally govern our societies we have missed upon this obsession for startups and our scale ups has left us in a way, shine. How no, you're not a startup. You are an industrial player. You have the cash, you have the talent, you now have responsibilities of a new age.
Speaker C: I love this. This is part of I forget it might have been within crater with Andrew. Sometimes talk about the number of things a car manufacturer has to do that every single part of this supply chain has immense responsibility here and they can do it. I mean, maybe it's quashed innovation in the car space, maybe we could have much better cars. And the counterfactuals are sometimes difficult to understand but we as society made that choice in that area and the only reason we're making a different choice here is because not because they're consistent, but because a completely different culture is the car industry from the tech industry that is reporting. AI, thank you so much for coming. Have a good flight tomorrow.
Speaker D: Thank you so much.
Speaker B: Yeah, I kind of have a similar.
Speaker A: Reflection, which is why I wanted to follow Nico. I'm kind of thinking of the polluter pays principle from environmental law as well.
Speaker C: I'm not familiar which one is it?
Speaker A: It's called the polluters pay principle. Essentially, if you are emitting gases or any sort of pollutants in the air, it's your business to figure it out. And I do wonder often because of the tech culture perhaps that we often don't think of that as a compelling framing because a lot of what's going on is kind of this discussion around analogies and framing, right? Airlines. I mean, just think about all the analogies that we've used just in this one discussion. So there's a competition of which analogies and which framings are going to win out. And it's not trivial. I think it's important because those parallels can apply. So I think the polluter based principle is compelling to me, even after this discussion, that there's a clear information asymmetry folks from outside of these companies are going to struggle to be able to offer solutions, in a sense because of the lag which but who's the polluter?
Speaker C: If your system downstream was connected to the financial system, you're using OpenAI system as part of it, and it crashes the market.
Speaker A: Who's the I mean, at the very least, I think the originator is the person putting out the model. Arguably, there wouldn't be anyone downstream who's either incorrectly using it or misusing it if the model didn't exist in the first instance. But I recognize that a blanket polluter based principle does not work. Then you're going to stifle innovation.
Speaker F: Sorry, just to clarify, if you think about Facebook and Cambridge Analytica, largely Cambridge Analytica would have been guilty or whatever of that abuse, whereas Facebook was not. So do you think that was just as? Again, sort of an analog maybe it's not perfect, but relative to Facebook having an API, they controlled access via the API, and then it was used for a nefarious purpose. Should Facebook have been responsible because they.
Speaker C: Made the platform available?
Speaker A: Yeah, I think it's complicated.
Speaker C: Right. Like, think about the panel kind of conversation.
Speaker F: Facebook could have not offered naked AI.
Speaker H: Yeah.
Speaker A: Another panel could be OpenAI, for instance, using Sarma to kind of hire data enrichment workers in Kenya, for instance, who they didn't follow some of the safeguards they were exposed to content that they're not supposed to be. Essentially, folks who are training the data sets, who play a critical role in the reinforcement learning.
Speaker I: The company they used actually, yeah, the company yes, we used it at the previous company.
Speaker C: This seems somewhat related to Alison you were talking about. Maybe this is in the UA act like foreseeable or risk. You have these kind of qualifying phrases to try to set the groundworks, which will probably be defined in jurisprudence, in the future of guidance.
Speaker B: I think responsibility is a big question here, and that identifying the different components in the value chain. Deciding who takes responsibility, I think, is really important for knowing what guardrails we can set, because at the end of the day, if someone violates one of those guardrails, we need to know who's responsible. And the black box problem is difficult because we're asking companies, in some sense to define the black box rule. We're like, okay, gradient descent great. It's doing its thing with all this data, and there's some weights here and there. We're asking companies in some sense to be as transparent as you can, do as much as you can to try to define the rule that we're kind of trying to go around by using some of these more complex algorithms, right? That's kind of like based on intuition and all these other things. And then you have all these different components, whether they're people helping out with the training, there are contractors, there are people who are fine tuning all going into this huge black box weight problem and we have no way of assigning responsibility. And I think the transparency can't go to the level of defining a rule that defeats the point of AI. Right? That's kind of an amazing feature about AI. In some sense we'll learn probably about our world, about language, about how like medicine, just by doing running these algorithms, we'll probably discern big rules from them. But the big part of using these AI systems is that we do not know the rules. There's something that requires a complex algorithm and weight to get that. And there's a lot of folks involved in that process and there's going to be surprises along the road. And that's inevitability. That's the nature of tech. That's always been the nature of tech. And we need to figure out what to do and who's responsible. And perhaps open source AI is the answer because I don't know how else we'd be able to protect some of those things. But I think that's the big question is defining responsibility.
Speaker E: I could share some things related to what you said and what sort of emerging thing here, and this is maybe my mental model coming out of this conversation that tries to address the perspectives here. But it sounds like we believe that or a theme here. It's really that responsibility has two components. One is attributional liability and the other is agency over decision making. The agency over decision making responsibility should be proportional with the trust we have in the agent making the decision. And people are black boxes. And if some person had a history of repeat offending a certain kind of crime, you wouldn't put them in a position of responsibility over that kind of situation. And I think in the absence of trust, you need transparency, which is monitoring and explainability to a person. I think a similar standard or principle seems reasonable to apply in context of AI agents, where it's like, unless we have lots of trust with this model, don't let it be a therapist, let it make boolean church queries reg x functions. That seems safe, right? It's untested. As things become more known, more trusted via past scenarios, you can give them more responsibility, or in the absence of that trust, you need transparency, which is mechanistic explainability.
Speaker C: That's one of the reasons why if open source models are delayed from the frontier by enough time, it might be okay. Because if the frontier model provider has a responsibility, especially before pre deployment, for a tremendous amount of checks to establish trust and then afterwards we're like, actually this is working out pretty well. If it gets recreated, then we can feel more confident about it. That's not exactly how things are going right now, but open source models are not currently, at least at the Frontier, and they're probably never going to be. I mean, I won't say never.
Speaker I: It's not very good.
Speaker C: Yeah, I'm not going to say never, but that's one aspect of where sometimes one of the responsibilities like GPT four, there are rumors right now that it's a mixture of experts, model, and what that essentially means to me is that means it's not gigantic, it's not just a humongous model. It means that it's probably more quickly going to be made in some open source form, like someone's going to figure out how to just put a bunch of Llamas together. You're going to have a GPT core. I actually might beg to differ with that. I think it makes it more complicated. Okay. Maybe it's like a whole different technological improvement to go on. My only point about it, regardless of which direction, is that before GPT four, we didn't know exactly how much you can do with these models. OpenAI takes the responsibility for showing the world, hey, you can do this. Which means that a bunch of people are going to work to make that happen. And hopefully before you show someone that you can do this, you've taken responsibility for some component of the risk space related with that Frontier model, it's like Kobayashi before he showed the world that you could eat 56 hot dogs as a person. Also showed that he did not die.
Speaker B: By eating 56 Black Swanseas right there.
Speaker I: Sorry, it was a Star Trek reference, but I don't think it was.
Speaker C: There's a lot to learn from Kobayashi, really.
Speaker I: I don't know. You talk about he's a hot dog.
Speaker C: He'S an athlete of some sort. Other takeaways? No, I was going to say or Open questions.
Speaker H: I don't know if it's a takeaway, but one thought that's been on my mind, which is potentially answering your question on catastrophic risk or whatever word you want to use, is that as of now, a lot of conversation around guardrails is very I'm probably stating the obvious, but like US EU centric. And I think for me, the bigger question is which countries have we excluded from the narrative, knowing the fact that also when you talk about the fact that it's mostly English speaking and the world is not English speaking, well, there is stats to show that 2% of the people globally speak English at home. 2%, and more than 60% of the Internet is English. That just is just the sizable implication.
Speaker E: Of who's the most common language spoken in the world.
Speaker H: It's a 2% at home that is.
Speaker C: Changes upper perspective.
Speaker H: Yeah, I mean, it does, sure, but I think it's like English is not my first language either, for instance. So then I guess the idea is.
Speaker E: Most English speakers, it's the second language.
Speaker C: Yeah, she's saying like, the stats where lots of people speak English is not. Reflective.
Speaker E: Totally.
Speaker H: Yeah. I think what I'm talking about is, who are the people you've left behind by virtue of the fact that you focus only on English as a language representation? And then what are the countries you're leaving behind by virtue of the fact that English speakers are designing your guardrails or talking about your guardrails or have the vocabulary or taxonomy of risk of Guard and Guardrails? And that for me is an open question and not many people have an.
Speaker C: Answer to that in the most minor way. Well, it's both minor. Not so minor. Some of the largest AI conferences have recently tried, hosted in Africa, for instance, so that they could bring a completely different subset of machine learning practitioners into the fold and also give an avenue to network and basically within the confines of a conference's power try to rectify this in tiny ways. But even if they have it every once in a while which is better than never yeah, for sure. Completely kind of disempowered here. How do you represent even within English speaking weird, you know, countries there's a huge diversity of values and ideas and they're only tacitly all reflected in our governments and principles. How do we expand that to bring others to the table?
Speaker H: The obvious way is when we speak with companies, it's often if you are setting up within the enterprise the responsible AI pod, for instance, how are you bringing diversity of thought? And it needs not be a checkbox of like I need a man and a woman and it's just diversity of thought from multiple lenses. Like how are you ensuring that as an organization? Yeah, and I mean, the other thing also is that culturally there are different cultures where being involved and speaking up and bringing your perspective is very different in different cultures and being able to accommodate for that diversity or that like if even if you bring together very diverse people, it doesn't mean that the output is going to be diverse unless everyone has equal airtime, say, for instance, or every person is involved at equally critical points of your lifecycle. Product lifecycle. Ms. Lifecycle. And that is something to be accountable for. Right. I think it's more of a conversation with I don't know how it's implemented in tech companies, for instance, but it is definitely a conversation with enterprises when they're thinking through, okay, how do we ensure that? But I think for me it's also just like more of like a macroeconomic thought.
Speaker C: Yeah. When I see the organization I've seen who represent this most strongly like trying to get broader stakeholder engagement or a broader range are often quite wealthy enterprises who then have the luxury of adding on these extra aspects. But when we're talking about guardrails and standard setting, the organizations are like governments or nations and who is the higher level body to make sure that diverse voices are represented? One on the language front, cohere for AI I think is doing a pretty interesting multilingual effort to just collect the data set necessary. Like my partner, she speaks Canada and 40 million people speak Canada. But that's still not that many relative to these more prominent languages. And so it's underrepresented that's.
Speaker E: Right?
Speaker C: Yeah.
Speaker F: See you later.
Speaker C: Cool partnerships in AI. Had a partner forum recently and one of the first questions they asked, which is kind of the same as the question you asked, which is like, let's look around. Who's not in the room? That kind of question. Which it doesn't solve it, but it's.
Speaker F: A first step just building on that. One of the things that's hard, I think, in this is this question of whether it's guardrails or rules or a set of outcomes that are based on, to your point, like a set of cultural norms that are from a dominant culture as opposed to a plurality. So there can be a set of guardrails. I guess historically, these types of guardrails might have been enforced or built through morals and culture. And language oftentimes serve to reinforce those cultural norms and understandings through generations and through stories and through memes, et cetera. When it comes to applying this to AI and large language models, it's not entirely clear to me how to adapt these guardrails to different cultural priorities or different places around the world.
Speaker C: Are you familiar with the technique of constitutional AI?
Speaker E: Have you heard this phrase?
Speaker C: No. So there's an idea this is an anthropic idea, which is like when you do reinforcement learning from human feedback, you're essentially saying you'reinforcing the streets. You're saying learn the values of this group.
Speaker F: Exactly.
Speaker C: Whatever group that is.
Speaker D: Right.
Speaker C: And that's one way maybe you can have different groups. You're like, I don't know what they want, but I want to make sure Idioms are represented or whatever you can do like that constitutionally. I tries to make this a little more transparent where the input to the reinforcement learning system isn't people, though there are a bit of people, but it's a constitution you actually can write down, I want this system to be this and this and this. And if you look at what anthropic, how they tried to approach their responsibility, they took principles from other.
Speaker F: Global melting pot of constitutions.
Speaker C: Yeah.
Speaker F: So what I'm wondering, though, is, again, nature thrives when there is biodiversity and like a microbiome type situation. And if the outcome of these types of conversations is to have one authoritarian.
Speaker C: Well, let me just say it's not just that their constitution tried to it's that you can create your own constitution. You can have a different constitution. It's a mechanism through which different organizations, different peoples can say, this is what I care about, and create a system that is somewhat constrained. Or sometimes it's kind of the reflection of having the system itself might be able to represent many different values, but it has a propensity to act in hegemonic ways and so are there ways that we can shift it to be a better representative of Hawaiians in discussions with the rest of America? To be if it's a government representative, whatever. And so trying to find new technical mechanisms to allow you to have systems that are actual representatives of you, I think is critical as we I guess.
Speaker F: The question, though, is we didn't really start thinking about a plurality of guardrails for different contexts, cultures and places. And so that seems like very important, whether the idea is to have one regime which is sort of governed by a central body or one that's disaggregated and decentralized.
Speaker C: Okay, so we are at I'm seeing people get up. Thank you all for coming. Appreciated your time and participation. Yeah, thank you.
Speaker H: Is there a way for us to stay in Sasha?
Speaker C: Okay. We have been thinking about we created a discord community early on. No one really used that.
Speaker I: Do you want to stop recording?
Speaker C: Oh, sure.