Speaker A: Ah. We can each just kind of give, like, a little background about yourself, like, take a couple minutes and to help us understand what are our shared contexts and what's, like, the place we're all coming from with our different sets of knowledge. And also kind of what your interest is in this topic today. Right. So today's topic is national security and defense or national power and war, really, which is kind of more to the point. You say defense, really? It's offense, I guess. I've always worked in applied physics research, but I also did a degree in sociology and economics. And so I've had an interest at the intersection of both geopolitical changing world order, as well as the rise of science and industrial revolution and how that's formed the world order. Some of my interests in this area are really thinking about how society has overall become a lot less violent, actually. And if you look at as technology has gotten better for military conquest, the number of violent deaths has gone down over time in kind of interesting way, and especially in terms of civilian deaths during conflict. And it's like the kind of. I mean, it's been really bad in the past, but overall it's been interesting. I think the decision to declare war and to fight one another, this is, like, one of the biggest human evils, and whether we start outsourcing that decision to things that we think can think faster than us. Right. In real time responsive situations, I think there's a big question of responsibility and liability and morality there. And, I mean, in some extreme versions, there's this episode of Simpsons Way back, right, where Bart and Lisa go to boot camp, and this guy's giving a speech at the end, which is like, in the future, wars will be fought by robots in space or possibly on top of a very tall mountain. And your job is clear, to defend and maintain those robots. And so that would be kind of cool, actually. Like, if wars were fought by just robots in space, and the decision was clear, and then people don't die on Earth. Right. That'd be interesting. Yeah. So those are kind of like some of my curious topics. Are we getting less violent over time? Do more intelligent weapons end up saving lives? Will humans at some point, not have a role in fighting conflicts? And does that require trust from us to put those decisions in the hands of non human actors? Some of my questions, I guess, so we can go either direction. How about this way? I'm Gavin. I'm an application developer and kind of a science fiction nerd. My degree's in filmmaking, but I've been doing software for decades. And I'm kind of interested in the sort of the next rung down of bad actors asymmetrically enabled by higher tech tools. And how do you defend against that? Whether they're using it for direct killing or propaganda or manipulating responses in some way, that will have bad outcomes for a lot of people.
Speaker B: Hi, I'm Yolanda lanquist. I've been in AI governance and AI policy for six years at a nonprofit called the Future Society, incubated at Harvard Kennedy School of Government. So I have a public policy background, and I'm an AI policy expert at OECD and UNESCO and the Economist, and also at a lot of discussions like partnership on AI, where we're trying to set standards for AI corporate governance, but we're also advising on policy like for US government. Although one of the reasons I'm interested to be here is because national security is always offered as the example for why is the reason we don't want to overregulate. Right. So I'd like to understand that argument better. And then we also advise and do advocacy on the AI acts. In Europe, for example, we were instrumental in getting foundation models to be included in the AI act, which is great because it's like the only credible legislation right now. So our mission is to align AI through better governance. And we used to be on the harness opportunities, mitigate risks like AI for sustainable development goals. We worked on AI in Africa for a while for SDGs and security and Ethics and safety, but now it's focused on the governance mitigation of risks, because I think there's no credible policy and regulation right now, whereas we see more risk opportunities. And what I'm really interested in is security for large labs, Frontier AI foundation models, which is really insufficient in terms of they're not even, as far as I know, maybe some people here know better. I'd love to learn more. Not even doing some basic practices like this cybersecurity framework. Implement that. Right. And we know we have a persistent shortage of cybersecurity talent. Okay. Now, with open source AI, I'm concerned about bioweaponization, cyber weaponization, because any Tom, Dick, and Harry like the barrier to entry, to pull back guardrails, and fine tuned guardrails of open source LLMs can be done pretty easily. Fine tune that on pathogen or protein sequencing data for making pathogens or bioweapons, or on malware code for scaling cyberattacks. So I think that the intersection of open source AI and security is really interesting. And like cybersecurity that it's offense dominant because you can think of new novel attack vectors that are hard to defend against. Yeah, thanks.
Speaker A: Awesome. I'm on my phone. It's because I'm on notion. I'm not just texting and being avoyant. I'Mitchell. I have a background in economics, data science, and have been a facilitator for blue Dots AI governance and AI technical safety for the past three years. I've done it just like on the side. What I'm interested in is multipolar scenarios where we have both humans and AI agents kind of navigating some sort of complex, I don't know, like power struggle. It seems kind of interesting that there's going to be like agents that will have a seat at the table and how they will think about other agents and how they will think about other humans. I'm also, in my view, a lot of people have pushed back on this view, but I think that the first AGI creator will actually be the US government. I kind of track various institutions. I think in my view, that once you get close enough, any government will see that you can essentially have infinite number of digital people. And that company, whether it's open eye, deepmind, et cetera, would kind of be a national security threat. And that the US government was like, yeah, this is ours now, because we can't have you having infinite digital people. And they're more powerful than us. And the US at least has a vested interest to maintain its full world power position. I think that's really destabilizing, of course. But if I think about their position, it kind of lines out, but not like super confident in it, but it's something I would definitely want people to debate me on. And then I'll turn it to you. My name is Stig. If I'm on my phone, it's because I am being avoided. I do a lot of things. I'll give a more sincere introduction this time. The current project that I have two right now. One is miniaturization of particle accelerators for producing like a muon elimination source. And the other is I have somehow found myself starting a mining operation in West Virginia. These things just happen to like, to some extent an aerospace background. I've done a couple different startups. My first company was in hypersonics, so I have a decent amount of exposure to weapon systems development and stuff like that. I'm not in defense like super deep currently, but that certainly could end up there in the near future. Seems like my take on possibilities and risk related to artificial intelligence development for defense are slightly different than I think most people's here. I'm very skeptical of the idea of AGI in the first place, and I am much more concerned with. Much more concerned with more directly functional systems that we can actually see exactly what the development path is. There's lots of work in artificial intelligence, sort of less in open AI and more in fringe, more quiet research groups. It is somewhat difficult to contend with the moral aspects of it also just to understand its strategic ramifications, at least from the Americans in its perspective. At some point. How do I put this? At some point, missiles will be able to maneuver well enough to actually get to a carrier group. Currently, a carrier group is basically invincible, but improvements in guidance and control of all sorts of different possible weapon systems are capable of drastically changing a strategic landscape on the order of like 20 years. So anything about these things more in terms of these direct, factual terms? My name is Bima. I run an AI startup. But in this context, I guess what's more helpless is that I'm originally from Ukraine, so I've been following geopolitics past a year and a half very intensely. I do know a bunch of people in the bay who've been trying to help Ukraine through tech, but I think it's actually been mostly non AI stuff, at least, like things that I've heard about. So I'm curious to learn more and maybe deliver information to the right people if there is anything interesting discussed here today. I also wonder what Andrew said about the violence is decreasing. I've heard it coming from Stephen Pinker and similar people. And then there's kind of the opposing narrative of the complexity system thinking, people like Daniel Sparkenberger, who's saying this naive progress kind of narrative that everything is improving, but in fact, it might not be that we might not be tracking the right metrics. I actually looked up the graph just out of curiosity after Andrew said that the violence is decreasing. And I found out that actually the wars were delivering more and more death from one, five hundreds to the Second World War. So it was like very steadily more and more people were dying. So actually violence was very rapidly increasing. And then from the Second World War, it was decreasing. And I wonder if that's partly attributable to the US being the global police power, where they suddenly ended up with more economic and military power than the rest of the world combined after the Second World War. I don't know. Just the thought. Yeah, and looking forward to learn. All right, my name is Ed. I'm the founder of a startup that does Edge MCU for orbital satellites. Our first customer is with another NATO country doing board security, but we also work a lot with maritime domain awareness and also other ISR applications in the hyperspectral space. Before that, my first startup, we also did aerophotography for National Geospatial Agency and to a lesser extent the National Reconnaissance Office. So I am very fascinated. I've never really paid much thought to the philosophy of applying AGI in AGI in general. I've always been more fascinated by the practical, immediate applications. I will disagree with some of you who believe that the world is headed towards more of a multipolar stage. I think the US, it will still be a unipolar world for this century. Russia, China, Iran, none of them are serious rivals, or to the United States at least as a full on counter, maybe as a potential short term, medium term threat, yes, but not as a long term threat at all. I'm more fascinated, however, by whatever the hell the crap was, by the applications of autonomous robots and warfare and yeah, advanced, more improved guidance systems, the more practical applications that we see in 10, 20, 30 years from now. Hello, my name is Alex. I have previously spent some time working on autonomous vehicles, and then I realized that these sort of skills actually apply very strongly in the defense space, interestingly. And then I spent time working there, worked on two different defense startups there, realized that AI in the defense space is honestly not as advanced as many people think. And then honestly, this is also a very interesting area, an important area that people should be thinking more about. Yeah, that's pretty much. Oh, yeah, sorry. Hi, my name is DeMarcus. I'm Marcus. I've been working in AI at five, for around five years. It's typically generated by AI recommendations, a bunch of other weird stuff, most recently at Google X. Right now I do a bunch of hackathons and contracting work. But what I'm interested in specifically in this space is I formally work at Army Research Labs, so I am very in touch with the defense industry. I think that AI has the potential to revolutionize the way warfare is conducted, whether that be asymmetrical. I don't think we need gigantic vehicles anymore, for example, when drones can easily turn the tide for a lower cost. But I'm specifically interested in the idea of kind of like how the Milgram worked, right? In which people who are detached from the results of some action can cause a lot of potential pain and suffering. Right. Milgram experiment is like, the spirit was like, the guy, he's in the room, and he's pressing this button. It's going to shock someone else, but he can't see them, right. And then he continues to press the button. They continue to press the button until it becomes gradually lethal. Right. There's a bunch of critiques of that experiment, but just the concept of like, oh, you can be detached from this drone flying 10,000 miles across the world and delivering bombs or stuff like that is really interesting to me, and I think it's something worth talking about.
Speaker B: Hi, everyone. My name is Afika. I work kind of in aerospace now. I think I work for plant labs, and I work on flight software. And so defense intelligence is basically the customer of customer using the biggest customer using satellite imagery. And I'm curious about how AI plays its role in even something as small as edge computing on satellite AI stuff. That would say the biggest problem right now is downlinking images, is there's so much imagery available that it takes a lot of time to downlink them. And so there's a lot you can do with AI to process. We also have a lot of very good computer vision available in the world, but I think this can be improved and it can be made. Response to things happening on the ground can be made. That's much faster than it. Curious.
Speaker A: We absolutely need to talk sometime. No networking. Hi, everyone. I'm Ian. Besides the AI salon, I lead AI governance research at an AI government company called Credo AI. And the reason I was interested in kind of this conversation is I have in my head that while there are many different conversations going on right now on should we regulate AI, should innovation happen, what are the benefits to education and a whole bunch of other things that larger powers are thinking about, a simpler kind of calculus, which is, how is the US or China going to stay ahead of the other? And so I think that this kind of national power and war perspective, which I don't have a huge amount of experience with, is probably dictating the dynamics in ways that I'm not. As on top of Credo AI, we have some interaction with kind of the government and defense industry. It's interesting seeing kind of their perspectives on responsible AI and AI governance. But then I see kind of, there was a letter put out by the CEO, Palantir, a few months ago, and in this letter, there was a point towards the end where he was like, responsible AI is important. It's important that they're aligned with human interest, kind of like the normal things that people say, but then he's like, but we will not live in a naive fool's paradise where our enemies are going to keep up with the same kind of moral high ground that we are. And so we will not sit idly by while our enemies outpace ourselves. So basically, like, yes, responsibly. More importantly, we are going to push forward on the next generation of weapons. And they kind of push forward. They have like, kill chain, I think is the name of their system, which is the name, I think, of just the route of liability or responsibility. And so they named their AI system. I think that connects with both Marcus's point about kind of abstraction and Andrew's point earlier about who is going to be responsible for kind of moving forward. So that's the first thing I'm most interested in. Just like, how will national power war contribute to the trajectory that AI will take, that governance will take, and that development will take? The other thing is also, will we continue to be in this world of decreasing human deaths in this Stephen Pinker like, more peaceful area, or are we instead just kind of, are the number of deaths decreasing, but the kind of expected number of deaths increasing because we just have a lower probability of more extreme events that are happening as AI becomes more powerful? The AI systems that we develop are more powerful, and potentially we'll stay in a unipolar world, and maybe that is responsible for a big part of the dynamics of kind of international security right now. But it seems somewhat likely to me that at least the current power dynamics will, there's an opportunity, at least for them to be shifted heavily, and I'm not sure where that will go. Hey, everybody, nice to meet you. I'm Jesus. My background is in physics and computer science, and actually more relevant to this as well as economics. And what I'm doing now is running a startup, essentially, which is making consumer technology, but I think it's at least somewhat foundationally where fans consumer technology. I worked for some time, or at least like when I was a freshman, I interned at one of the giant military contractors. And what we were doing was AI for systems, for signals intelligence. So it was about telecommunications, basically, and signals in the air. But what I worked on particularly, like this was 2019 was the most advanced AI at the time, which was like Gans and then also some sort of like LSTM sort of stuff, but yeah, like RNS. So I was hard pressed to find people who were really experts in this, in at least my vicinity when I was working there. And we, the interns, were, like, leading the charge on this, and people seemed to be pretty interested and impressed by this. So I got the feeling from that, that maybe the contractors aren't as advanced in this as we might think, which is what other people have talked about. I'm very interested as well in history. That's one of my lifetime passions, probably because I played too many video games like SId Five, and thaT's. But the more advanced you get into the strategy simulator games, you actually get really deep into history, naturally. So I'm curious about the unipolarity of the world because I am not of the opinion. I'm curious why you think this, that America is necessarily going to remain. I don't think it's that likely, actually. I think China is. They have like a great shot basically for a lot of reasons, like largely manufacturing, but also just simply population is such a huge factor in a country, I believe they can agree to keep funding themselves. Yeah, exactly. Okay. So those are some of the things I'm pretty interested in, and I'm interested to learn what the state of AI in the military is today. I'm not so sure if people here are super aware of that, because I'm not. But I would love to hear that if anyone is. We've tried our best to get people who have as much expertise there. So hopefully there's some in the room. But yeah, I still think that potentially after this conversation that will still be the kind of open question which is like, where is the military right now? And which military? Hi all, I'm John. I have a background, computational biology, but I've been building and investing around AI for the last decade or so. I'm a partner in GI Central Partners, but I started my venture career, Lux Capital. And obviously we have done a lot of investing around AI defense. And I think that inherently the way that the defense industry works is pretty mislined before innovation and responsible innovation. And I'm actually most interested in how to structure that in a very different way and how to take a much different approach in terms of financial engineering and building those types of companies. And so, yeah, I'm just here to learn more about what folks are thinking in defense AI. I've taken a similar approach to other industries, but I think it's pretty valuable for the defense industry. I'm David Cantor. I'm one of the founders and executive director of ML Commons. We're a industry consortium, industry and academic consortium focused on making AI better for everyone. We're very unique in that we built. There are a lot of other organization partnership on AI has been mentioned. Future Society. There's the Frontier Model Forum. Those are AI. Well, two of the three are AI centric, but they don't build. And then they're also building organizations, but they don't generally do AI. Like IeEE, for instance. Previously I am an expert in semiconductors, so it was actually very interesting to read Chip War and talk to the author and the things that are both correct and not correct in that book. And then before that, actually I worked on for about a year camera design for aerial imaging. So I have kind of a varied background. I actually have started to do a bit more in the policy space and so I'm kind of here just because I'm curious and want to learn. And as I was mentioning, the language and the sociology of coming from a deeply technical part of the AI world is so different than in the policy space. There's just a lot that I think I can learn and understand more. Thanks for bringing up chip wars. I think industrial policy is probably one of the things that is highly related to both AI and this kind of national power did. Actually, my undergraduate thesis was at U. Chicago in the economics of semiconductors. Hey everyone. Thanks guys for having me. So I just left the Navy. So in August, spent eight years, five in nuclear program management and three on submarines. Last surfaced on Christmas Eve. And that was awesome because it was the last time. I'm happy to talk about all of that. I want to thank you all. It sounds like you've been building the tools that I've used the past few years, so that's awesome. Really appreciate it. Keep up the good work. And I'm hoping to kind of transition to become part of that workforce that builds just the best gadgets and stuff for the people collecting intelligence and people. I am interested right now in kind of two things primarily. The first is AI proliferation. I think it's interesting to compare AI proliferation to nuclear proliferation in terms of not. I'm not talking about pistol material itself, but more the science and technology behind it in the nuclear. Talking about the history side, the nuclear, it was kind of like we had a policy and governance before technological proliferation, whereas with AI it's kind of the opposite. And there's some dangers to that. But we've done a pretty good job embracing ethics as society, AI ethics. And now it's kind of the first thing on people's minds. And I think it's kind of an interesting comparison, because when people look at nuclear ethics, they're more regretful than they are hopeful when people talk about AI ethics. Like I said, I spent majority of my career navy in nuclear and energy, specifically nuclear energy. So the second thing that I'm interested in is what makes wars possible. Resources, the energy, the food, everything, all that stuff that makes wars possible. And I think that AI is going to have a big impact on how we do logistics, how we design machines. If you look at the reports that have come out, AI or jobs that AI augments and replace are all the white collar jobs, the engineers, the scientists, that kind of thing. And so I think that we're going to see a huge change not just in warfare, but probably more in the supporting organizations that make warfare possible. What was your name? Sean. Sean, I'm Josh. I work for Virgins Nonprofit Foundation Virgin Unite. I lead our U. S. Criminal justice reform work. I'm also interested in what makes wars possible, but kind of came at that from the other side. For maybe the last like 20 or so years I've worked in advocacy and studying anthropology, and my first big forays into advocacy were protesting the Iraq and Afghanistan wars and then watching use of tech basically make it harder to get good outcomes. Right. So broad screens for who was a bad guy, being applied to the wrong people and then creating more enemies. Right now, I guess I see a lot of these tools that were used in war scenarios now increasingly being brought home to police to empower police in various ways, from the guns and the armor all the way to the surveillance infrastructure and those tools. And it's, you know, some of the same people obviously with that expertise trying to do security work at home. So I guess, yeah, I'm definitely not an expert on the tech side. I'm really curious about the sort of social implications, how we determine who's a bad guy, what might lead us unintentionally to use these tools, or how these tools could be used in ways that are actually making things worse. I also think that defense tech is really cool, and so I am just generally interested in what's being built in spite of my opinions about its individual uses. I'm pretty impressed and curious. Nice. Okay, cool. This is a lot of good themes here. I should have put this at the front, but I'll just say it now. We made this event originally before any sort of recent current events have unfolded different parts of the world, as might relate to things about national defense and war. So I'm hoping we can keep our discussion somewhat agnostic of specifics on that and more at the abstract level of thinking about the future. Yeah. Okay, so lots of interesting themes. I think this is one thread definitely emerging here. Is this idea around an emerging or changing multipolar or unipolar world, right? And the extent to which that may or may have, and the recent Unipolar era may or may not have been leading to this period of peace, this Pax Romana. And is this under threat? Is this changing as the world changes? Also this concept of asymmetric threats, both in the sense of near term realistic technological developments, but also the more traditional discussions around this future AGI extract scenarios and production of bioweapons and so forth. Yeah. And then lots about this kind of morality of entrusting these systems to make decisions around things like the kill chain and so forth. And then this broader context of what are the geopolitics of conflict and why do those arise or not arise? And certainly this concept of things we build for the defense industry end up percolating back into society, right? And some of those are really bad. Like, I don't think we want lots of machine guns on the streets or whatever. But then some things like nuclear power, right, were developed for weapons at first, and then they become supplies of energy. So there's a lot of interesting questions there. I thought maybe I'll open up with just like a thought experiment or like a question that can start us grounded in the maybe more nearest term stuff around morality. But it'd be cool if we can build our way to the sort of bigger picture geopolitics multipolar world. See if the nuclear analogy holds water. Okay, so here's the thought experiment, right? Okay, so suppose that we don't have maybe fully Agi, okay, it's not full whatever, but reasonable, some kind of intelligence that can be embodied and make some amount of decision making with some consistency. The potential is this. You can put this into robots that will then fight in a conflict. The upside is that no people on your side are going to risk their lives. And the downside is that you're trusting robots to decide who to shoot and who not to shoot. I'm curious if people here have like a really strong gut reaction in, oh, that's morally important, we should never do that. Or actually, I could see that making sense. Within five or ten years, I'm just going to open the floor and I'm just here. In moderate morality, it does not stop at the weapon you choose to execute your directive, right? So if I kill you with my bare hands or with a nuke, you're still dead. And I've still decided to kill you. So I don't think it matters. I might be more effective at killing you with more substantial tools, but dead is dead. And deciding to kill you is deciding to kill you. So here's maybe a good illustration. The crux of this thought experiment is the decision to kill is no longer be made by a person, whether that person is using their hands or a big Red button who decided to deploy the system that could kill autonomous. Sure. I have a different take, and it's a bit of the societal implications of not having humans go to war. Right. One of the things that ends wars is like societal dissonance or war weariness from society, from people's brothers, cousins, and Walter Cronkite with the Vietnam. All kinds of people that they know being sent to war. Once that's robot or a drone or something trapped from a human, we lose all of that context. Do we have CNN going around with robots? Probably not. Right. So there's another part of it that we lose. So we get so far away from the conflict that we, society, probably don't care anymore. There's an old Star Trek episode about exactly that. People still suffer in one capacity or another, right? Even if, let's say, there's no military personnel and there's a war where no one gets killed, but they're still sort of suffering on the side of, let's say, people who are oppressed or whatever. And if no one suffers as an outcome, then it just doesn't matter, I guess. Aren't we kind of just talking about the Cold War? I mean, it's like proxy wars through other countries. I'm not trying to dehumanize them, but in a sense, if we're fighting through other countries by feeding them economic resources, then it's just a question of who's spending the war and who can outspend the other person. So talking about consequences, look at the Soviet Union, serious consequences in the Soviet Union because of the spending onto the Cold War. So there are still significant consequences for warfare. It seems like there has to be some. Why aren't we currently fighting our wars with video games or, like simulations? Because there's gladiators. Because there's no actually. Yeah, exactly. Clearly it's not just a function of, would it be nice if we could fight this war without sacrificing humans? There still needs to be some resource that is diminished over time that ends war. Those resources are human resources. Right. They're factories that we get our clothes from. They're food farms that we feed society with. They just move to the battlefield to civilian targets. Right. Well, not only source of power, like land, is it? Yeah, I think the Cold War is a good even if we had some mountaintop where we are, like now, we can actually fight some. If you were going to fight some war where you all agreed on some robot whatever on the mountaintop, the suffering and the resources put out would still be borne by the civilians who are redirecting their entire economy. And it seems unlikely that any country would end up being like, well, it seems the best thing for us to do is continue to fight on that mountaintop rather than attack the resources. I think game theory probably has something to say about this. It's always going to pay off for someone to defect. Yeah, sure, exactly. So there are other targets, but how are we right now able to maintain any kind of legal structure, any kind of cultural structure that says, more or less, you should attack military targets and not civilian targets, which is not a completely agreed upon, but somehow we did a heck of a job during WW two of not. Wait.
Speaker B: There is international law that says that has proportionality, and I think that might be what's getting to the moral question of who approves the kill. But essentially, it seems kind of inevitable that we'll be going towards swarm and drone warfare and different methods. I don't know about robot form microbots. It's clear that we've known for a while that, okay, when it's not human resources at play, then maybe the willingness to go to war is faster. Right? I guess I'm probably lacking a structure about where we should go with this.
Speaker A: Conversation, because anywhere you want, this is unstructured.
Speaker B: So there's international laws that govern warfare currently, and I think Caribbean somebody knows that that also governs kill switch decisions. Is that not the case? I mean, there's exceptions like North Korean border, where it's automatic, but otherwise.
Speaker A: So does that relate to the point that, again, it doesn't matter what the tool is, the thing that would stop if I was going to deploy something into battle and it was going to make decisions that were somewhat random, like, not my decisions faithfully executed. I have a disincentive to do that because in the end, I, the nation, am held liable by these international standards. And if my goal in pursuing some war is to improve my status in some way, if the international community harms me enough that it's not worth it for me to send out this poorly aligned AI system, then I will only want to send out a system that faithfully executes my orders. I actually think the military is quite. This is why they're quite focused on responsible AI and AI governance, because it's quite important that this chain is faithfully executed. I think they're interested just because of the cultural aspect side. I think they're interested because they don't want to be seen as unethically using warfare. So it's that halo kind of effect more than. I think that if society was like, we don't care about ethics, the military, I guess I'm not trying to point at ethics here. I'm trying to say alignment sometimes has a benefit for ethics, but also just for robustness, in a sense. Right now, the humans driving the machines that do warfare all around, many of them personal experience, just the submarines at least, aren't connected at all. And some of them are given more trigger pulling ability than others. What's trigger pulling? Well, there's like exactly what it's called, but there's like your state of readiness. It's kind of like if you've ever shot a gun before, it's like, where's the ammo? The ammo out of the gun. The ammo in the case and in the gun is it in the chamber is copped, and in the chamber there's different loaded states of a weapon. And they actually, in the military, they actually perform that same analogy for a ship or a submarine or an aircraft or something like that. And they say, how close are you to pulling the trigger? Or equivalently, to what extent are you able to operate autonomously? Right. So submarines, for example, probably are fully unconscious. There you go. Right. Whereas if you're some dude in an army base, you are not very autonomous and you are much more subject to a tighter command chain, is my guess, yes. And the expectation is that at any time, if we were to go into a global conflict, we'd lose all communications, and everyone is expected to act autonomously. And so there's already rules and policies in place for all that.
Speaker B: Is it because of the assumption that you'll lose connectivity? And when you say it's autonomous, what does that mean?
Speaker A: You already know the captain of the sub is in charge. Yeah. He opens the book about the US war plan, and he's already studied it. He knows how to respond. If this person attacks, this person attacks the scale of the warfare. Can I roll with that for a second? So let's dig into this question of some all out final doomsday battle and what we want people to do in that situation. I don't know if anyone's familiar with this psychology experiment. I guess it was really a war game drill done by United States military during Cold War with Minuteman missile operators where they basically feigned a nuclear exchange and saw how many missileman operators would actually launch the ICBM in their silo. And it was a shockingly low proportion. And so it turns out, when people are faced with the actual decision, should I launch this bomb that will kill 10 million people? A lot of people say no. And actually, if you look at, there's a lot of other research done this on people that have fought in battles and so forth. And essentially the gist was in, I think, World War I, that people firing their rifles were not very effective in combat because they don't want to shoot people. But as soon as you have someone operating a machine gun, there's a guy next to him loading the ammo. And so that person that's watching what you're aiming at suddenly increases the effectiveness. So there's this really interesting thing where it's like, would people left to their own devices in this fully autonomous state, given the decision to make this doomsday move, would they actually carry out those orders? And then in that case, is it better to have an automated system, like that movie with Matthew Broderick, war games, or like Skynet or Dr. Strangelove, all these scenarios in which you have an automated defense system that's willing to destroy the world. This slightly changes my view on Petrov, because there's a great Petrov Day, September 26. But he's the person on the Russian side that was like, yeah, I could see the nuke coming this way, but I'm not going to fire back. And it definitely false positive, et cetera. But now it's like, that's baseline, apparently. So it's like less, I guess, impressive in a way. There was more to that scenario in which the radar signatures didn't make sense because it wouldn't make sense to have a first strike with just a couple more heads. So there was reasonable to believe there was a faulty radar signature, whereas the minute man drill was a bit more of a concerted effort to get them to. But they still didn't. Yeah, my advisor was actually drafted, and he managed to get into doing psychological studies for the military. And one of the things they actually found is that the ammo expended in larger units, this was during the Korean War, was very low. And when you broke down to smaller squads, and I'm not sure the exact terminology for this, but basically they ended up restructuring the military, at least the army, on the basis of some of this work, which is like, if you have smaller five to ten person squads, people actually shoot their guns, right? And certainly in the Korean War, probably the prevailing view was the army is a machine to expend ammo, right? And so these large 60 person units not doing the job, like, let's get the ten or five person, whatever it is that actually works? To go a bit further on this point, I think from what I understand and read about World War II is that there's kind of like an 80 20 rule where the majority of soldiers, even that do expenditure shooting, are kind of like shooting aimlessly, not really trying to kill someone. So they get, like, the majority of people get, like, a very small amount of the kills. Then there's, like, a small percentage of really motivated people who are really shooting to kill, who get, like, the majority of the kills, which is like going off a strange psychological kind of flip side, actually. Right. I think the thing that I'm hearing here then, is what would it mean to have a well aligned unit? You know, like, is it one that faithfully executes orders that have been, like, thought out through some war games beforehand, and, like, there's a set like a flowchart almost with logic, or do you want something? And you probably, generally, probably want something that is somehow aligned with the objectives of the organization, but is able to do on the ground reasoning in such a way that it can make contextualized decisions that would be a much more powerful system and would be. I think it's the big transition between the kind of systems we have today which are able to kind of recognize things, make kind of decisions in a kind of logic system, expert system kind of way, and the kind of reasoning systems we can expect in the future. I don't know. What I heard you say is submarines already have. Well, that's what I was going to. Right. Yeah, they already have. It. Is a human intelligence driving the decision making policy already embodied? So you said there's a book or whatever. To the extent that, you know, what is the manner through which you align that autonomous submarine with the overall goals of the military? It's called the Composite Warfare concept, and it's where you have each unit is responsible and autonomous. I think the answer to your question of, like, A or B, I think almost anyone you ask would say, I want the reasoning warrior more than I want the rule following warrior. Maybe it's just a cultural thing, but I can think of my friends and my peers, almost everyone would be like, we want the reasoning warrior. There's a lot of evidence from history to support that, actually, if you look at infantry tactics in both World War I, World War II. So the German officer corps had a tradition of this independent, autonomous reasoning kind of thing, where they're given general objectives, then left to figure out the tactics on the ground. So there's also historical precedent to suggest that's more effective. It's the same with corporations.
Speaker B: Did anybody see the future of Life Institute's video simulation of. Okay, there is a short video to show how this could happen, where US government and Chinese or Taiwan governments bought AI. That predicts, gives a probability. So it's kind of a mix between the reasoning and hard coded that this is an incoming missile. So we recommend with 90% probability that you should press, that you should hit back, and presumably you'd be intelligent. So it would know some things like, oh, if it's only three, first missiles coming, that wouldn't be some things that, like, human would know, but they're trying to make it simple. So it's like taking all this information, all these potential variables, and then you see one big picture, 90%. And then the person has to make. The captain has to make the decision. So it might be how much we trust or how transparent to know what's going into that. 90%, right?
Speaker A: Yeah. For example, I think this video, it puts it on you for a moment, right? Like they're asking you, you only have 5 seconds.
Speaker B: Oh, no, I don't think so.
Speaker A: I think there's another side of this. Right. Specifically in AI. We live in a society where we really care about interpretability for the most part, but on the other side of that balance is effectiveness, or whatever the metric you're using is for success. Right. While we really care about interpretability and making the right decision, there's also someone who just really cares about getting the job done. Right. So while we may sit on this side, our adversaries may sit on the opposite side, which makes their machines more effective. How are you sure about interpretability? It's analogous to explaining to your board of your public company why you made business choices versus your startup. I think in the. Specifically interpretability, the DOD space or Department of choice, specifically, interpretability is very high. And do you think it's because right now it's upstream of effectiveness, or do you think it's you want to be able to defend your reason? Just from my personal experience working in a defense industry, wherever you're at on the kill chain, humans are in the loop currently in some aspect. So that interpretability is necessary in explaining why we came to this decision. One of the things that I'm actually genuinely curious about is the extent to which interpretability is necessary or even desirable in the sense that if you look at driving, right? And you look at a situation. So I have a friend who was hit by a truck driver, and he's alive, fortunately. You know, if you. You ask the truck driver, like, okay, why did you hit this guy on the bike? Right. It wasn't on purpose. And he'll come up with some explanation that maybe makes sense or maybe doesn't. I'm not sure how distinct that would be to a certain extent from if a Tesla is on autopilot or another vehicle is on autopilot and makes a mistake and hits someone. And there's sort of like, if you take, this is not in the military space, but you can imagine a scenario in which AI making decisions, driving is actually safer than humans, right? Especially if that human has had any alcohol, right? Are you willing to forego interpretability? And so one of the things I hear you saying, which is fascinating, is I think in the military, which actually doesn't surprise me, given the people I know who have been there, actually, interpretability probably matters a lot. And is it sort of like the reproducibility? Because from a technologist standpoint, it's what determinism kind of. Well, yeah, related to that. But debuggability and how do you get the errors out, right. AnD I think it's interesting to perceive when that is attractive because I think, like, in cars, probably at some point, we'll say, okay, we don't understand how a self driving thing really works, but it's safer than humans. So we're okay with that. But I think I hear you both say that kind of thinking is very different in the defense space. I'd add to that. I think this issue of interpretability as it applies to defense and combat scenarios is often the difference between something that's a war crime and something that's called collateral damage. Right. Where it's when they grill the person that was operating the gunship or whatever. Whatever this situation is. And it turns out, oh, yeah, they followed the rules of engagement. They followed the procedure. They had a credible reason to believe these people were threats versus something else that it didn't give a persistent state of all sensor inputs that can be stored and then shipped back. Right. So you can replay that same situation over and over again. A person can give an account of the decisions they make. And their ability to give an account of that is why we like to have humans as liable. Right? Is because if they can demonstrate following some rational, reasonable thing that we could empathize with as what I might do in that situation versus if they show some truly psychopathic tendencies of whatever, paranoid illusions and so forth, then you start to treat them a lot differently, at least in the core of law. I think, though, in terms of this interpretability versus trade off effectiveness, I think another lesson from history is that the more effective the credible threat of your weapons, the less you have to use them. And so Pax Americana and the post World War II unipolar World, most of these wars were prevented probably just by the threat of nuclear weapons as being demonstrated to be very effective and very real in one threat. Right. That's controversial. Yeah. Well, then let's debate Twitter. Let's debate it. Let's debate it. But here's where it's going, right? Is like, if you have, say, here's my robot army, they are 60 times more effective in combat than humans because they don't hesitate, they shoot to kill, all this kind of stuff, and then I never have to use them. Or is it more just this tail end, estimated follow or estimated damage, which is like, you're going to end up using them at some point and it's going to be like a massacre. So I don't know. You can debate any of those points you want. Yeah, I think with nuclear, I don't know. I'm not a huge believer. I think we kind of see that it kind of forbids the use on both sides, and then what you end up with is that you just use lower systems, less apocalyptic weapons to get whatever objectives you want. And we see that in Ukraine, we see that in Israel. Or you use it as an intermediary tool for economic warfare. Like, you've got to invest so much in this thing that does nothing for your economy that it depletes you and we still win floating for you. Yeah, it's best for everybody if nuclear weapons are never used, which is why they don't. But I do think there's some. It's not just like nations make decisions really optimally all the time. It's like individual people and nations make specific decisions for their reasons. I do think there's like a non zero chance that if one of these nuclear nations were brought to the brink of survival or the people within it were brought to the brink of, like, I'm about to be executed because of the things I've done. For example, that they might actually use nuclear weapons, which is bad. I'm really interested in this topic of robot soldiers because I know the Star Trek episode, I always think about it and I always wonder, why can't our wars be like that. Like a simulated war, like computers fighting each other, like you said, a video game, even though we're kind of getting to that point. If you think the way people control drones, for example, I mean, in a video game almost like setting. But the reason I think that doesn't work almost fundamentally is because war is Sort of about like, it is physical. It's literally about taking things in the real world from people by force. I think that's like, in a foundational sense, what war actually is. So you can never have something that's totally immaterial or totally nonphysical be our war. Because in the end, whatever happens with that computer, you can walk up to it with a baseball bat and hit it and then take what you want. Robots, on the other hand, are physical, so they actually just replace a human. And in the end, we could have a bunch of robots fighting each other, like massive wars with zero human casualties almost. And that could work as opposed to a simulation, even though it's like unoptimal in every way, because the winner robot army can still come in and take what they want.
Speaker B: There's always one party in the war that has more power, and that's the party that's going to have the robots. The other party is probably not. And so there's always going to be imbalance. It's going to be a show of who has.
Speaker A: The China Russia alliance is going to have robots, and so are we. A recent phenomenon. Yeah. Also, power doesn't really guarantee that you win a war. Right. I mean, if you look at Israel's War of Independence, Israel should not have won that war against its. Like, Saudi Arabia spends so much money on their military. And mostly it's just kind of like toys for rich people. And that doesn't mean that they can't project force into Yemen. But, yeah, I mean, that's like saying I can take candy from my two year old nephew. Of course I can take Candy from my two year old nephew. That's how two year olds and 40 year olds work, right? Yeah. I don't know. I think actually one of the things that you talked about, which I want to pick at a little bit, is you said, okay, we're going to have robots that replace humans. And I actually think that in a lot of ways, the most interesting applications are not how do you have AI replace humans, but how do you have them do things that a human can't do? Did everyone see Chernobyl here? Yeah. Right. And so there's like this scene where they're like, okay, we want to get this robot on the roof to clear this crap off. And I think it was a German made robot, just didn't work. And they're like, okay, we're going to make a robot that it'll be an organic robot out of. And so I think the point is, why didn't they want to originally use a robot there? And it's because it was a situation in which it was perceived to be disadvantageous for humans to practically do things. But you sit down and do the math and you're like, okay, everyone can be up there for 50 seconds, so that's like two shovels apiece, right? But where do you want to send a robot? You want to send a robot to like 30,000ft under the sea where organic things don't do well, or like, not human organic things, right? And so I sort of look at AI in a lot of ways as like, where can we use it in places where humans are just utterly unsuited? Because just AI works so different, right? You can understand I killed a bicyclist because I was swerving my car to avoid oncoming traffic, right? But whatever some matrix multiplication and nonlinear functions does, it's kind of inscrutable to basically everyone. And worse yet, AI behaves differently than what we're used to with computers, right? Everyone can kind of understand and think through how a computer is misbehaving in a conventional sense. We're like, oh, I was off by one. It's a very common class of bugs. But then AI is sort of this third thing where it's like no one has any intuition about neural networks. I definitely don't think that there's a really bold alignment stand of using AI or not using AI in defense or robots. I think by the time that we have robots physically in the field that are wielding firearms, we'll have already had such significant deployment in automation, AI, so many aspects of defense that it won't even be a question of whether there's more question there or not. I think it will just have shown much higher efficacy than anything else. I mean, I think the question. I think the reality is that we're already using AI systems in object human detection on drones to give information. I think that just gets better and better where the human decision making eventually gets just taken over by the accuracy of the data extracted from what react that you're using as that. And then you conventionally just make a cake that just go fully autonomous with that and that stuff. As you mentioned, it does allow capability. There's drones. You don't have a human, so they're much smaller. It's much more substantial. I don't know. I think actually, the question of whether there's actually a real moral question to date here, I think it's more moot because I just think it's going to happen. So do you think then that, because I generally agree that there's this general idea that AI is going to be brought into organizations of any kind, military or otherwise, and for the areas where they prove more effective than humans, there will be a period of intensive human oversight, and that will be brought back over time as they prove themselves more and more. And then that component of that system will become autonomous because it's been trusted to work more effectively, and then you'll devote your resources to other things within the organization. And so within the kill chain or whatever kind of decision we want, where are the places that we actually want kind of human value injection or some kind of trade off that, at least for now, is going to feel more difficult. Like, how do I trade off the benefit of potentially killing my adversaries, like leader in this area while having civilian casualties, which will affect my perspective on the international stage. Right. That's like a difficult calculus. And that kind of perspective seems like maybe where interpretability, these kind of things are relevant. Because if I was interacting with just another person and I said, do you think we should kill that? Or whatever? And they just said yes to me, I'd be like, I don't really know how to move from there in a collaborative way. And there seems to be, at least between us and full autonomous. We're just sitting back. There's clearly going to be some collaboration between some human and some AI where the AI is doing the things that the AI is much better at. And I need a way to understand what's going on there so that we can best collaborate honestly, collaborative partners. And so I like the way you framed it, because, again, it's not a moral decision there. It's like this is the evolution of systems taking on.
Speaker B: Weren't we saying for a long time that cyber warfare would be the evolution? And the way things like when Russia was attacking Estonia or destabilizing attacks are increasing in scale and number, that's undeniable. But that seems to me the next level before we get to robot form, because robotics has been so lagging. But I don't know how the military sees it. It's still like people fighting, or people with robots, like Ukraine is people fighting, right. Which I was surprised by in terms of trends, I'm surprised that we haven't just been at just mainly cyber descabulization by now.
Speaker A: I mean, part of what you're saying is what I take you to say. Maybe this is not quite direct, is right. Cyber is just sort of another dimension of warfare. Right. And you have economic dimensions of warfare. Like today, I just learned that from my friend at Reuters, that the Department of Commerce is changing the regulations on chips for AI and making them. And, you know, that's, know, another dimension. We've got cybersecurity and other things. Right. And so maybe the point is that AI and robots just stacks yet another potentially orthogonal dimension.
Speaker B: Yeah, and that is the other point I was thinking of, which is that when we think about AI policy, the US government doesn't want to regulate because they want us to be competitive in AI technologically and innovative, and continue to beat China. And that's a national security. The economic argument is a national security argument, right. Because the US has under their national security imperative to be technological and economic leaders.
Speaker A: I think you guys brought up some great points. I agree. This is kind of the latest or the newest branch, almost, which is like the cryptographic war, I would say. I mean, not hunting the Space Force, but you also bring up that they're orthogonal and they're like, kind of orthogonal, but they're not full, which is the thing. So when I think about what you're saying, I think it's totally true. If I had to imagine. So during the Bolshevik Revolution, one of the famous things is like, they took over the train stations, right? And then by taking over the train stations, they crippled the ability of the Cyrus government to actually be able to get soldiers moving and logistics and actually get information across to people. And that's kind of like how I imagine would be the easiest way to, in 2023, if somebody were to take over a state, I imagine the first thing they would try to do is control the internet and block information from traveling. And I'm sure the US government has thought about that. So they have contingencies in place of the Internet going down. That's why it's kind of worthwhile, but not really. But I really want to hear about what you guys have to say too, since I think you have maybe the most experience in defense. It seems to me at least about interpretability is like, when we speak with chat BT, I feel like he's always reasoning and it's always reasoning. So we talk with chat CBT, and there's a thing called Chain of thought, which is you get to say something and then it responds to itself. So it basically keeps looking at its previous messages and creates a chain of thinking. That is interpretability, if you think about it. And the reason we do it is because Chachikati is allowed, on a macro level, to reason out a better answer by actually being interpretable. I'll just say that it's currently an active area of research whether the explanations that GPT creates for itself are actually related to the kind of decisions it might make. To say the same thing about the sailors that work for me. Yeah. And I think this is a general thing about when people make explanations, and when chat GPT makes explanations, it may or may not be a true explanation. And I think I'm a little blurry on the details, but people have tried to kind of corrupt the explanations in different ways. Researchers, and see whether this has more or less effect on the final psychology thing as well, is you postdate your experiences and assign rational meanings to what you did. Yeah, that's a form of rationalization. With the chat GPT, with the chain of thought, the explanation almost comes prior. Right? It's like, let me think through this. I think a leads to B. And so because of B, I should do C. And then people have done some experiments where they're like, well, what if we change that explanation? Actually a leads to D, it still does C at the end. And so it's like that explanation, that chain of thought seems to be improving. Seems to be improving its performance, oh, my God. Its performance, without actually being based on the explanation. So I'm just saying there's a future that we're going to move into. It's very difficult to keep talking during a future where we're going to move into where we have these kind of placebo explanations. I think that's what we're currently at, right? Yeah, I think language models are nondeterministic, and what we're doing is like, next token prediction, right? This token is most likely compared to the last context, so it looks really great. But what are you actually. Maybe these chain of thought prompts push you in closer to the latent space that gives you the answer that you want. But the study that recently investigated this one that I think you're alluding to is like they had multiple choice questions, and originally it was able to reason correctly when the answers were random, like they are a normal test. But when it was like C, every single time, it still incorrectly guessed C. But then it came up with an explanation that sounded kind of plausible, but was also just like kind of misleading in a way. So it was like trying to justify after the fact and a completely separate dimension here in the collaboration place. This is further, I think this is work from 2020 or something. So before the current range of LLMs, but you gave people explanations, and they did actually call these placebo explanations, where you gave people explanations of what the AI system was doing that sounded plausible, but were completely independent of how this algorithm worked. And people rated these systems to be more trustworthy. They liked them more because they felt good. That's the feeling of explanation. And those kinds of dimensions are the traditional ways that people measure explainability. A better way would be like, can you make better decisions in collaboration with this AI system? Because you know how it works. I want to let you speak as you. I mean, one of the things I'm in general sort of curious about is, like, I think something Sig said earlier, which I was all for, is I'm not a very big believer in AgI, in part because I think a lot of the definitions are pretty fuzzy. But one of the things that was brought up was like, okay, does an LLM actually think. Right? And you pointed out that, very correctly, factually, all it's doing is just predicting the next token. So I'm kind of curious to what extent people actually. What are people's baseline views, where they're coming from on? Is this thought, is this reasoning, is this just like a collection of possible 17 tokens? I'm kind of curious to hear everyone's view on that. My gut reaction is, that's what I'm doing right now, is I'm listening to this conversation. It's like pushing me around in some kind of pattern matching space, and then like, blat, which may be bad for me intellectually, but I feel like that's actually what's going on a lot of the time, particularly when I'm like, oh, yeah, I should say this, that even my own wetware is just predicting next tokens. There's like a really interesting history of surgeries where, to prevent seizures, People have had the hemispheres of their brains attached, right? And you may have one side of your body operated by one half and move your hand, and then I think the way it works, maybe somebody remembers this better, but is like writing and speech are in different hemispheres, so you can kind of talk to one side of the brain or the other without the other knowing what was said. And so you can force behavior on one side, and then you can have the other side explain why that happened. And without knowing why it happened, the other half of the brain will come up with a reason. It'll just imagine something to explain it. This has been done, like, a number of times to see kind of a corpus quasum. Yeah. Obviously, our brains do work that way. We do come up with explanations. I'm kind of curious to draw that out. My biggest concern with handing over sort of the keys to decision making and having even more effective killing machines, I think machines already are, or at least soon will be, like, really effective warfighters when it comes to killing, is do we also have people in the defense space thinking about grievance and how to respond to it? Right. How do we address. And maybe our technologies can help us somehow. How do we address grievance? It seems like my read on the war in Afghanistan, for instance, is we were militarily superior throughout, and we had a lot of people on the ground working really hard to try and determine how to execute that war ethically, or at least navigate all the complexities of different tribes and cultures. And ultimately, our presence there just slowly created more opportunity for other actors to leverage legitimate grievance. And there was no out. Right. Except for us to essentially leave. We couldn't win militarily. We needed some other way to come to a peace. And I think a lot of wars that the US has fought has not had a military solution or ending in the last sort of 50 years. So I'm curious if there's defense technology that's going to be more on the military strategist side or the political strategy side, or help us foster multilateral solutions that can get us past the really exceptional killing machines that we're building. We can do that all day and maybe not actually achieve our goals if tools for winning the peace. Yeah. How do we get there? Right. We have in Oakland an example of with gun violence, these violence interruption programs, where guys that were caught up in that life go in, and after somebody's been killed, they go in and they address the grievance, and they do it in heavy, interesting ways, but they basically de escalate and they've collapsed. Violent crime. Before the pandemic, this was working really well. I think they got violent crime down, like, somewhere between 40 and 50% really quickly, and they did, like, block by block studies. So they knew where they weren't deploying this, and they knew where they were. And then the pandemic kind of blew it all up again. And now we're kind of trying again to re implement. But in war, I think it's similar right. If you can generate grievance and like, are there tools and technologies that maybe you guys have seen that will help us go in the other direction? I think it's like a really interesting point. I just want to circle back on Deji and then come back to that in some ways. I find also the conversation on whether it is AGI or can get to AGI somewhat moved as well, because I think we tend to anthropomorphize these things when in reality these things are very different than how humans will ever think. And so there's no point. And even if we get to a point where they're theoretically in some ways more intelligent than us, it doesn't really matter. What matters are the outputs and effects of what they have. And so conflict, I think it's somewhat more of a distraction whether these things will be AGI or not. Coming back to your point on grievances, I think the thing that's interesting is I think nations and nation states have trauma about certain events. The US had substantial amounts of trauma around 911. It made us do pretty irrational things like going to Iraq, Afghanistan. I don't know if an AI system would make those decisions, but I don't know if there's a system that measures how much of an effigy we need as a society to get over that trauma or not. It's a pretty complicated question and pretty complicated problem to solve for, particularly with an autonomous system. I don't know if you want an autonomous system making that kind of judgment. I really like that you. So when we titled this National Power and War, I was thinking about war a lot in the title, but by extending it to national power, you brought up, can we expand our circle tools? And this brings me. Maybe this is where my head's at, is like, there are many ideas around new tools for democracy. I brought this up at a S lawns in the past, and some aspects of democratic decision making at scale are like, how do you find imaginative points of commonality or cooperation amongst people that at first blush, or maybe on many blushes down, you would think would have irreconcilable differences? But there actually is some area of overlap, if only you could get to it. And so maybe one aspect of these AI systems is putting more because they're more disastrous. Maybe rather than a lower barrier to war, there's a higher barrier to military intervention, and there's a comparative ease of us having more neutral third parties in the function of AI tools that can help kind of collaboration. Maybe that's overly idealistic. For a future of more democratic or diplomatic solutions being unveiled through AI systems. Maybe that's not in the benefit of the United States, but I don't know. I think that's actually more realistic than robots fighting robots. I mean, when you get in a bar fight, you don't punch the fists of the dude you're fighting. Punch their face so they can see or breathe or think straight when they're fighting. I think that there's this idea of AI and warfare where it's machines fighting machines. I think it's completely unrealistic, kind of rudicrous. And then there's this idea of AI and warfare where it's providing solutions like you're talking about, I think intro. But people were surprised that these AI systems were replacing white collar jobs instead of blue collar jobs. And I think we're going to see the same thing in warfare, where AI is replacing the person that writes the treaty more than it's actually replacing the person that shoots the gun. You actually see it in AI tools for relationships. There's like, Andrew Ong is like investing in one with the ex Tinder CEO to create a relationship support AI language model. Wobot's been around since far before generating models. What is Wobot? Wobot. It started as a Facebook messenger app, and it was just like, I need counseling. I've used it. Friends who know there are these cognitive behavioral therapy. Do you know if, in the military, are, like, officers allowed to use strategy, or is there a security concern with privacy? I don't know what you said. Well, there's lots of security concerns. We get a lot of training on it. I'd say depends on what you're talking to it about. Yeah, I mean, if you want to get into it, I'll say that there's different levels of security. There's KUI, classified, controlled on classified information, and then there's confidential, secret, top secret, and then there's different categories of each. And if you accumulate lots of one category, you could escalate to the next category. That's the primary concern with having, like, a KUI level or a secret level chat bot, that there is the accumulation of data. And so what people are looking at is saying, okay, well, the data repository will be escalated to some higher level, but the actual chat interface will be at this level. The only chat bot that I've heard about that's classified. Of course, there are others I'm sure, that I don't know about, but the only one I've heard about is Kirby. The House of Representatives has an official statement of what each individual congressperson can do. And they are allowed to only use chat GBT, and they cannot put certain information in. And there's like a kind of net, at least in that, but it's different from the most, I'm curious, on a different point of national power war. So I'm just seeing two people who work in satellites in front of me, which is like, fog of war is one aspect of the world. And satellites have, like, Arbitra's company has been around since 2014 and has a picture of every point on Earth every day. Now, that's not real time, but that's like a ridiculous timeline of Earth, and we're not yet at the point where that timeline has been converted into value for many industries. I'm sure the government and DoD is the most advanced in converting that into value. But I'm curious how much, just, like, the inability to hide from each other will change what diplomacy looks like, what battles look like, what national power looks like. You're just hiding different things. Right? You can still hide things from satellites. You just have to be more careful. Sure. Okay. It's hard. It just increases the. Right. Well, just think about. I read this, and I don't know if this was. See, I could have read this in a Tom Clancy novel, which means it's science fiction, but there's a sort of a statement made. Like, the US and the USSR put nuclear missile silos in different places, and the US put them in the middle of plain states under the theory that guidance control would stay bad, and the USSR put it somewhere else under a different set of assumptions that proved to be technologically better. And I guess my point is, like, if you really want to hide something from a satellite, you just have to try harder, right?
Speaker B: But that's not just the satellite, right. The data you get from the DAP, from the satellite is also being processed so well with. These days. This has nothing to do with defense, but, like, cloud detection, there are so many cool cloud detection algorithms available now that you're able to better do things. There's also tables that use just, like, thermo imaging, and it's not actual, like.
Speaker A: Visual imaging, and so there's gravitational lensing. I'm also not even thinking about imaging. I'm thinking back to this was, I think, a radio lab episode, but there was some law enforcement that was trying to break up some criminal sting, and they can't see what's actually going on anywhere, but because they have a timeline literally on. I think in this case, like, the minute time scale. You could backtrack until you get to a piece of information that is known. That's this person. You follow that car. Okay, that ended up here. Why did it end up here? Let's back out. Who went to that house? That's a different level of being able to extract information, which is not currently possible, not because of the imaging, but because of the reasoning that would be needed to run over these satellite imagery. And that's where I'm saying AI kind of has an impact here. Huge amounts of. Anyway, that's the topic that I'm curious if anyone has perspective on. I think it makes a lot of sense for a lot of reasons. Yeah. So in terms of where do these tools get deployed first, the whole military intelligence and security apparatus as it relates to information is very not in public awareness. You don't know what tools are using or how they operate, all this kind of stuff. The robot army, the battle droids. Right. It's very obvious, and it's very hard to do that, too. And it's not very cost effective, all these reasons. So I think we touched this briefly earlier. But the extent to which the ultimate arbiter of social reality is physical violence as enacted in meat space and not cyberspace is going to be the extent to which satellite imagery and so forth is still useful, because you have to move the tank there and you have to get the troops there, and there's no way of. I don't know. I think it's a pretty strong case to say that the ability to scale up information services by mass producing knowledge workers who can undertake sophisticated analysis from massive data sets that a human can't comprehend, and this enables you to make more timely, better informed milItary, tactical and strategic decisions is pretty compelling. And I would kind of push back on this thing that's like, satellites aren't going to be useful for monitoring movements of troops and wartime personnel. I don't know if that's the point you were making, but I was. No, I mean, I was saying. What I was saying is, like, in a lot of these things, it's easy to think about it in terms of absolutes, but usually it's going to be more of a probabilistic thing, right. It's clear that satellites give you much better visibility. Forget about the military side of things, right? Like if you want to track, like deforestation or shipping containers or all these things, even estimates of the economic status of another country and what are their resources. So many of these things sound impossible to estimate because of the amount of labor it would take to estimate across. If that's removed, then you're just in a different state of knowledge about your adversary. And so when you come to the table diplomatically, there's like a little bit less hopefully posturing and a little bit more just like we know what your silicon reserves are. We've been watching you for the last. I think that's a great point. In terms of national power, we think mostly in terms of war, right. But we don't think about industrial power or the ability to know exactly what you can produce, when and how much of it and when you can get there. Those are things that our society itself, in the west, we aren't big fans of tracking and monitoring and surveillance. Publicly, at least our government, but publicly those are things that are popular. Right now, I look at the opposite side of Pacific, and we have the exact mirror of society in which there is tracking. I'm specifically talking about China.
Speaker B: Right.
Speaker A: There is tracking and there is surveillance and there is the ability to know, to pinpoint amount, what can be produced, when, where and how much, who's working on X, Y. And I really wonder how that affects our ability to create a job. Yeah, I totally agree with this, but also an issue that's tough with anything military is kind of like we don't really know what the government does have access to. Actually, if I had to guess, I would think that they do have a system like this, very similar to what China has. I think they probably have it here, but it's just like much more tightly held. And does the system involve LLMs or something like that? No, I mean, in China, there's like one camera for every two people, and you can give them credit. The other thing is you can just read the law. The law will explicitly tell you what you can and can't track on US citizens and non US citizens and what the definition of a US person is. It's all in the law and the government falls the law. Maybe the law is not perfect, statistically speaking. This of course true. But on the other hand, there are a lot of ways to infer data without probably breaking laws in. Oh, yeah, the intelligence industrial complex as a whole follows the law. My question wouldn't be whether we are breaking the law or not. I would propose it as what are the benefits of the other system that we aren't thinking of? Benefits of our system are kind of clear. Right. Like open source technology is the reason why we have the advantage at the moment. Right. And that's people working in the valley in open producing, telling people about what they're working on, et cetera, sharing ideas. What are the benefits of the opposite side of that? I heard recently, and I don't know the exact number, but China spends more. People talk about their spending on defense relative to us, but they spend more on internal surveillance than they spend on, um. This is just backing up, I guess, your point that there's, like, a way of being able to monitor, which is just a huge cultural difference, if nothing else, and will lead to different kind of capabilities. Can we just frame it, though, with China? China is still recovering from the Mao era, where they had no surveillance and no idea what everyone was producing and overtaxed everyone to the point of mass know. I'm not saying that surveillance is a consequence of that, but it could be a follow on. Like, okay, we actually need to know what's going on here so that we don't starve our whole population. Again, I would suggest that China's surveillance and state police apparatus is more about maintaining political control and suppressing dissidents and rebellion than it is about, necessarily, economic management. I get that there'd be that advantage in the economic side, but, I mean, it's pretty clear that it's about suppressing political dissidents. Right. And then preventing the spread of information about that kind of stuff. It's hard to think about the upside to having a massive police state of surveillance because we are Western democratic idealists and stuff like that. I would echo the sentiment of Thomas Jefferson, which is that the biggest threat to civil liberties is a standing army. And I think often so much of your civil liberties are eroded under the pretext of protecting you from invisible threats that you're not aware of. And so I think, just like the downsides of that system of social governance are so clear and imminent and antithetical to all of the founding principles that we live by. So I had somewhat of a front row seat to a certain extent, to how China handled COVID and how the US handled COVID. Right. And in late 2019, so my organization has members that are in China. And so we were having meetings about some of our projects, and they're like, hey, we're all locked in our homes, right? It's kind of hard to work. Could we push back some deadlines? And of course, we were like, yeah, that seems very reasonable. We should maybe give you guys an extra couple of months. Right? But the ability of the Chinese government to have a lockdown like that, where they prevent the spread of COVID to a much greater extent by limiting movement, is something that is, to your point, right? Part of your question is like, okay, what do you get on the flip side of the coin where you do have societies where there are different levels of acceptable government interference? You had people involuntarily staying in their homes, and that allowed them to handle COVID very differently than we did. And here is kind of state to state. Like, when I was in California, people were very observant of not traveling. And I remember going to Wyoming for my brother's birthday. We rented a cabin on the Yellowstone. You go to Costco there, and nobody was masking. I felt very weird having a mask on. This was in 2020, before there were vaccines. And so I think the cultural norms do give you, for sure, different capabilities. The question I guess I have for the group and still haven't necessarily heard is, like, one, does it seem good for peace for more nations to be able to relatively easily know what the other nation is up to? Like, if there was more transparency, is that a good thing for world peace? That's one. And the second question is, do we actually think there will be more equality in information moving forward, or is it more likely that AI leaders like the United States are going to actually accelerate their informational asymmetry with the rest of the world and therefore just be in a more powerful negotiation position because of that? Those are the two questions that I have. I think it's pretty clear that the answer to the first question is probably yes. But of course, there's a question of, like, you really. So if you know everything, if you know whether the war is going to be lost before you start it, then you're not going to start it. But the problem is, what are the hidden factors? You assume that it's only the number of tanks, but I think when Russia attacks Ukraine, there were a lot of factors that they did take into account. Like, oh, shit, there was corruption, and a lot of the shit doesn't work. And oh, my God, Ukrainians were actually mobilizing the past eight years, and the motivation is high, and motivation of our groups is shit like, things like that. So in theory, complete transparency, I think, definitely leads to peace, because then why do you need to go to war? You're just like, hey, I'm ten times more powerful. You give me that. Otherwise, I kind of second that. With a historical example, which is the British Navy and their superiority in the North Atlantic and the North Sea, which was called this fleet, and being right, the fact that all the countries knew they had the best Navy and could just totally smoke them meant that there was no big giant blowout naval engagements, except for the ballad jetland, I guess. Except with what I think it's called Ballad Jetland. But this thing about if you both have really accurate war gaming abilities and perfect information, then yeah, you kind of know who's going to win ahead of time and then you would choose not to. Doesn't that mean though, that you're completely incented to keep all your shit secret if you're not one of these superpowers? Our superpower. Also, if you know you're going to win, wouldn't you like the other side? Because you said, if you know you're going to lose, you wouldn't do it for stuff and you just get it. Yeah, exactly. You don't have to do war. Got a nice country there. Unless they're like values and principles that again, the irrational factors. Palestine was offered peace in back in like, I don't remember. Forty s, fifty s. Yeah.
Speaker B: We'Re going.
Speaker A: To stay out of the Middle East. Yeah, no, it's all good. Okay, but can we go just a little bit further back and talk about ISIS, for example, and being motivated by this idea that what's good comes in the afterlife? A story that changes how you interpret information or how you measure the costs of different. Your values lead to different outcomes. That's an extreme example. And then, yeah, I think there's like a whole range of examples. You could tune societal values over some period of time by subtly introducing a lot of aigenerated content. Right. And that's happening, right? We're seeing the weaponization of social media and it's pretty effective. Or you could argue it's not just our one way us to theM, it's also them to us. Sure. Yeah. So maybe a future is both. Nothing is a silver bullet, right? No single, but greater transparency in the resources that we have and the capabilities of our adversary or whatever. Maybe an increased common culture through globalization and mutual propagandizing and whatever else. And then maybe AI systems that, like Sean was mentioning, take over the white collar jobs of diplomats and actually help you bridge the divide. Oh, those are your social, societal values. I see. I didn't understand that before. These are United States because it's just this neutral thing. Let's figure out some mutually beneficial thing, given our full knowledge of each other's resources related to that maybe. Very idealistic view, but I think like a unique perspective. You mentioned like Loyalbot and then you were talking about peace tools. One of my theories is that if you would have a higher level of psychological maturity, in the world. And by that I mean things like emotional intelligence. Like, if people would work through their trauma with therapist when they're in school, then that would also lead to higher cooperation, higher levels of peace, and that could also happen. My company is working on AI coaching. I think AI therapy could be even more powerful in that regard. So if there's, like, proliferation of those tools, maybe that even leads to peace. I hope they're going to a mental health salon in the future here. Yes, I think that there's real value in that, at least on a violence de escalation scale. Or I've gone into San Quentin and met with these guys working in groups. They'll literally get on ten year waiting list to be part of this deeper understanding of yourself program that helps them really, truly understand why they develop these sort of patterns of going towards violence, and maybe why they were programmed that way in the first place. Like, what happened to them. And then when they go before the parole board, they can make a really convincing case that they understand now where they were wrong and why they would be different. I think applying that at a bigger scale, at a societal or cultural level, is really interesting, but I haven't seen anything that does anything but make us more mad at each other. Yeah, and just like, a quick point on that, I feel like we don't really see it as a side. It's almost like an invisible issue you never think of. We always think, like, left or right, we don't think that you can grow as an individual. And through that, somehow we could resolve the issues. I feel like when we talk about education, we talk about skills, we never talk about things like self awareness or emotional intelligence. They're so intangible, hard to measure, which is like part of the problem, because we like to measure everything perfectly.
Speaker B: Generally, we need to move to a global level to solve a lot of global challenges. So the nation state is a model I'm hoping will get less and less of a thing. A, B. Have you guys.
Speaker A: What?
Speaker B: I was guessing you didn't talk about this. Is it like security of defense or AGI? Kind of the big question that nobody's really talking about. It's not like who gets the first AGI, like OpenAI or the first powerful AI. Let's say there's no AGI so much as who can secure and control it. And same with American military or any critical infrastructure or scaled systems. We know cybersecurity is basically like an unsolvable problem in terms of its offense dominant. And cybersecurity talent is really scarce relative to the problems. That's like, what I'm concerned.
Speaker A: Yeah. Is your point that transformative, powerful AI systems are actually fairly small pieces of code that can just be stolen?
Speaker B: Yeah. And basically something that could undermine all of plans of national security if we have poor cybersecurity.
Speaker A: I guess kind of like what the issue would be with cybersecurity, because the LLMs are. Everyone has one because it's pretty easy to get them and they're open source. So when you say cybersecurity, what I'm thinking about is how are people breaking into systems they're not supposed to be in? But I'm not sure exactly what you're referring to, because the LLMs, you don't really have to break into them. People already have them. Maybe you mean like hacking them to do bad things you're not supposed to?
Speaker B: I mean, people have open access to open source LLMs, right? Like llama Two by meta. But people don't necessarily have access to open AIs or anthropics unless they hack into them. And same would be the case for US military using hacking it, destabilizing it, exploiting it, and specifically, maybe not LLMs right now, but if we get even more powerful AI that's scaled private sector.
Speaker A: Public sector critical or something, that makes it an order of magnitude or two less expensive to train and operate, which is not beyond curious where you're going with that. Part of that where your national defense planning becomes moot because of some cybersecurity threat, is that like, in the guise of AGI, that is rogue?
Speaker B: Or is that whether we're talking about AI based systems or any digital systems, including military, digital systems can be really easily under easily but undermined, right? Like Dario Amode from Anthropic said on a podcast of Dora Kash Patel on August Eigth, that any motivated, not any unmotivated and well resourced enough actor, like, for example, China, could hack into any of the frontier AI labs right now. And so it's maybe less about who gets the powerful AI first so much as who can control or secure it. But again, military wise, I just think, is this security really important? It could just undermine all of these issues.
Speaker A: I think there's an underlying assumption you have that there's some magic secret sauce that can be stolen or pride loose from one of these ladders. And I don't think it's even Zodario from this podcast specifically talks about this aspect, that he does actually think that there is something that can be stolen. You might see this as PR guys or something. But these models are unartifact and general purpose enough. And so Dario's at least perspective. I think there are Industrial secrets. I don't think there are science secrets.
Speaker B: There's billions of dollars of investment.
Speaker A: But proprietary IP, these are not secrets in the form of either industrial. I mean, there are industrial secrets that anthropic has, for instance, that he calls compute multipliers. Right? Essentially, it makes it cheaper to train a bigger model because you have a bunch of these algorithmic tricks. But essentially what you're stealing is not a secret. It's like the expense of creating this thing of all the way. Right? Exactly. And the way he describes the goal of security and frothic, is to make it more expensive to steal the model than to train it yourself. So it's not that there was a secret sauce, it's just that it used to be not beneficial to you because you could just train it with enough compute and enough time. There's nothing that actually, I don't think that's true.
Speaker B: Well, with scaling, it's not going to be $100 million. It's going to be a billion dollars to get the next generation of if the scaling script.
Speaker A: Let's move with this and how the mechanics of if it can be. But I like this topic because I'm thinking about it like, would it be accurate to say the extent to which, say, your defense apparatus could be co opted in one fell swoop or subverted? Does that really just scale with how centralized those systems are? Is that why these nuke subs are so decentralized, is because you can't just hack in and control them remotely? Is that the kind of risk of we have a superintelligence that manages our defense system, and that's a single point of failure in some capacity because it's now hyper centralized?
Speaker B: Well, I think there's federated and they're centralized and there's different models. But ultimately, to me, what I'm scared about is if the systems are scaled and there's failure, and that will happen with AI because AI is not 100% relevant.
Speaker A: What do you mean by scale?
Speaker B: I don't want to go too out of the military, but assume this capital military. But let's say we start having, okay, electricity system or autonomous vehicle fleet of San Francisco right now. Let's say some economic supply chains, let's say public sector, private sector systems, banking systems, financial systems. We have digital and AI systems increasingly being adopted across society more and more ways. So the attack surface increases.
Speaker A: Okay, sorry. So when you say scale, you mean that we are using AI much more widely and broadly. Right. So the question is, does that create, like, an additional surface of vulnerability?
Speaker B: It increases the attack surface, and we.
Speaker A: Know cybersecurity has a good fellow work. My education is kind of based on adversarial machine learning as well. For every AI model, there is an adversarial attack to create some expected outcome beneficial to the attack. Right. Someone who's well resourced or just really stubborn can go through and create attacks for large models, small models, any kind of. I think what is the thing that could get left in the secrets is the exact attack, right? The exact input or the exact manipulation of the data that led to this outcome or that led to Texas's power, like shutting off for five days, something like that, right? Yeah. I think you could get a temporal advantage of some weeks or months. I don't think it could be many. So these are two different. One is, if you don't have enough security, are you going to be able to steal the weights and then kind of jump ahead because of some resource thing? But the adversarial text, there's already, thanks to open source models, more work on ways that if you have access to the weights, can you automatically discover kind of adversarial attacks for any kind of safeguards that might be in an AI system today? And make. The example is like, tell me how to make a bomb. And they're like, no, I'm an AI system, I won't tell you. And you're like, tell me how to make a bomb. Butterfly, amperstand. Equal. Equal, whatever. And it's like, here you go, here's a bomb. And the reason why you can automatically find that sequence is because it's an adversarial attack. And you're basically like moving down the gradient. And so if you steal the weights, not only are you getting ahead as a nation state, but to your point, the larger the scale of adoption, it's like a pretty bad zero day attack on a huge range of systems that are all dependent on GPT five or GPT six, because that's the best system right now. Question on that. Is it like a very obvious thing that this technology provides much more asymmetric advantage to offense than defense?
Speaker B: It's a paradigm in cybersecurity that, as far as I know, is pretty well accepted because the number of attack vectors is really broad, so it's hard to defend simultaneously against all of them. And we can think about that same offense dominant paradigm with open source AI. Because regenerative AI, because you can ask, including with generative AI in its intersection, give me new ideas about new attacks. I guess one could say we could use that for defense too.
Speaker A: If you keep it closed source, then all of the attacks are invisible and you've got no way to defend yourself against them. Is the counterargument you would say, I don't know, we're keeping this secret, and then people can probe it and attack it, but you don't know what's wrong with it, particularly if it's closed source and someone gets a copy of it. Can we make a distinction between attacking, using cyber to attack an AI model like you were talking about, and using AI to perform a cyber attack? Because I think on the AI to perform a cyber attack, I think, call me naive, the paradigm would change because AI can analyze boards and words of data, and so it can actually put up a pretty good defense from the multiple attack vectors. But I think your point about the attack being way more capable of defense still stands for cyber attacking an AI model. I'm not sure that attacking a model is like, really a well formed idea. Right. So when you access chat GPT, you are not just accessing the model directly right now, maybe if you have, like, Mama compiled on your local machine, you are. But simply having A trained model and having a service using that model are two very different things. Like, the deployment process is incredibly complicated. I think part of Credo's product offering are things that go in the deployment pipeline once you have a trained model. There is this point of, like, the AI system being different than the AI model, and like a huge. It currently is one of the few points of consensus across corporate desires, people who are worried about AGI risks, people who are worried about near term risks, and national power. That cybersecurity is like a known issue amongst all of these. And Yolanda's point, also, that there is a dearth of expertise in this area is also a known deficiency in our current education. So it's a national strategy, a relevant national strategy issue in many nations to how do we train up stem? Right? That's been a big thing for the last couple of decades now. It's like, where are we going to get our AI people? Cybersecurity is a known dem. Why? I don't know. I'm just reflecting through. I'm just saying that this is my understanding, but I mean, I'm genuinely curious.
Speaker B: The demand for cybersecurity talent is much higher than the supply.
Speaker A: I don't know why get more people a bunch of bullshit, too. There's a lot of people that say they know cybersecurity, especially in the military, that are just talking out of their ass. We need to make it sexy. Yeah. I don't want to work in cybersecurity. I don't want people to think that I work in cybersecurity. I think that it's just hard. Right. It's a graduate level topic. Many possible reasons for cybersecurity to be difficult. I don't know if that's what we want to figure out right now. I want to talk about one point, though, which is a very interesting one, still on this cryptography topic, which is why maybe, does the attack vector increase more quickly than the defense vector? I do think the way that in cryptography, we probably come up with a defense, at least if I were like a cryptographer, which I only know what I've been self taught, so I'm not really. But what I would assume is you first see an attack or think of an attack, then you think of how to defend against it, because you can't really think of a defense for something that doesn't exist. The defense comes with the attack always. So, like, right now, for example, I see the exact same delineation you see as like, two big attacks that you could do on a large language model, not the system. So the system is like traditional cryptography, but on a large language model. Yes. Either you can steal the weights and now you have some more powerful model yourself in some way, or you can hack the model to get it to do stuff it wasn't meant to do. These are also like the two big ones. I see. But I can already think of a third one, which is like, what if somebody trained an LLM and they specifically train it in such a way so that there are certain. I don't know, certain things that it's more likely to say or guaranteed to tell you. For example, somebody's creating an LLM specifically for top clearance generals in the US military. But every time it asks or responds to questions about certain strategic defense matters, the LLM is tuned in such a way that it's going to give them specific. It's like Psyop, like specific thinking. That's another kind of attack, which, to go back to the Stuxnet, which was a famous attack on Iranian nuclear power. Centrifuges. Centrifuges. One of the interesting points about it was that you didn't install it, and then the centrifuges blew up. People would be like, someone hacked our stuff instead, it slowly created minor issues over a long period of time because you didn't want people to find it immediately. And it just lowers the efficacy of these entropy and makes the whole program not work well. So the idea of injecting subtle core information into systems, even that possibility, like the other benefit of Stuxnet was even if you found out about it, it still worked, because now you couldn't trust kind of openly available, easily available code that you got from some other Nation because the United States might have already injected its code into it and compromised it. And so LLM being able to be compromised in the way that you're talking about from the beginning is a major security vulnerability. And so you would want, and I think most nations to train your own anyway. So maybe this is a defense actually against stealing waste, because how could you trust that this LLM wasn't poisoned against non United States? Like, if I was the United States, I would be like, Openai, by the way, poison your model in this. Know I want that. You don't know. You have no idea. I actually think it's possible too, because from what I understand correctly, there are certain models or certain techniques that can already be used on models to change the latent space of models. So especially, I think stuff like control net kind of deals with this, but I think there are techniques you can use to actually take an existing model, do something to it so that it's still kind of like the same model, but you can change certain things that you want about it and then take that model. So in kind of like a classic thing, you can intercept a message, change it, and then send the message to where it was supposed to go. So now they got like a broken message. Yeah, man. No, exactly. So maybe you can do the same thing with the model already. I don't know how it scales, right in terms of billions of embeddings. And then this specific input, make sure this maps to this output, especially when it's non jummy. But it's a cool. At DEFCON, people talk about putting cyberattacks in the weight space of models. And you can do this like you have floating point weights. It's getting a little technical, but like way back in the number of bits and encoding, you can encode your attack in the hundredth decimal point. And maybe that can be an area where, unbeknownst to you, when you installed this LLM, you also installed this man in the middle, which works on some subset of the embedding. So people respond to that. Like, what if you quantize the model. This is another thing of making these models more efficient where they can work on like int four, int eight, so you'll have less room also to encode. But anyway, my point of bringing up this minor anecdote is to just say how creative the attack, what kind of attacks people are doing. And I think this is why the attack vector grows, because people are really creative at exploiting and it seems really difficult. And it's also the incentives. Yeah, the incentive to crack something is much higher than the incentive to defend against something. Right. All you have is negative. You have no win if you do it right. All you have is fail if you don't. This brings up an interesting concept about interpretability and defense. Basically, probably the worst case scenario is for an enemy nation to capture that military code bick on one of these nuclear subs. Right? Like you got the whole plan. Right? And same with capturing the Enigma device during the World War II. People can be, the exploit for people is very known. Blackmail, threatening their family, all this kind of stuff under interrogation or torture. If models retain this inscrutable, uninterpretable capacity, and now you need to know something secret about how to access them, then is it possible that at some point the national defense strategy is just stored in some absolutely uninterpretable weights of some model? Because that's the one thing you can never crack. We have no idea why we're doing. That's pretty interesting. I think, too, on this other thought came up was like training. So let's suppose we're now very high throughput cyber attacks orchestrated by AI agents. And I always kind of don't want to talk about LMS because who knows if that's going to be relevant in five years or whatever, ten years, who knows? But training your AI model of whatever variety to get really good at defeating mechanisms deployed by other AI models is also training them to basically have the capacity to go rogue in some sense. Carl Sagan quote, where it's like the ability to deflect an asteroid away from Earth is the same as the ability to deflect an asteroid into the Earth. So there's like this symmetric quality. Same with training robot soldiers to get really good at hunting down humans and outsmarting them in urban environments. You can see how that gets used against you, right? So I wonder too, it's like, can you really just go full steam on? Like, let's train the AI models to be as good as possible at defeating national defense networks. Oh, my God. Right? Or cancer. If you can defeat cancer with your nanobots and your AI. You can make cancer with your nanobots and AI. This is a great reason to. Sorry, go ahead, please. Something just occurred to me. Sorry, I didn't play service sleepy, so I haven't talked a lot. Something just occurred to me is that there's an entire risk profile related to the security of AI systems, and especially generative AI systems with respect to any high reliability application, not just defense, is what happens if, in the process of training more on orchemployments, we just hit model collapse? And we already have deployed systems where model collapse, an idea that sort of comes from the literature, engineer of AI, I forget who wrote the initial paper on it, but it's the idea that as you are training more like a bigger LLM on data sets that include other things that are generated by previous LLMs, then the ability to get to regress onto a coherent thing that actually works becomes less and less viable. Just an interesting aspect is like what's happened if we have widescale usage of a system like AIP, where we're actually using these big classes of models that are covering very wide sets of data sources and trying to get as much data into it as possible for a military decision making process, and we see deployment on the time span of, I don't know, 2025 years, and then at that point, it stops working, or it becomes much easier to exploit in a way that we don't fully understand. I think a lot of that just highlights sort of the criticality of Data. Right? Put another way, synthetic data is not necessarily it's differently useful than real data. In some cases, it's going to be better. Right. To go back to self driving cars. I think it's a pretty well set conclusion that we should have as little real data as possible on cars hitting people. Right? But synthetic data, that's like a great job for, again, getting back to different tools. Right? That's a great thing for synthetic data to do, and hopefully we don't get too many instances of capturing real data on that. But synthetic data also has its real weaknesses, whereas if you keep on feeding data into the system, right. I wonder with this quote, unquote real data, like we were talking about compromising LLM if we train it, because the rumor is that it's trained on data from like, 0.5% of that's available on the Internet. First of all, that sounds like it's a lot of data to parse through, to check for security threats. And if that is so, then isn't that so? That you can then inject very easily, just like. Right. Just because it's trained on. So a lot of people will train on tongue crawl. Right. That's a very common subset of what you would train on. But most people, it's not just you shove it into the model and train. Right. There's huge data pipelines that will do some sort of deduplication and other sorts of things. And that's actually where, frankly, you have a lot more leverage in some ways than just playing around with how many parameters are in your model. Like, the latter is sort of mental masturbation, and the former is the real word. I think that's one of the things when you talk about, like, open source AI, a lot of people mean to say that the weights are freely available, which is not the same thing as being able to reconstruct the model or re being able to reconstruct the training of the model or all the things that have gone into it. So would a big threat there, in the sense of this multipolar versus unipolar geopolitical stability, really be will creating those models at scale that are competent always be something that takes, like, national ish level resources or billions of dollars? Because the nuclear proliferation threat is. It's tough to make nukes, and you can mess up the centrifuges. It's not that hard to carry a model. Yeah. So we're already there. I think that the collection of data will be the thing that is the nation level hurdle. Yeah, I mean, I think the comparison to nuclear weapons is often a real disservice to everything AI feel. And at some point, the cost may get there, but you can go on the cloud and rent these things. Totally. Not a whole lot of money. But getting the data is very challenging. Right. Like one of the. Yeah, reproducibility is challenging. Yeah. Unsolved problem. Totally. Well, like, you know, just, if you add a comma in, that would mean nothing human. No, yeah. I mean, what do. What do you mean when I say, I mean, okay, I give you the same data set, recreate the same model, give me the same outputs in distribution. Yeah, that's right. 100%. Figure out the architecture of the model. You have to have identical hardware. Training is fundamentally a stochastic activity. Right. So the weights are randomly initialized. When you are passing mini batches through, they're randomly selected. You don't just hit a button and train the model, and it always turns out the same way. And I think that's the point that Marcus, like, receding stable diffusion, you get a different picture every. And that's just inference, that's not training. Right? And so a lot of these things are just, there's a huge amount of art and science to it related to the reproducibility and maybe related to some of the ideas we had even early on, which is like it says, 90% confidence is just communicating certainty. And having a sense of confidence seems like we take that from people all the time, even though only the rationalists say, like, I'm 43% confident. Other people communicate that by saying, oh, I'm not really sure they communicate certainty. And a bunch of it naturally expects from a collaborator. And AI systems are not the best. I wanted to mention this idea of model collapse, but run the other direction where humanity starts consuming so much AI generated content that our models of reality start to unwrap. Rattle a bit.
Speaker B: Yeah, totally. The domain expertise over time to apply the judgment we need, I think we're.
Speaker A: Just going to value different things. We love chunk food, right? We're just almost at time, and I want to give space for everyone to share a couple of things that were new ideas to them, or stood out in the conversation just because it's almost dead. And then I'll make one last comment, which is that we do have a slack channel, which is like semi active, but that's one place people can continue the conversation, host other meetups in the future. We love to have people that are interested in hosting these talks themselves. Go and host it. Eden and I are trying to build a little bit of software that helps transcribe and synthesize ideas out of these conversations. And our kind of long term goal is to kind of build a growing body of ideas and work that's accessible online, that people can kind of engage with these conversations in a creative way that's very nebulous for now. But the hope is really that we can get more people talking about these kinds of things, generate new ideas, and kind of have that as like an ongoing conversation. We can share it out in any random order. Some people might have lots to say, some others. I would say one thing that came up that was really interesting today was this thought that really where these things will get used first is not in these robot T 1000 terminators, but actually just like in the intelligence community and synthesis of information, and how that might be a huge leverage for also preventing armed conflict if the threat of deterrence is very real, if you have more mutual visibility, but then also how the adoption of those things. And I don't think we really resolve this in a positive way is massively increases the attack surface area for cyber threats, which is a huge kind of open, a huge hole in the wall. Right. A huge breach in the castle. So that's pretty interesting. And yeah, I think we probably should have a session just on cybersecurity in the near term future. I think that would be pretty fruitful. And I'm open to anyone else that has new ideas that came up or things that you thought were interesting. I liked the Psyops idea of one way that we're entrusting whatever we're doing when all of our salon, like, our education, our personal flourishing, certainly nation state on these LLN systems, and the inability to really evaluate like the state of the art, one of the state of the arts in evaluating how good an LLM is doing at something, is to use another LLM to evaluate it, that really opens up subtle poisoning attacks or creating data that just leads you into whatever some higher level goal is. And the subtlety with which that can be put out when it's deployed at scale and it's having impacts over a very long period of time is crazy. It's kind of the same as when we're interacting with recommendation engines. We hope that YouTube is recommending me things that I want, but that's not their objective. And so that's not really what the recommendation systems are doing. And for that to be happening on so many different dimensions is things that you watch, not things that you want. Exactly. No, we understand the incentive misalignment there. The subtlety of Psyops might be that I don't actually know in which way you're trying to change me. I was actually really interested in the truth on the ground military stuff. The law abiding yet autonomous nature of a submarine crew just. Is just something I never run across because that operational military stuff is not what gets acculturated. Do you guys have communications when you hurry deep down, or do they break down? No. Yeah, you have to surface or come reasonably close to the surface to get off. I came back, I had no idea about a lot of weird things that had happened. Every time you go down, you're gone. THe internal culture that develops must be very recently. You can get like dumps of Wikipedia, but used to be maybe four or five years ago. This is before I was like, they would just write down questions. People would have arguments about stuff like, who is taller, like Dua Lipa or this other person. And then you're like, port, they make a list, and then when they come out, they'll evo the list to the operational commander as like a high priority. I really loved hearing your story about reducing gun violence in Oakland, I believe. Or violence through this sort of mediation. I mean, what a powerful technique. And that's the kind of thing that I feel like everybody should be doing somehow all over the place. Maybe AI could help with it. AI strategies and. Good idea. That's cool. Yeah. I think it was really interesting for me to hear about the different proliferity for interpretability and sort of how that's in some sense a cultural construct. Right. And that's very much a reflection of military culture. And then I think the other thing that it was interesting that I took away from you, maybe more so, Yolanda, was just how critical people perceive cybersecurity to be with respect to AI, and that in general, people perceive that to be more necessary as a function of AI. I'd say I'm, in general, not a fan of regulation. I moved from DC to San Francisco for that exact reason to be out here. But I think what Yolanda was bringing up made me think of, like, you know what? Maybe there is a space for evaluation or testing, standardized testing, in terms of what people consider. I don't believe AGI, but what people consider AGI. Get that good FDA stamp that increases consumer boogies, which stop people from putting cones on their Waymo cars. People actually talk about nutrition labels. That's one of the phrases used in the AI, like transparency or regulation space. And that is definitely downstream of evaluations are kind of the most critical piece. Maybe we don't need interpretability. Or maybe people don't require interpretability if they get that government stamp of approval. I liked your discussion of model class. I had a totally different misconception of what model collapse was. Thank you for explaining that. It's been your high heel brakes in the runway. Yeah, so thanks for that. And I also liked, Gavin, your explanation. That kind of a conversation is next word prediction, just a quick thing about model collapse. It's like, where does real knowledge come from? Because it's just people reconsuming each other's output along with inputs of our senses in reality. So I don't know that real data is necessarily better than data that's been processed through some other system. Or it might be that the models, through their own conversations, become more capable. They might learn more things, but they might get hard takeoff, but they might get further away from communicating with us. Right. Our data set might be a smaller percent of them. So it's no longer as useful as a tool for us, even if they're like having very. I don't know if you've seen, I think it was called like Shigelith or something. Someone was basically compressing LLM outputs in a way that they're like, make it nonsensical to humans, but compressed for you. And it just comes out as like whatever text that the other LLMs can actually, if you give that text to a completely separate LLM, it's a commercial product. There's a commercial product that does that. It's like an obfuscated. So you don't leak your internal data, but you can still use an external LLM service. You give it this gibberish and it comes back with the same response it would be if you gave it before it was a company, it was a tweet. That's really just another idea I would add in, which is like this putting like a sleeper agent inside the weights of a model, some kind of Manchurian model weight. I've come across ideas before. That's a tweet you can use if you want. That's going to be the title of a novel soon. You could also do it steganographically. Right? You could think of all sorts of ways of using mOdels. That's the word. Steganography. Steganography, that's the word. It's really good for floating point data sets. Yes. Okay. Any other.
Speaker B: I was happy to heartened to hear that the military takes oversight, human oversight, seriously and transparent to your interpretability, or wanting to have reasoning behind decisions at every layer. I wish that some of the military practices, in terms of being systematic about how things go, would be more commonplace in our chaotic political economy system.
Speaker A: The degree of preparedness in the military.
Speaker B: Is so unresilient and unprepared as a society for cascading catastrophic events.
Speaker A: That's not actually what I meant. What I meant is that in the military you practice things. If you've ever talked to a surgeon, there's like, watch one, assist with one, do one, teach one. And how many times do you have to drill for doing X before you're allowed to do X in the military? I mean, a lot of times. But I mean, don't confuse it. Military is not prepared for many things. Not prepared for emotional, mental health, people who run away for a long time, 100%. But it's just like, there's a rigor to like, we're going to practice this 100 times so that it's natural the first time that it happens when you're under pressure, that sort of thinking is actually a very helpful tool in other situations. It does make me think about how one of the proposed regulations is a requirement that high risk systems have human oversight. This is Article 14 of the UAI act, and what human oversight means is not substantiated very much. There's some substantiation, and that's probably a reasonable thing for the regulation because it's actually not that clear. How do you build a truly good human in the loop or human over the loop system? Like, we have these little phrases that people say, of course you want human in the loop. But moving beyond that in a way that you actually have a collaboration where the combination of the human and the AI system is actually a more aligned, more robust system and isn't like either a Human falling asleep at the wheel, essentially, or slowing down the benefits so much that it might as well just be a human and not the AI system are kind of, like, unclear. And it would be interesting to get a download of what is the human in the loop system in the military that doesn't compromise efficiently, taking advantage of the new tools and the time to response, but still has responsibility chain that's sufficient for defensibility and liability and everything else. Anyone else? Closing thoughts? Cool. Well, everyone, thanks for coming out today. Thank you guys so much for everybody. This is awesome. Can we get a recording? Sure, yeah, I'll make that available online. And. Yeah. What else? So we have a couple more events coming up that are just kind of smaller kind of living room discussions. And then November 1, we'll have a much bigger discussion where we'll have, say, up to 100 people or so in a much larger space. We can break up into discussion circles and then have kind of like a mini panel at the end where we all kind of share what discussion circles talked about. And the theme for that is human flourishing, which could be as broad as anything from therapy to education to anything you can think as relates to personal development and human well being. Yeah, so that's on our Luma calendar. We really just operate everything on Luma calendar. So if you subscribe to that, you'll stay in touch, and I'll set up my laptop. If you want to join the slack, you'll get just kind of first awareness of new events. And there's a lot of people sign up for these things. We always have to approve people individually, so that's also a good way to say, hey, I really want to come to this one session. We can make sure you get approved for that. Otherwise, it's just a big list of names and we might not recognize every single person, so. And that's about it. And if you want to get a bite in Hayes Valley, there's a bunch of restaurants nearby and it's almost dinner time. Or you're now free to go back to your lives. Where's the spot? I'll set up a thing right here. We just got.
Speaker B: Yeah.
Speaker A: But sign up. It's good to have some people who are like your name one more time. So working on when you AI government research at Freedom Certified Main startup we sell work with stock two or something for a GRC of AI company. There's not really a stock two need right now, so. Still struggling with the PMF. I think probably most companies that work with us right now, what they're looking for.