Speaker A: We got a vest guy here. I am a vest guy, but I am, I am, I am only here to keep the loud mouths from keeping the quiet people down. Okay, as a, as a loud mouth, I rely on all of you to watch the watchers.
Speaker B: We're just gonna.
Speaker A: Just for audio context, this is economics and AI and economic and ethical implications of AI. Oftentimes ethics are influenced by economics, it turns out, so rarely, it seems the other way around.
Speaker B: Fortunate. Well, I think I was watching the, I think she talked more about how we need to really change our educational system and because people are using AI to now just cheat and have the AI do their homework. So she's saying, okay, well, maybe we need to not have homework anymore and reinvent our entire system. I think the problem is, and same thing applies to our economic systems, is that we can't take it away anymore unless everybody just comes online and decides like, okay, let's ban openai. It's no longer legal. We're not taking systems away. Now. It's more of a question, like, how fast can we adjust our other systems, including economic, educational, to match what we created with AI? That's.
Speaker A: I'd like to call up that. I was actually having this discussion with a teacher who says, well, chat GBT's just a plagiarism machine or a cheating machine, and hosted some known Chomsky. And I'm like, well, maybe like right now. But it also puts a lot more load on the teachers to understand if their students have actually absorbed the material, right? They can't just read through a paper. They now actually have to engage with the students and see if they've absorbed that material. So I think that's going to have an economic impact in the world of education for mean, I guess, just like.
Speaker C: It'S also just missing the point of a lot of what we like, a lot of the writing is to learn how to write and how to read. Like, half of America doesn't read at a 6th grade reading level. That's going to get worse if we don't need to actually write our two papers anymore. It's a little bit like a calculator, right? CalculatoRS came out and people don't know how to do a list. And that just helps you learn logic, even if you can do it. It's like a question of, like, is it a good idea in terms of building those basic skills of reading writing?
Speaker A: And.
Speaker C: That'S pretty important.
Speaker A: So my question was, is AI going to give us a systemic ability that is more competitive than capitalism? Because I think we've all experienced the sharp edges of what capitalism does. So I wonder if we can talk about any. I have no idea that this is even a possible thing. I'm just wondering if any of us have seen a glimmer of where artificial intelligence could unwind something that we take for granted as sort of a rule of physics that won't be going forward. I think that the human needs are growing in a linear way. What we have today is much more than what we had like 200 years ago. But technology and AI is growing, so we have a lot of gain. What the humanity needs can't understand, we are able to generate much more than what the humanity needs. And so we have this gap that we need to understand how to. Talking about capitalism, it'd be kind of the antagonist of the farmers right now. It's kind of we have the technology, the potential the farmers might win. It's like we are not distributing anymore, like just piece of bread or dangerous part. We are able to distribute something more valuable for the use. Yeah, I mean, the incentives of capitalism are winner take all, do the best for your stockholders, you'll get sued, et cetera. Is AI going to give us a tool for doing better than that in terms of how people experience life? I'll put the term technological socialism out there as just a concept. There's socialism, but with technology, we might be able to use AI and robot and everything to UBI to distribute the wealth. Just technological socialism as opposed to just regular socialism, whatever that is.
Speaker D: The part that seems the most interesting to me with what you guys just said is capitalism is optimizing for technological.
Speaker A: Growth at this point.
Speaker D: And with the topic of the table, which is economics and ethics, I'm curious if there's a way to continue using capitalism to advance AI, but align that with ethical development of AI at the same time. Because it seems like right now we're just optimizing for developing and deploying it as fast as possible, but it doesn't seem like that's aligned. It might be, but it doesn't seem like it is aligned with also making sure going to be good for us. I think right now, current AI systems do have the direct incentive to be ethical in some way. Right? Like tragic tea, et cetera. Do not say out certainly like racist things or racist things. You prompt it. But right now there's like content filters for within healthcare.
Speaker E: It's not shown that diagnostic. It's more likely to diagnose a black person who's got sore throat with an STI. Compared to a white person with mono, for instance. And so there already are biases within it. Training set.
Speaker A: Yeah, there's training data biases, which are.
Speaker E: Leading it to already cause problems.
Speaker A: And they're also, like, you can think about AI not just as chat GBT, but all the machine learning systems that optimize for some economic goal, like getting your engagement, selling you more advertising and so on. So think of the Facebook experience and the way that you get enraged talking to your relatives on Facebook because AI has surfaced those enraging conversations.
Speaker B: I think what's interesting, that we basically had the same situation with social media where we let the genie out of the bottle, didn't have any regulation whatsoever, and then the government just sort of started catching up way, way later after this debit was already done with the election and polarization, everything. So now what's good is that they just passed. So they're trying to get ahead of the AI trend. So I think that's quite promising. And I think that they're trying to learn the music of social media. But the question is, is it possible to, when everybody is racing in the capitalist system, to create the best models and have them own the market, is it now possible to regulate that where there's not going to be secondary effect? Job losses, companies just, I feel like on every podcast now, all these enterprise leaders are saying how they don't need to hire anymore. All our staff is already 30% more productive. We don't need to hire. And now LinkedIn just laid off 10% because of their engineers, because they're like, well, we don't need them anymore. We have AI. So I think that we're still very, in the very early stages of that trend. And what's going to happen to this whole model of people must have a job when there are no jobs, because now people that are in those jobs are effectively doing a job of five people because of AI.
Speaker A: That's always the question, like technological productivity, putting people out of work. So we've gone through it with mechanization and automation and the industrial revolution and so on. What do we think might be different this time?
Speaker B: I was really interested in your question that you brought up about could AI or AGI or any of these tools change something? That right now, if I understood your question correctly, that we see as kind of like fundamental, or the parameters of.
Speaker A: Yeah, I mean, what are the physics, the laws of physics that we might be able to break?
Speaker B: I guess a smaller part of that question that I had are, what are our current laws? Or what are some of the sets.
Speaker D: Of things that we do.
Speaker B: Yeah. And then maybe we could brainstorm what those are.
Speaker A: I think the question being, how do we define capitalism and basically how humans participate in it and how, I guess, organizations participate in it and how nations participate would be like an interesting question to kind of agree on. And then kind of like, what changes? I think labor plays a part in it where you can accumulate small amounts of incremental amounts of capital to labor. That's a question mark, whether or not that's safe. It's kind of like the starting point of accumulating capital. Seems like maybe that. I'm not sure how kind of effective that starting point looks in the future where human labor is maybe not rewarded. That's kind of one big question that I wonder what replaces that, right? Like creativity? Is it kind of intuition that we can kind of give them a shape? How do you kind of replace labor in a world of infrastructure skills?
Speaker C: So if you're talking about definitions of capitalism and tying it into. My favorite definition of capitalism is it's a system of information. Literally just all it does is provide information to producers of should you make more of this stuff or less of this stuff? That contrast to be like more of a command economy where it's a top down, hey, let's make more widgets of X, Y, and Z. Now, you could imagine a world where you don't need the market invisible hand to guide information about what we need to produce, where there can be a smart plan economy. But that's like functional because you're not limited by human limits like the Soviet system. That's possibility to sort of the question of like, that's a redoing of capitalism, right? It's just a question of human incentive and motives, right? Because it's like you don't need to, if you get what you need, not based on your effort, encourages a lot of your writing and all that stuff. And I think that's sort of like where human nature gets into it and there's some people that are very philosophical and very much are able to overcome our biologies and our sort of inherent drive. Probably a lot of people in this room, I don't know if that's a question.
Speaker B: What if that's not our inherent drive? I think that's one of the laws of physics of know, people didn't have to compete. Maybe they still would innovate. There's like premises kind of like underlying.
Speaker C: Yeah, sure, sure, yeah.
Speaker A: I mean, like you took the sort of the potlatching tribe to the Pacific Northwest that lived in a very abundant environment. And sociologically they don't have a you must work. They were like, well, we're going to show off by having enough wealth, enough excess to give away. So that was the sociological incentive, but it was not the survival incentive that modern capitalism is telling us to go back to our offices to enjoy.
Speaker C: But I think the idea of working, I love your point in terms of that, hey, we're entering a situation where we have so much abundance, right? And we can produce so much more. Just tying it to this original thought of human flourishing. There is something very meaningful to doing work that provides us purpose, right? And as we lose that because we don't need to work, it's really the question of what replaces that. So even if we can remove the capitalism, remove these incentives of needing to work to get done, is there actually something in place that could actually assist us in still having meaning and being able to? I think that's like also great.
Speaker D: What would that be for you?
Speaker B: I think it's highly individual, based on you and your unique gift that you gave into the world with. And I think that's what's exciting about AI, is that instead of doing the job just for the paycheck, just to ultimately hate, which I think is a very common experience, most people nowadays, especially in the capital, where you don't have enough work because there's no safety net, versus here there's a real opportunity to have these AI systems do the work that people are not cut out to do, or they just don't find interesting and inspiring and focus on their gift, which might not have economic incentives to put them to the best, right.
Speaker A: One of the questions I think that I'm asking myself is like, maybe capitalism has like in an ideal state, not in the current kind of state, in an ideal state has kind of like an alignment with our kind of progress as a species and our survival as a species. Kind of reward efficiency or better services, better products, better skill sets and things of that nature. And I think that kind of alignment of, we all kind of want to make progress. We all want to survive. So survival, progress and human kind of energy are aligned in a world of kind of full abundance and lack of survival, existential threats, what drives our behavior? And to your point about meaning, how do we kind of align towards some sense of progress so we don't just kind of stop, right, and become aimless?
Speaker D: Kind of like, well, I'm curious on use progress, and I have my own internal definition of what that is. I'm really unclear on what that even really means as a society and we're progressing to.
Speaker A: I think you're right. I think I probably have in my head GDP numbers, which are not a good reflection of our kind of health and wellness as a species. I think one thing that I don't.
Speaker D: Know, I feel like I hear, but then I never understand fully that AI will get rid of jobs we don't want to do. So we can focus on can do and want to, but I really feel.
Speaker A: Like it's going to also get rid.
Speaker D: Of things that we want to do and find meaning. I feel like I see those things might go worse. I feel like it's far away from replacing a plumber, but it's closer to replacing making art, something really meaningful that doesn't really provide value to the economy, but it has more to do with how they think and how they perceive the world. And my real fear is like, what happens when that kind of work can be done by something that you have in your pocket, like what gives you a drive?
Speaker A: I think that a lot of people who work in tech are concerned about their jobs. Right. We think about it a lot. A hurricane hit Acapulco last week that annihilated the whole city. I think climate change is going to be a defining issue of the next 30 years and our adaptation. So there'll be plenty of work to do to build defenses against insane weather that comes in. So how do we deal with. So I think there'll be work to do. Won't necessarily be tech. Until they have Tesla humanoid robots rebuild the city. We're going to have a lot. And this is kind of what I mean about can we change the structure of the incentives? Because I work with a lot of people who do computer security. Nobody wants to spend money on computer security because it is a cost center, not a profit center. So is mitigating climate change. It costs money. You're not going to get rich mitigating climate change. All you can do is lose less. Right. So what are we going to be able to do with a better understanding, better AI tools? Go ahead, Terry. Not to interrupt, but I'm just going.
Speaker D: To cut it 1 second. Yeah, I have talked about how we're.
Speaker A: Going to split into two groups, but given the time, we're just going to.
Speaker D: Keep the conversation kind of going till about eight four, and then we'll come back together. That's all I just want.
Speaker A: I think it's possible that our measuring system actually leads us to believe that we cannot generate wealth by kind of getting ahead of climate change or kind of having investments in that. Similarly to how we measure security, right? Like the absence of an intrusion versus building, I guess, better firewalls and systems and monitoring and things of that nature. Right. If we quantify that as an asset that you're brewing, if we quantify the trees that we have and the health of our population and things of that.
Speaker C: Nature.
Speaker A: In the same way that we quantify transactions. Right. And efficiency of labor, logbook on our orders in manufacturing, if we can change with value, I think that word came back, and I think we can kind of accrue that, right? Maybe one of these answers is AI will help us rewire ourselves into what we consider valuable. I think that is really interesting.
Speaker B: I want to thank you for including my voice. I have to head out because of early morning commitment. I've taught anthropology, and there are three things I carry into conversations with students. I'd say, first, we are evolving. Evolving just means change up and down. I think if there's any point, I prefer upward change. So we're evolving. Technology matters. That's why all of you are so interested in what will this mean? And there are alternatives. Being drawn to ethics comes from seeing how new technologies come up, and it depends who holds the money to drive the innovation. So I think a big question is, you're bringing up environment, you're bringing up meaningful work and things that will give you purpose. So the question, I think is, who will plant the ideas? Because those ideas splinter into other ideas. So if only certain demographic in certain countries with certain needs to be satisfied, will it really help you again? So I, as an anthropologist, feel very drawn to all voices and multivocality. And I think it'd be really interesting to define AI for people who've never even seen a computer, and ask them, if you could have all of your daily work done, what would it be? And to get a sense of what humans. So we are limited by what we're raised in, by what we work with, by who our friends are for opening up. Can I rip on that?
Speaker C: Actually, one of the things I think a lot about is how different cultures in different countries will respond to the emergence of AI, because I think we'll be really different. Like in the US, we're a very individualistic country, and we really value choice. That could actually be a pretty negative set of cultural norms that will allow for sort of unmigated and unregulated AI, right? Whereas think about countries like China, where they've already put in strong legislation on how much video games people can consume, how much social media they can consume. TikTok is very different in China. It's education. And so I think we're going to be running a lot of natural experiments based on culture in terms of how AI is applied, and it's going to be like, pretty wild set of outcomes. I don't think it's going to look the same everywhere, to your point. Really, really.
Speaker E: One of the things I kind of move a little bit to the ethics thing about, and also the climate change, say, the amount of energy usage with GPUs for LLMs and so forth, and reports of football fields worth of GPUs, data centers and all of that jazz, how much of the harms of both from an energy perspective, and also the bias and everything else that us encouraging human flourishment, how do we balance that with the potential harm in all of those areas?
Speaker A: As a first blush, where does the energy come from and what do we do with the Wave team? So if we're burning cold power GPUs, that's probably bad. If we're beaming in microwave power from geosynchronous power satellites, maybe it's good, or maybe AI will design us some proper fusion reactors.
Speaker C: Finally.
Speaker D: So it takes a really long time to create a nuclear reactor, like ten plus years. So most likely the next big model will be trained by, like, natural gas. It'd be enough energy to power the next thing.
Speaker A: There is an LLM that was produced in France and they powered it with a nuclear reactor, or I should say.
Speaker D: Like a new nuclear reactor, because I think roughly, I forget how long it was, but I think it was something like GPT four training was like equivalent to New York City for a day or some absurd amount of time. So it's like, as we continue training, getting more models, there's going to be more energy needed. So it's like you have to either take away from the grid, or add to the grid and bigger models. And bigger models.
Speaker B: I thought it was really interesting what you brought up. I was curious, from an anthropology framework, would you evaluate AI as you would any other tool, or do you think it should be evaluated under a different framework? Any other tool, any other technology could follow the same framework and evaluating and thinking about how it should be used? Or is it new people? What did you say?
Speaker A: Maybe a new people.
Speaker B: It's a new people, a new culture. It depends where you draw the bounds, but when you think a tool. We started with hammering on rock and making better things like that, and even chimpanzees, I think this realm is growing into a culture of its own, where interesting cultures are shared and learned. So far, anything that we're developing with tool comes with a set of shared assumptions. So I think it started maybe as literally physically evolving into its own culture. Are there other tools or technologies that you draw parallels to in history that you feel like also had culture around them? I guess writing and that it is symbolic symbolizes what we think large language models or whatever. Yet another language, the way that it puts things together has symbolic assumptions. That's a really cool connection. Yeah. I kind of wonder how structure it is both a tool because physically doing things. Yeah, I've noticed that a lot. I've been seeing things that Uvali anthropologist, historian and he has really great stories and books about this. My very first startup in the Valley, I worked for Tucson Harris was, I think one of the partners, and I know that they were just involved with this initial legislation that just passed, and I would recommend everyone to listen to your undivided attention.
Speaker D: That really caught my attention with what he said. I think you mentioned it briefly, but you really talked about how if AI becomes better than most humans at language, like at generating language and persuading you and talking to you, then in 100 years, right now, our culture is all based on language, and we have something living with us for centuries that's better than us language. What will that mean for our culture? Who will be responsible for what we value humans? Or would it be something else that is parroting what we think is human values but generating its own value? I don't have any concrete thoughts. It sounded really scary.
Speaker C: The one thing you just said is if AI becomes better at language.
Speaker A: I think it is. It already is.
Speaker C: I think it's better than the average American writing and reading, without a doubt. So we're past that. What does that mean?
Speaker B: We start getting.
Speaker E: How do we define better at language?
Speaker D: So far it's a bunch of tests, like the GRE writing exam.
Speaker E: But is that necessarily. Is that the bar?
Speaker A: I think the bar is.
Speaker D: Can you persuade human like, that's 1 bar, I guess. And then other bar.
Speaker B: Can you express your thoughts in a way that are understandable and sounds good to you as a person, not just expressing yourself with. You can't fully form a sentence. That's how low the bar is, also just comprehension.
Speaker C: I'm going to give you a passage. Can you tell me objective facts out of that passage?
Speaker A: Right.
Speaker C: Way betTer.
Speaker D: But the scariest one that also you already wrote about was like, can you form relationships with humans even if you're not a human?
Speaker B: AI can already do that like the whole AI girlfriend experience and all that. Right? Lack of understanding of the opposite gender.
Speaker A: This is sort of a side tangent, but humans really hallucinate someone else's state of mind all the time. They're very good at seeing patterns where they may or may not. Okay, so this is the subject of most of theological debates that I've been in. But regardless, I think if it feels like a real person to you that you're communicating with, subjectively, you're not getting very different information than you would from an actual biological human. So your subjective experience in AI is going to be very similar to your subjective experience.
Speaker C: There's a pretty big difference, though. Kids have sex with the AI.
Speaker B: Yes.
Speaker C: And when that changes, it gets how.
Speaker A: Much is in the mind, man?
Speaker C: But I mean, like, jokes aside, there is something very magical to touch that we all react to.
Speaker B: I mean, we have five senses. Being able to speak to somebody and have this, that's just like curing, I guess. Right? That's only one sense engaged. Where is that? Brain slides, right?
Speaker A: Sharing what I'm rattling personally on this, like, can we do something about the flaws that we're experiencing in capitalism with AI isn't going to help changing our perspective on what is valuable. I think using the language facilities of AI to convince us of doing something that's in our interest, that doesn't seem to be in our immediate interest. Make it feel like it's working. That's like a fruitful thing. What about the. Are there any practical counters to the perversing sentence of the United States current economic system? That was my initial question, was, is there something we could do that would outcompete it? So if you're AI powered and your job is summarizing legal briefs, maybe you could out compete human paralegals, put them out of a job, but you could outcompete them at some level. So that's like a toy example, but I'm wondering if there's anything we can do that's actually productive for humanity that will be better than the model of kind of winner take all. My shares are worth more and then have captured more of the economic value. Well, I wonder if a lot of the capitalism has an integration with the governance structure. And a lot of times capitalists will do regulatory capture and gain a lot of power. So you end up with three monopolies in it, and the Electoral College did that too. So, I mean, we can AI governance, rethink governance with AI, that doesn't allow regulatory capture in the same way. And things like that.
Speaker D: So earlier you were mentioning when one person can do the outset of five people, et cetera, et cetera. And so when I recently listened to a podcast, a lot of this is Bob from this, which I can share somehow, but it's like Max Weber, 80,000 Hours podcast, and he goes through this and breaks it down of what would happen if you just imagine just food, like, reduced in cost from the average grocery bill, I think per month is like roughly $500. That goes down to 100. And everyone now has 400 extra dollars. What are you going to do with the extra $400? Right? You can think of like, oh, well, maybe I get, like, a slightly nicer apartment, or maybe instead of buying groceries as much, I will go out to eat and do nicer things because human made this meal, as opposed to manufactured pasta, making this pasta. And so that also, on the margin might make it where you individually work less because you don't need to work as much. And so there's like, these really unclear arrows going to all directions of, like, well, now your costs have gone down, but then your propensities to do leisurely activities have gone up, but only for certain people. But there's also people out of a job because you kind of augmented maybe, like one person, and it becomes like a really fuzzy picture. So I'm just, like, curious if other people kind of thought about this in detail of what happens in this kind of slowly transitioning economy of very efficient, productive people.
Speaker C: I forgot about this.
Speaker A: I think we're mostly.
Speaker C: Just some of.
Speaker D: The points I don't think we are.
Speaker C: Where we're seeing AI really disrupt is like the middle class, right? Like white collar jobs that paid less than 100 people. A lot of those jobs can be either augmented so fewer people, or completely replaced. Where we can replace anything is like the lowest paying jobs in the economy, such as, like, warehouses, fast food, plumbers. Plumbers. We need robots.
Speaker A: Plumbers make good money.
Speaker C: Janitors, right? We're just really far away from those robots. So, like the COVID situation, we actually have a little bit of an experiment where the government made it, that made less than $50,000. It made more sense to be unemployment than to work well, what that caused was like, target, Walmart. Everybody has to raise wages to bring people back. There's no way to solve those jobs. That just created extreme inflation, because then we. Everything has to be the way we want. Right now, we're taking out the middle class job. There's no way to solve the bottom, the jobs that we really want. And so you're really just creating more inequality where you're only left with extreme, really rich people that are high end and low end and nothing in between. If somebody has a solution to this.
Speaker A: I would love to hear Mark's job.
Speaker C: That's one.
Speaker A: Smack up against human nature.
Speaker D: I think we're maybe saying the same thing I'm saying. Mark's basically said there's going to be a revolution and that just continually happens and that people overthrow the powers that be.
Speaker A: Right? Yeah. Would you say completely? I think in a short horizon, I agree with you. Long enough horizon, the bottom stack of work also disappears. Right. I think we are able to.
Speaker B: Fix.
Speaker A: A sink and things of that nature.
Speaker B: Yeah. Especially if you're in the middle. Like, you're basically majority of the population that no longer can't easily find the job. They're just like, well, I have this time on my hands. I'm going to become my own plumber now because I can't afford that.
Speaker A: I think like a machine, right? That's also a general purpose machine that can mop the floors and pick 80% of your household floors. I don't think it's a huge stretch to imagine that next decade something kind of like, I'm just going to riff on this and tell you a story, which is my sink is leaking in my bathroom, and I'm looking at it right now with my glasses, and it pops up something saying, hey, Gavin, you want to fix that? And I was like, yes. So about five minutes later, a drone drops off a set of plumbing tools. And in my glasses, I have an overlay of exactly how to fix this particular problem. Right? So I have no plumbing skills and I have no plumbing tools, but they've now been made available to me very quickly. And I'm being walked through this project step by step by step in a very fine grained way. So I think that could happen with almost everything that people currently do for free. Very good story. I like it.
Speaker B: So we all become the Swiss army knife to be able to do everything that we need and no longer need, except for something extremely specialized, which means we also don't need to make money anymore because we don't need to pay for anything. Right. That would be like utopia, kind of.
Speaker A: I would like to kind of try to take the opposite point of view, which is not necessarily I'm more towards the fear for kind of just constitutionally. Like, I'll fear for a while and then I'll try to see the upside. Look at the upside potentially, if everything is distributed kind of through these marketplaces modules like the plumbing module. It is possible that you could kind of just come up with a better module, right. And participate through, I don't know, like creativity or just knowledge and have a worldwide distribution kind of of the utility. And what if everything becomes a marketplace and everyone has access to the marketplace? Because ultimately it's creativity and concepts and kind of like the equivalent of code that distributes this versus kind of big pools of capital, kind of atoms that you can kind of quickly every niche will be occupied as it is on the app stores or YouTube or whatever. Yes, I agree. We can't all be.
Speaker E: I wonder whether hypothetically, can we have an environment where all jobs are taken away? Because in theory they say even something like fires within the system, unless we are in a completely equal society, which is probably impossible, there's going to be some flaws within the system which may not be fixable with AI, and consequently there may be jobs which are entirely different to what they are now. And I don't know, maybe there's funnels if there are fewer jobs. But I just wonder if hypothetically we can see ourselves within the society where there are no jobs.
Speaker D: I think even if it's like not every job goes away, even if we have that situation where only like 25%.
Speaker A: Go in the next ten years, we.
Speaker B: Just don't know how to deal with that. Yeah, I agree.
Speaker D: I think all these ideas that we think of, one common theme that I notice is everyone's like, in the next ten years, that's like really sick. And I feel like no one at the right level in our society, I e like governments are really thinking about on that timescale, right?
Speaker A: They're mostly going to be dead.
Speaker B: Same with climate change.
Speaker A: Yeah, that actually used to exist. We used to have people.
Speaker D: I feel like what's going to end.
Speaker A: Up happening is probably we are going to have like two or five years, something really bad, and then we have to react to it.
Speaker D: If we look at history, it's always.
Speaker A: Like reaction to stuff.
Speaker D: We always have to work with it. Feel like we need to make people.
Speaker A: Work on this now. Think about this now, right? Humanity's proactivity records pretty bad.
Speaker B: Health care, we're like, housing is a human right fighting for two jobs to be able to survive. Do you think that's a realistic thing within our current system, government? I think if you just reshifted, like the need for what? The role of a job, meeting the needs of general because there's enough. And like where we're putting value, I think that's not something that we need ten years from now.
Speaker A: That can happen.
Speaker B: That can happen. Nice. That's not the.
Speaker A: UBI.
Speaker C: How do you get somebody to.
Speaker B: So.
Speaker C: If we have UBI, how do you get somebody to stock shelf at Walmart? How do you fill out groceries?
Speaker B: Whatever. I have to buy a phone every three months. I have to buy plumbing. If we make something that lasted, like TVs are made to break, everything's made to break that we consume. So we don't need water, we don't need, Walmart can have farms, and we can have a buy nothing group that you can share and probably survive off of without ever buying something ever again. So, no, I don't need to stock on Walmart shelf.
Speaker D: There's also this concept. It's either called the donut economy or cyclical economy, which is exactly this, of like making fridges, for example, that last generations, or like light bulbs. It used to be a conspiracy, but it's actually just true that light bulbs back years ago are still running. And it's because they did get together to make it more efficient. There's like, actually in Stockton, California, there's like a light bulb that's just still been on since they first doesn't need.
Speaker B: The plumber that needs to come every three months. It probably won't.
Speaker D: But I still think to your point, though, there's going to be like, things on the margin. To your question, will we become 100% unemployment and like a utopia of some sort? Maybe like 99%? And then just like sometimes you work some of the time to be a janitor because no one wants to do that. No one wants to take stock shelves at whatever place.
Speaker B: There are plenty of people that want to do that job. They just don't want to do it for $12 an hour or for like, we're still at $7. We still have a tipping wage. We're in California. We forget people make $2 an hour in every other state except for New York, California, right? It's like, no, nobody wants to do it for that wage. If you paid a decent wage, right? Like, that's why people are unionizing more now. They will probably do it and go to that job every day. But we humanize that part, right? It was convenient. It was okay during that time. And then all of a sudden, all the work that we see as like new caller, low wage, no skill. That's the highest skilled job. These jobs that we're talking about require a high level of skill set. You don't need a college degree to do it, right? But many will argue that what a CEO does and what they probably have more value economy than what the CEO. But we've shifted of where we put that value because we brought that up. I have a question. I really like what you said about universal basic income and health care and gardens. I was curious, when you think about the path to reaching those, do you see AI playing a role in that path? Yes or no? And then if yes, what kind? I don't think we need AI. I don't think technology. Can it contribute? Yeah, that would be great. What is the contribution? Technology? Can I give it a prompt to be able to convince someone? Yeah, no.
Speaker E: I'm curious what I think you mentioned about we could change the definition of a job today, essentially. What would that definition be? Because I'm all for universal basic income and so forth, and that kind of just sets the bar the lowest bar at a level which is in concordance.
Speaker A: With health, blah, blah, blah.
Speaker E: But then there's still going to be a drive for people to work harder, to get more than their neighbor, to have a better quality of life. And so I'm just curious how you define what a job should be, what should be the purpose of a job?
Speaker A: I think it's a good point.
Speaker B: I want to hear. I wouldn't mind processing it.
Speaker A: You're raising the lower part of society, right? So of course you want some possibilities, but it's not a strict need anymore. There are no work, no jobs anymore. If we're going to raise up the lower bar and people can decide, or if I want a better car than your, but I still get a shitty car. Someone is giving me a shitty car, I don't know. Or whatever I need. So we are raising the lower bar and everyone going to benefit. And that is more volunteering, philanthropy, whatever you want to do, which is your passion. So you are flingIng. People will have more time to do what they like to do somehow, or if they want something more of what? The distant team.
Speaker D: Very useful.
Speaker A: I'm going to put out there that a time frame that 20 years from now, just to put an arbitrary number out there. We're in this transition zone, but in about 20 years, everything. Maybe AI will run a human zoo and we'll just see animals in the zoo. If we're lucky, we're lucky.
Speaker B: Can I just be in the sanctuary? I think I actually enjoy.
Speaker C: They'll call it a sanctuary trust, but they'll call it due to the shadow.
Speaker A: Oh, it's the kingdom of our masters.
Speaker C: I do feel like a lot of this wraps back to your original question of, like, how do we change capitalism? Because I think the idea of can we pay a living wage to these jobs that we currently don't, the reason we don't is would actually not work in this capital system because we're rewarding output. In terms of. Most of these companies run a very low margin. They don't make a lot of. Amazon makes two or three.
Speaker A: Right.
Speaker C: If you raise the wages of all their employees by 10%, suddenly the company. It's actually that simple. But that's just because of the consequences of capitalism. For us to achieve your view, I think we need to achieve what you want.
Speaker A: Well, yeah. And the consequences of capitalism is as Amazon's efficiency improves, right. As their margin improves, that margin is not returned to the labor force. It's distributed to the shareholders.
Speaker C: Well, potentially to the consumer, like lowering prices and things like that, that can.
Speaker A: Happen as well, but no more than required to maintain the market dominance to ensure the rising perverse incentive, especially in a world of concentration, kind of economic value. I guess you mean monopoly. Monopoly for sure, but just kind of like. Yeah, monopoly. Because I think this year's stock market kind of reflects that already. SMB is up a certain amount. All of it is kind of seven companies, right. So you're starting to see a dislocation between everyone else and then the companies that own these technologies.
Speaker D: And then a.
Speaker A: Bunch of private equity firms who have no idea.
Speaker B: Every grocery store in this country is owned by two companies.
Speaker A: Every grocery store, every Safeway. And no, it's not even Safeway.
Speaker B: It's above Safeway. There's two companies that own every grocery store. So it's like there's no such thing as competition.
Speaker A: Well, Amazon's one of them, yeah, but.
Speaker B: I don't think there's a parent company. And I remember the chart. There's like too many brands for me to know. I know there's two.
Speaker E: I'm just wondering, do you think the question is how do we, I don't know who we refers to. How do we change capitalism? Or how does capitalism change almost passively? Because in this setting, say, in a democracy, it's people, essentially capital. And consequently, if we are changing how the people are living their lives, I. E. There are no jobs, then therefore the value, what they're motivated by is different, then I wonder whether that might change their political view, such that the ecosystem which supports capitalism may shift.
Speaker A: Right.
Speaker C: When a quarter of the jobs disappear.
Speaker A: Well, you kind of have to do something because then it's revolutionary. The rich definitely don't want are you.
Speaker B: Asking what the 1%? Was that what you're asking?
Speaker E: I guess I'm just asking everybody else. Should the question be how do we change capitalism with AI? Or just how does capitalism most capacity change as a consequence rather than as a deliberate act?
Speaker A: Does humanity become only the 1% because 99% of people are just not around anymore, their functions have been replaced by AI. There was some connection as we moved into that state, I'm sure. Would this population kind of drop? Kind of keep dropping?
Speaker B: It could. It's like the question of, does conscious capitalism exist? Can you have conscious capitalism by design? So it's either you have it, I don't think you can really change it. I understood your question more so as asking of will people intentionally dismantle capitalism, or will a series of events occur that capitalism falls apart? Is that your question?
Speaker A: I think we may as well just add some features.
Speaker B: Add some what? Add some features to capitalism.
Speaker A: Carbon credits. Let's throw that there. All right. A couple more UBI kind of modules.
Speaker C: It's almost like certainty, right? Like if many jobs get automated away. Our current economy requires you to have a job, to have stock, right? There's now 10% less jobs, right? That's Pitchforks time unless something changes, right. So I think to your premise, I think as AI eliminate, if we believe AI will eliminate jobs, then what you described must happen because there must be a response.
Speaker A: Do we buy the kind of parallel with previous big revolutions that new jobs will emerge and we just don't know what they are like creators? Didn't make sense to me, like two decades ago as a living kind of question. But it's an undeniable kind of current kind of reality that economically people can make money that way. And it wasn't obvious historically. You look at countries that have had a rapid rise in the standard of living. The populations of those countries are typically not very angry. They might want to take over the rest of the world, but it's working well for everyone if there's a lot of abundance. Where the revolution comes is where a few people have most of the wealth and a lot of people are suffering for lack of enough. I think unemployment among has always been kind of like a leading indicator of within America, we have normal corporations, but in agriculture they have cooperatives. There's also B corporations, but they don't seem to be very popular. So maybe it's just sort of a corporate formation to be adjusted too. It's sort of hard to know why people try to start corporations, but they don't seem to do very. That's minutiae. But.
Speaker C: I guess this also touches upon.
Speaker E: Equity across countries in that of the people who will be more likely to be disgruntled. And we say perhaps a similar thing with oil and COP 21 and the developing countries going, well, hang on a minute, you've already had your industrial revolution and profited from it. We want to do the same. And I just wonder how we tackle that issue from an entry perspective also from how that changes the world beside geopolitical.
Speaker A: Very interesting question because currently there are huge barriers for entry, right? Build a model, have the compute, have the energy and then the skill base. So it's really kind of inaccessible a lot of geographies, right? Does that like if we just kind of imagine a long term, very long, far in the future, 100 years, does that remain true? A worldwide maybe like energy abundance is there maybe like hard sales. I wonder if this current barrier to kind of advanced AI over 100 years kind of evens out.
Speaker D: Well, the US purposefully cutting off a certain level of chips to export and making it where the US companies like NvidiA, AMD, et cetera, maintain that lead and therefore hold to the engineers part. Where this gets a little interesting is where China is trying to set up its own education task force that they, they have a lot of people, but they're academics, like kind of institutions, but they kind of have the input to be. So we're not going to hear it seems like kind of creating that environment where there might be, I don't know, maybe at least two players and maybe kind of have two sides.
Speaker A: So you choose the American advanced AI.
Speaker D: Chips of the hundred years or the kind of Chinese version.
Speaker A: Europeans, I think they're going to be players as well.
Speaker D: Or they just choose from American because it's like cheaper.
Speaker A: No, but I think that's the question, right? I think that's the question of if we kind of open our imagination, kind of like making cars was maybe the dominion of one country and then two countries. Now every country is capable of making cars, right? So making AI at least like to a certain degree of sophistication, which again like cars are pretty recover my phone. She.