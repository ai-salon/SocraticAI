Speaker A: Startup called Envision. So what we do is we repurpose Google Glass to basically help people who are blind and low vision to live more independently using computer vision. So people wear our glasses. Usually people are blind and low vision wear them, use it to read text predominantly, and also make video calls directly from those glasses to a friend or a family member. And the glasses, they've been out for the last three years. We've also got people using the app and stuff. So from that perspective, I really believe a lot in the human and AI integration, simply because I see the fact that people wear a repurposed Google Glass that came out like four years ago running not so state of the art AI still become being very useful for people who apply to low vision. So my hope is that as AI tends to get better and we're able to kind of reduce the amount of, I would say, the latency in trying to interact with the AI now. And I think that is going to unlock a lot of independence for people who are probably disabled. That's my perspective, just stemming from the fact that I work on this company.
Speaker B: My name is Lila, I live in the Bay Area, but I am from Norway, and I'm the CEO for a company called Ellipsic Lab. It's a public trading in Europe. What we do is that we have AI versus smart sensor platform that we use AI sensor fusion and ultrasound to enable devices to be easier to use smarter. We're currently deployed in over 500 million devices, and we work. It's a B to B. We work directly with laptop manufacturer, smartphone manufacturer IT devices, and it's all about making things easier to interact with, safer to interact with, and also having devices to help know, for example, detect fall detection, heartbeat detection, breathing detections, and so forth.
Speaker C: Hey, I'm Marcus, I'm from Finland. I live here. I'm a composer working with immersive sound for nervous system regulation.
Speaker D: Hey, everybody, I'm Jesus. I also live here in the Bay. I have a startup which is a journal, but it's kind of an AI enabled journal, and I'm very interested in transhumanism. I've worked sort of in like neuromorphic computing in the past, and that is basically recreating brains with electronics. I personally am very scared of the idea of messing with my brain because it's very fragile and it's kind of like you don't want to mess with it too much and even more scared of imagining someone I love having their brains messed with. So for me, it's like a huge risk and a complicated topic. But that being said, I'm also very terrified of the fact that it's what he said, pretty much. We're guaranteed to live like 80 or 100 years at the moment. That's like every human who's ever been. And I just think that doesn't have to be the case necessarily, although we have taken it for granted or as given for a long time. So this is maybe what I think is, like, one of the most important topics for me personally, and because I would like to live longer than 100 years, maybe not forever, but at least more than 100, is how I personally feel deep down.
Speaker B: How long do you want this for?
Speaker D: I don't know, like 500 to 500,000 years or something in between there.
Speaker E: Yeah. Great to meet everyone. My name is Sash. Previously, I've been a researcher in the CS and kind of neuroscience space, which is kind of why I find this interesting. Following that, I was actually in healthcare for a number of years, did stuff kind of on the brain hall side of things, diabetes side of things, and now I am in. Although I've still spent a lot of time on engineering, on research.
Speaker F: My name is Valnu. I live here. My day job is I'm an executive coach for tech stunt startup CEOs, and I especially work with cutting edge technology, especially biotech and AI research founders. But my passion project is humanity and AI age and world building and the philosophical questions around it. And I wrote a manifesto on it. And one of the pieces of that is involved with transhumanism. My take so far, which may change, but my take is, I feel like AI can help us to become more human and using it to become more human rather than going beyond human. I'm not against transhumanism forever, but I feel like if we do it prematurely, we can downgrade ourselves rather than upgrading ourselves. So I think initially we should try to become more human, explore the consciousness, explore our body, and understand the system fully before we try to go beyond it. That's my take so far, but I'm also interested to be challenged with that idea.
Speaker B: Is lambdish. You can call me Oz. It just ended up morphing into Oz. But I'm working currently on an industrial application with a very deep background in financial technology, as well as some research in a new paradigm research with Laslow Institute. I also have other interests in terms of metaphysics, and I actually have another degree in terms of intuition medicine that deals with more aesthetic matters that I'm originally a quantum physicist. So these questions have been like, what is this whole thing? There are lots of questions that they could not answer in physics. So just really exploring different possibilities, including what we call currently.
Speaker F: To me, it's not different from science.
Speaker B: But what we call is spirituality or metaphysics, which is sort of physics that we cannot completely understand, and our perceptions within intricately or extricately. I like the combination of those. So I'm just really very open minded. And I will actually echo what Banu said in terms of we're an extremely interesting design ourselves and we have a certain understanding of what we are. Well, it definitely isn't matter or anti matter and just dark matter necessarily, but we are something bigger than that, and we just have previous to that throughout our lives. And we may or may not believe what happens when the physiological component dies.
Speaker F: Which we call as dying, or what.
Speaker B: We describe as human life, or even what it means to be a human. What is a human? Could it be like a machine? A very actually amazing machine? So I have those sort of questions, and when it comes to artificial, it makes me giggle because we've been using artificial entities and creating lots of it throughout ages and the collective component of it and so forth. So there's a lot to explore and it could be just an evolution. So I'm looking forward to discussing.
Speaker G: I forgot what I was going to say from listening to all the crazy backgrounds of a lot of Europeans here. Hi, I'm Michael, I'm originally from South Korea. I studied philosophy, mostly continental philosophy, which is the European political names, in college, didn't want to go to law school, ended up working at a bunch of early stage startups as everything but engineering, and worked really enjoyed fundraising as well as doing product management work in bunch of software focused platforms. I just graduated from UC Berkeley, master studying data science. I just felt like that's something that I'll have to go back to school to learn because I was a philosophy major. I can write, but I can't tables basically just graduated looking for a job. So if you know a good PM position, let me know. Human flourishing is something that I had to debate a bunch of times all throughout undergrad for philosophy classes, which is why I didn't want to join all the ethical implication and macro social groups because I've done plenty of those. But I was personally really curious. I was the guy who asked about the individual self knowledge. I always wonder about all the data that Google, Netflix, whoever has on me, and I wish I could collect all of my browser history, Netflix viewing history, and gain more knowledge about myself like I very rarely do, from only very few good friends who've known you for a long time, who could point out and say something that really changes how you think, feel, or how you understand yourself, whatever point in your life. So I've been curious about how do we sort of collect those data as well as if we're thinking much more into the future. What is the best interface for that? Because I already have a metacognitive mind that's like speaking against me. And how do I have this other companion? Whether it's communicating it through brain machine interface or just from a textual basis or like a fake friend, whatever interface it is. However data is analyzed, biosensory or otherwise, it's really curious how I can use this to learn more about my subconscious and behavior and who I am, basically to become you.
Speaker B: I mean, that's the next that you want, the tool so that it becomes.
Speaker G: You tool that eventually becomes me is less of like, that would be nice if I download it and if there's an entity and I can die and that lives on. But I'm more curious then you can.
Speaker B: Look at yourself.
Speaker G: Based on the data set that went into building that thing. But I do believe in the sort of chaotic and probabilistic nature of, I don't like talking about free will, but the momentary randomness of how we can be. So I don't think it'll just be completely digitally with this invites, create what we have, and even physiologically, I think there'll be a randomness there, but more so just about. I'm living right now, and I don't know how long I'm going to live, but there are conversations with therapists, with friends, or data analytics about my own search history that tells me more about who I am. That changes how I think about myself. And those are rare to come by.
Speaker H: Self awareness, right?
Speaker G: Awareness, knowledge, understanding. Oh, you're not feeling well today to. Hey, the past five years, these seem to be the question that's been bugging you. Why don't we look at these?
Speaker H: These are the self limiting beliefs, user.
Speaker G: Cognitive biases, or your attractions or whatever, anything relationship. Like a good friend who knows a lot about you, who can say something passingly. And really, we've all had that conversation. When a friend's like, oh, you're blah, blah, blah, you're doing this again. And it just totally changes how you live for the next couple of years or months or days or weeks.
Speaker H: Yeah, it's amazing. Let's finish the intro, so we'll come back to the. Yeah. Okay.
Speaker E: Nice to meet you, Michael. My name is Paul.
Speaker D: I'm sorry, I'm also from Europe, I'm.
Speaker E: From Switzerland and I have a background in psychology and philosophy as well as an undergrad. And currently I'm pursuing a graduate degree in interaction design at California College of the Arts. So naturally, I'm always interested in interfaces and how we built them, especially when it comes to AI, because it's developing rapidly. How can we build practical interfaces today that will help us have a meaningful relationship with AI?
Speaker G: Also, how can we integrate it?
Speaker E: Transhuman? I think we don't really have a choice whether we're going to merge with AI or not, but I think that's way down the road. But we have to make sure that we get our values straight before we go into the field because that could potentially end up in some kind of dystopian. So I'm not from Europe, I'm actually from Toronto, in the Bay Area for the week. Yeah. So I'm a founder building a kind of health tech AI future. We're building agentic software for healthcare. Essentially, my long term vision is I want to build a companion that sticks with you from the moment you're born to the moment you pass.
Speaker D: Because so much of health is actually this whole idea of sick people come from healthy people. So we should just keep healthy people healthy.
Speaker E: And I feel like if we can fine tune the little things that add up to big consequential illnesses later on in life, we can address them early through this companion.
Speaker D: We'll be able to solve a lot.
Speaker E: Of the challenges today with the healthcare system.
Speaker H: Amazing.
Speaker E: I'm just going to cut them for 1 second.
Speaker D: I know we had talked about breaking this into two, but given how people.
Speaker E: Are just kind of starting to go.
Speaker D: Around, we're just going to go straight.
Speaker H: Sounds great. Okay. I was just going to quickly make sure everyone's info is in there because I didn't start recording. But yes, we'll come back to that.
Speaker B: What you would do.
Speaker H: I didn't get the health companion.
Speaker B: Health companion for elderly people?
Speaker E: No, for birth to death. So the long term vision is from birth to death.
Speaker B: Okay.
Speaker E: But right now we're just focused on oncology and solar organ transplant. So any patient that goes through those journeys, we create companions for them. So it sticks with them through that journey till recovery to help optimize. So we integrate with any wearables that you might use.
Speaker D: But our kind of plays on the.
Speaker E: AI software behind it.
Speaker B: I'm an investor in two different type of designs.
Speaker E: Yeah, we should chat.
Speaker B: It's comp for elderly companions, for sure. And then it's another one, which is a robot that, when kids are sick, so they put robots in school so they can only for that interaction. That company is called no isolation.
Speaker H: Oh, you two should try. This is amazing. Just to finish the circle. And this is recorded. I'm Ming. My background is neuroscience and human computer interaction. So they're interested in the body as the interface, but for our benefit, to make us more human and lowering equity and lowering equity gaps and things like that. And I work for a consumer electronics company where we do work on wearables. I'm really worried about the direction things are going. I also run a group called Neural Tech and Consciousness in the Bay Area.
Speaker B: What's your name of the company? You can't say, I work for Samsung.
Speaker H: I just want to be on record.
Speaker B: You work for Samsung Electronics?
Speaker H: Yeah. So I think the interesting thing is that I'm very much interested in this thread about spirituality and consciousness and how we can. I'm less interested in intelligent or conscious AI. I'm more interested in AI to help us become more conscious. Everything you said there really resonates. How do we become more aware of our patterns and how do we live without the constraints of our socialization and limitations of our culture and everything? Like that huge topic there. So, yeah, I kind of want to pull on that thread, but wondering if other people have other threads they want to pull on.
Speaker C: Maybe to continue on that. I think it's really interesting. How do we train ourselves when we interact with AI? How does it even change our expectation of the human to human interaction? How will it change our use of language, for example?
Speaker H: Or Is the language still going to be relevant? That's a huge question for me, because.
Speaker C: Philosophically, language has been thought as one, like a Wittgenstein limitation of our thinking.
Speaker H: So inefficient, it's linear.
Speaker B: Ed brought up the topic, like, of truth. I mean, so a lot of stuff that I spent a lot of time on, everything we're doing in our platform is like defining the ground truth. And then right now, we're doing some new things, which is like self learning or self tuning, which is like, different techniques to define the ground truth, to sort of train your models to represent the accurate, what we want to represent. I don't know if that is.
Speaker H: Yeah, what kind of truth are we talking?
Speaker B: But this is a different type of truth that we're doing, where truth is like, did you fall or not? Are you breathing still? But when it comes to sort of some of your topics, when we're talking about companion and what is sort of right for you and no offense. So I work with all the big manufacturers like Samsung and all of you know, when back to your point, have people or different companies having all this data, it doesn't take much to skew something. The data just a little bit to left or right.
Speaker F: Exactly.
Speaker B: And you are completely manipulative. And you can do that with whole countries.
Speaker H: And capitalism has incentive for companies to do that, which is the biggest worry.
Speaker B: For me, maybe that this is not what we want to talk about, but how do we define the ground truth or how do we make sure that this model is. It's almost like blending from ethics, but it's not really. But how do we technically even define the ground?
Speaker H: I honestly think it comes down to metrics, right? We have GDP as a really poor metric of country flourishing, and that trickles down all the way to how we fund companies, how we build products. Engagement is really poor, top line metric for anything that we're building for human flourishing. But we have that because all this trickle down incentives, so we don't have, like New Zealand has things like an index for human flourishing, rIght, where they don't call it that, but how many old people are taken care of? How many delivered babies are delivered healthy, that kind of metric. We need that for AI building as well. And we don't have a consortium.
Speaker F: Maybe we will. Yeah, I think beyond that, I think developing new business models and getting out of the engagement model in the short term is the most important, I feel, in this product. It doesn't matter because I work with founders and I have seen this for 15 years. And a lot of people have good intentions. They have the best of intentions. But it's really hard as a founder, not to be influenced by the pressure. Yeah. The competitors, investors. It's really hard for a young founder, especially, to deal with this and then put the intentions out there, because we are blind to ourselves mostly, and we are rational. Everything is a story. That's what I believe.
Speaker H: Yeah, we're blind to ourselves.
Speaker F: And also in terms of. I think it's wonderful that everything is a story. Our brain is a story making machine and that the person that we think we are is a story. And I think we can quote as many data as we want. We're going to still tell stories and we're going to let the machine tell stories. That's how we understand the world and how we create the world. But I think the issue is when you're trying to connect it to a ground truth, like what does that mean in terms of human experience, where we are experiencing the world as a construct, as a story, it's going to be a new form of faith. And if everything is a story, the way I think about it, she tells us stories that works for us. And in terms of feeding the incentive, we have to first figure out what we are trying to achieve. I believe if that's the eight thing like that, there's the fact that transhumanism versus superhumanism, super meaning more human rather than beyond human. And the reason I'm thinking that because there is also stories that we culturally tell ourselves, for example, like negative emotions are bad. Nobody wants to be sad, nobody wants to be angry, nobody wants to frustrate. All my clients want to be at their peak for performance at all times. Never be bothered, never feel failure, never feel rejected. Right when we first start working. And then it takes time for them to really embrace the richness of human experience. And I feel because of our cultural value, if we give that power to people, I think a lot of people would just make those pieces of human experience. But then what does it leave us to be? Like, what do we want to become? Is the important thing. Are we going to try to raise what's more intelligent? Our value is so far we define ourselves as the intelligent animal, homo sapiens. So now we created something potentially is going to become more intelligent than us. So what makes us worthy, like what we want to be? And if we say the only important thing is intelligence, then maybe we're just going to merge with a higher intelligence.
Speaker H: In that sense, without compassion or consciousness, is highly dangerous.
Speaker F: Right?
Speaker H: We see that in human beings. Billionaires who have no compassion, they blow.
Speaker B: Up the world easily.
Speaker F: Again, it's failure. It's a subject of experience versus intelligence.
Speaker D: I totally agree with you.
Speaker E: And I think this is going to be like a big, big trend in the next decade that we build missions, because right now we're always talking about user satisfaction. But how can you make a user happy? And how can you make everything super easy for them?
Speaker H: They give you their data.
Speaker E: Yeah, but you don't want to be happy all the time. You said your clients, they always want to be like, maximum productivity all the fucking time. I'm sorry. And it goes downhill. So I think if we want to build interfaces for these machines so we can flourish, we kind of have to set our goal straight in the beginning of not maximizing productivity, but actually maximizing our psychological well being.
Speaker H: But with capitalism, let us. Right, as an investor, do you have a perspective on this? If one of your portfolio companies, like I'm not maximizing productivity and growth. Are you happy with that?
Speaker B: Well, I'm actually also a CEO for a public company, so I have to deliver to my investors. Feel the pressure, no, I'll tell you, for this company, no isolation. To me, the investment, I did not go in there to make money. And that is the ground truth or impact. It was of course impact. And of course I introduced them to other investors that had a similar mindset. However, of course, the founder of the company feels the pressure that she has to, because she has other investors as you're growing, that she has to provide some return investment. So it is a balance, it's a balancing act, but it's not that the investors expect like 2000% return investment. You know what I mean?
Speaker F: So I think the question that I have over there is like over here, whether that, no, I mean, because yes, there's the system and you need to have the return. Like the whole system is based on that. We live in a capitalistic system and we can think about, for now, I'm curious about what we can change in this system. Are there business models that allow us to create these kind of products? It's not a base of engagement.
Speaker H: We have focused research for innovations and stuff like that. But I do want to bring it back to the chapel to actually bring.
Speaker B: Us a little bit. I would like us to pause for.
Speaker F: A second and take a step back.
Speaker B: To just forget all the assumptions and current paradigms, including the ways in which we've organized ourselves.
Speaker F: So this is a bit of a governance as well as in terms of.
Speaker B: How we value our existence question, because it's just, when we think about human rights, let's just think about human cognitive rights, because it's just a matter of, it's not actually, they already exist, but they're not available to nonprofit or for public purposes. They're only serving and catering to private companies. Large behavioral models do exist. They've existed for a long time, and then they've also had lots of knowledge in terms of non symbolic evolution of humans.
Speaker F: So I think we're just so kind.
Speaker B: Of bogged down in like a little hamster wheel when we just talk about.
Speaker F: Like, when we just say things like.
Speaker B: Business countries, investors, et cetera, we're just really locking ourselves in such little buckets that I think one of the ways which the integration of the better systems collectively we can generate together, let's just call it generative AI or generative collective wisdom instead of intelligence that values, let's just say it values life because it's a very interesting form of organization, whether you're a human or an animal or a plant, and if you just optimize against that.
Speaker F: So if we could just start seeing those sort of possibilities, we can really.
Speaker B: Work towards that instead of feeling completely helpless and powerless.
Speaker F: Because oftentimes we just think that the system is given.
Speaker B: I've got to fit in. Whereas I think the opportunity with AI, AgI, whatever we want to call it, is just a breakdown.
Speaker F: I agree.
Speaker H: It's a shake up. Yes, it's a shake up.
Speaker F: And then, because if we had really.
Speaker B: Human cognitive rights, truly, and then we have human cognitive rights, then we would.
Speaker F: Just have rights to all of our.
Speaker B: Data and our information at all times.
Speaker F: Regardless of how it's been collected, curated, put together, and so forth. And it's also. That's why a matter of distribution, of responsibilities and resources, because every existence is like a resource of a molecule of Earth that has just turned into a life that is called.
Speaker B: Mean.
Speaker H: Here there's some responses.
Speaker D: So this is, to me, a little bit, a question about freedom. And I wonder if you look at it this way as well, because I'm very curious about the question, which is something that you kind of brought up as well. How can we use AI? Not necessarily merging with it or anything like that, but how can we use AI to help us become better ourselves, like superhumans? And that brings up the more interesting question, which is, what does it mean for us to be better? What are our desires? Are they really good for us?
Speaker A: How should we be acting?
Speaker D: Yes, we live in a system, so to speak, that kind of, like sets our lives. But I really wonder if the question is, should we figure out how to use these ais to empower individuals to be more free, as in to be able to have their free will and act it kind of like make it happen in the world, almost. Is that what it means to sort of have an empowered, like a superhuman? Or is that how should we. Agency. Yeah, to increase people's agency.
Speaker H: Agency.
Speaker G: Just want to add one other side to that. All the points were really good about model accuracies, ground truth, or what the objective function is for the model that we want it to do. We can code that in. But that aside, because I don't trust it'll be ever perfect, I think what you brought up is, how can it help us? Which assumes that I know what I want. I want to be more productive this week or in my life, or maybe I want to be more connected, closer to my family, then I need to ask it. And as you said, use the AI to help me do that. I wonder on the other side of that is not an AI that I can ask it what I want or what I think I want, but is there an AI that could tell me what I'm not thinking? And it could be wrong. If I say productivity, you can have a number of friends and it could be right, wrong work for you at that time, not work for you that time. But like friends and families are random, right? They come with their own values. Maybe you're super focused in productivity. Some friend can come and randomly tell you how to be more productive and it really clicks with you. Or someone might come and say like, you should really let go of productivity, you're burning yourself out. And that could also really light your evolve and change your type. So I'm curious about that thing that I don't consciously know I want and also related to that much more of a question of engagement that poses up for a lot of conflict. If I want something specific, you can get it to me. And I learn, I process it, I use it. That's great. But if I'm not thinking about it, or worse, if I don't want to hear it, how I talk too much in a group discussion or something like that, then if it wants to tell me that, if it is maybe useful, a 50% chance that it could really help me at this time, then what is the best way that it would make me open to hear that? Unless we're directly shooting electrical impulses to our head and just changing our mind.
Speaker F: Which we shouldn't do, we should never do.
Speaker G: But as a person, whatever agent format is, I know that notifications are telling me like, watch this, read this. But in what format will I be like, oh yeah, I do want to chat with you, I do want to have some conversation about this, or you propose an interesting idea or something so curious about that.
Speaker E: So that's something I'm actively thinking about. Specifically, it's kind of why I started my venture. It's because in healthcare, you send notifications to take your medication, but no one wants to hear it. No one wants to be like, oh, you're doing this wrong. And so for me, what I found is you begin to trust a companion or a long time, especially as you get being in a metacognitive state and reflecting on yourself is a pretty lonely process, right? And it's really difficult to do so. So I explore techniques around it and journaling. One of the reasons you can't completely.
Speaker D: Explain why it's effective, but one of.
Speaker E: The reasons is you slow down, you have a conversation with yourself, and I.
Speaker D: Feel like the companion isn't going to.
Speaker E: Be a notification that sends you a reminder, but rather, you're going to have routine conversations as a habit of exploring your own metacon. And through those conversations that are informed on the other side that you're actually going to be able to get insights into what's going on. So it's like, almost like journaling, but the journal knows everything about you and can actually write back to you and.
Speaker H: Adapt the conversation style to match your emotional state and things like, yeah, that would be really, I think that falls into the bucket what I call consciousness tech. Right? Like technology to raise your consciousness so that the external systems that we wind up building are more supportive of our core real life self. That would be an ideal.
Speaker F: I find this very tricky because this is what I do on a day to day basis with that function you're talking about. I'm doing, like, a human version of that for my clients. That's my job.
Speaker G: Right.
Speaker F: And there is a great responsibility in that, because there is a power dynamic in a way that what I say is he cannot say something, and I'm just in a human, there's a fallible. They look at me, they understand my limitations. Certain things they trust to me, like, in my area, but maybe not all the other areas. So I have this power that I need to be very careful about. Imagine something that I trust, that has this type of knowledge that I don't have about myself at all, and I build that trust to what it's saying. And then you bring the idea of any type of freedom and functioning that it's like, what is it even like? That there's a choice? Even if it. I'm talking about in the casual meaning, not the real illusion of free will. I don't even know how would you even have the illusion of free will at that point in time?
Speaker D: I don't mean to interrupt, but this sparks, like, a really interesting connection, I think, which is it seems to me like the person should always feel like they're in control and have the choice at the end. And you know what it kind of reminds me of? It's like, basically, I don't know if anyone here plays video games, but in video games, there's, like, a lot of things to do at any time, and they're kind of like preset paths that you can take, kind of like in a game, but also in some way that's kind of like what the world is because if you know something that a founder doesn't know, you tell them, like, okay, if you tried doing this, it's probably going to go like this, and this, and this is what happens. There's like paths, and then the founder can choose, or whoever can choose. Like, I want to take this one, or this one, or this one. This just reminds me of video games, which is interesting. And it's also interesting that games are so addicting. Like, we fall in love with them and the progression and the ability to have that control and progress on the path and also leave them and go to others.
Speaker G: Would you classify yourself as determinist, personal deterministry will?
Speaker E: Because the whole question is.
Speaker G: If you.
Speaker E: Fall into the camp of determinism versus free will, do you fall into the former camp? Doesn't really matter if this AI consciousness is consciously guiding your decisions or unconsciously guiding your decisions. I would argue that so much of our brains are so ununderstood and incomprehensible in the number of inputs and the number of input factors going into your brain from your earliest childhood experiences all the way to kind of the outputs that. That map to the outputs being your actual decision making processes. We don't know what the fuck's happening at all.
Speaker A: Yeah, I think that, for me, is also one of the big reasons why I find it difficult to trust these AI systems is somewhere I feel like with another human being, there is an intuition of thoughts that I develop. Having interacted with humans before. I know if someone's. I probably have a faint sense if someone's trying to manipulate me in a certain direction or being a bad actor. I feel like you have this very unfathomable thing on the other side all of a sudden who you have no intuition about, and you're just interacting with it through voice, through text, and it can keep manipulating itself to sound and convince you of something. And I don't know essentially if that is actually the right way to go about it or not.
Speaker H: Oh, I have a question for you. So maybe to bring it back a little bit closer to our topic. What if it has a body? Right? When you talk about this intuition, how much is embodied, the 90% of nonverbal communicated information that you're gathering. So if this AI has a perfect human body, would that change anything?
Speaker A: No, but I don't think in my case it will, because I still will understand that this is something artificial that is very difficult for me to comprehend intuitively. So for me it's like, okaY, I would not want to maybe have an AI therapist, for example. Or even if I do have one, I'm not going to really invest so much in it, because that investment comes primarily from the fact that these things are human. And since I'm, at least for the time being, human, because you assume now.
Speaker H: It'S based on your experience, humans more trustworthy.
Speaker B: Yeah.
Speaker F: Experience you're born with his product, watches your health all the time. Like you trust. You're born with it. You trust more than you would trust.
Speaker B: Exactly. So I think that we are testing our ways based on the past, so then it automatically goes through. But I had another comment.
Speaker H: I just want to say one more thing. Yes. When we are looking at kids right now researching Jam Alpha and their relationship with kids.
Speaker F: Right.
Speaker H: Their relationship with AI, they automatically trust the agents just as much as. Yeah, so that's just, you say, where are the ODs here?
Speaker G: That's the question that I have. I know I am very primed to anthropomorphizing whatever system this is and thinking it's like a coherent single system, if anything, I'm much more interested in. How do I discount that? And if the model is trying to be not just one type of friend who I know how they're going to react because they're an introvert who thinks about blah, blah, blah, and I know what you're going to say. Instead of that, how can I have a system that supposedly is one system that knows a lot about me, but that can behave like multiple friends and I can engage with, I can discount the trust so I can interact with like, oh, that's an interesting thought, but that doesn't ring with me. Maybe well, in three years. But how do I change myself so I can interact with supposedly a single model that is probabilistically telling me things, but also me, just think of it as a machine or multiple identities.
Speaker F: Interesting.
Speaker H: You had a point.
Speaker B: Because you said at the end you want to be the one that would make final decisions. I would say, even though I'm more advanced and ancient than you are, older than you, basically, I would say, if we get to the system where I believe that the system will make better decisions for me, then I am able to do, because of my mindset and my patterns, my neurological patterns, in my mind, that I'm going into the repetitive. I can't change my habits and I can't even think about the new things. I can't even think the thought of what new things I should do or change. I will make the system, make the final decision when it's more than.
Speaker E: Would you rather be manipulated directly or indirectly? Would you rather be manipulated directly?
Speaker B: I would rather be manipulated directly.
Speaker E: I would argue that any friend to friend relationship or investor to founder relationship, there's a lot of indirect manipulation. People don't respond well to objective assessments from my part.
Speaker G: For example, if I hold a belief.
Speaker E: You should say, pivot to something else. People don't respond very well if you just say, let it fly.
Speaker B: I'm very direct in one thing, in my leader style, and also as an investor, I'm very direct. And if you manipulate me, I will go young. I love confrontation, but healthy woman.
Speaker F: Kicked out of my group.
Speaker E: Also, you said something about anthropomorphizing of machines, and I went to an interaction design conference in Zurich, like the IXD 23, and there was someone from MIt that talked about emotions and how we interact with robots today. With emotions. I think it's very much our generation, like, I would call myself also as like a millennial on the edge, but we tend to interpret a lot of emotions into these machines, and I think that's very dangerous, and it's not something we should do. I think stepping forward, we should be like a machine psychologist that we talk to, but it should be like something that helps us make right decisions in the moment, but it shouldn't be an entity that we talk to.
Speaker D: I would love to hear a bit more on that. I'm just very curious why you believe that it seems so strongly that we shouldn't answer with the machine.
Speaker E: Because I think it's a uniquely human role to play. Like, if I'm your friend, and we both know that we're humans, and we can trust each other based on that. But if you have a machine that you start to trust, that machine can do whatever it wants with you. We're talking about manipulation, which is a real problem.
Speaker H: Human can also.
Speaker G: Politician, yes, but you're limited.
Speaker E: If you're a human, I can imagine you have emotions I can feel with you. But a machine, I don't know what that is.
Speaker D: One thing I will say about this, and I kind of get what you're saying, and I feel it. But on the other hand, also, we humans have, I think, like our own interests, everyone's individual. And I believe that comes from, in some sense, the fact that we die and the fact that we evolve this way throughout natural selection. Those are like my belief system, kind of. So I think because of that, we care a lot about our own self interest. On the other hand, Ais, from what I understand, from studying everything like how LMS work, basically. They're not trained in any sort of a generative way like this. So they're not trained in sort of like a generational way. I mean, so they don't die, they don't have any of this really. They're just trained on examples and they're minimizing some loss function, which is like how correct are their outputs to examples. And these examples are just from the Internet, a bunch of text. So there's no inherent thing about them that makes them want to be self interested, at least in their current iteration. So because of that, I would feel much more like a human would be manipulating me than an AI.
Speaker E: I mean, you're also referring to LMS, though.
Speaker G: LMs are trained obviously on common cons.
Speaker E: Like the general corpus of the Internet. A lot of people right now are going after AGI in a more approach akin to humans, where you think about from conception as a baby, you're just collecting all this input data from your environment. And a lot of people are trying to approach AGI that manner. If you had an AI that was trained the same way we were, I guess by basically collecting a bunch of inputs and learning a bit more conventionally, I would argue that could be even more terrifying.
Speaker F: Yeah, but I think let's assume they are not going to blade us. We just have the help to lay. Right, let's start with that assumption. We still have a problem with treating as humans. And then this is not just with the machines. I have a two year old and she looks at that Teddy bear and she gives quality, she attaches qualities like the anthromophoric fights before that even. It doesn't have to be a talking machine. And now, like that has intelligence and it can respond to you. Of course we're going to do that, first of all. But the problem that I have, especially like relationship building and in the more human form thing, yes. Humans have different agenda. We manipulate each other, we annoy each other, we frustrate each other. It's built in. Right? Like human relationships are sticky, they are not easy, they are wonderful. That's where also the magic happens, the love happens.
Speaker B: Right?
Speaker F: Like you miss them. There's this magic of life that we subjectively experience. The thing with the relationship that is more of a humanized human form, AI. So when it's designed to fit you, it understands you, it gives you everything that you need. If it's drama, it's going to give you the drama, right? Like if it's support, it's going to give you the support because it doesn't have its own agenda. Let's say it just wants to support you, right? And I have that AI husband, let's say, and I have a real husband, right? And then when I go, something happens here and I really want to share with someone, and he's in his own world, he listens, but he doesn't listen. Goes there, and I turn to my AI husband and I'm like doing that, right? It's so much easier to get along with AI husbands and to find that kind of, maybe even the stimulated connection. Do you think I can have that?
Speaker C: There's a risk that our expectations for technology goes up and our expectations for other humans start to go down.
Speaker F: It's already that case. And I want to just complete that thought. My problem is not in the short term, that for us to start abusing these things and then start preferring the AI husband to disclose my deepest feelings, but push it to decades, and then people are already having really shitty social skills at this point. It's so hard to develop friendships, relationships, relationship as it is before AI. And then when we mess up our ability to bond with human beings, because again, they are manipulative, they are frustrating, they're annoying, they reject us, they hurt us. Then if we lose the social fabric of humanity, that connection, then are we even human anymore? What do we define as human?
Speaker G: To your question and to your response, whenever this question of, like, we always paint like AI, we're going to trust it and this wins and that doesn't win, and we pick this and this becomes better. I tend to really trust in human diversity so much and adaptability that there will at least be just like the notification, behavioral nudging thing has taken over. But I grew up in a generation where there was no guardrails on that and I was fully exposed. But I always believe that human beings will. Not everyone, maybe 80% of the population, is just getting glued to the screen more than they should. And we should mourn that, but at an individual level, that there will always be 20%, 10% of more curious selfware, whatever adjectives you want. But those people will realize like, oh, I feel like I'm using too much of this, I don't like this. And then you start to change.
Speaker F: That fabric is already weak. We have a long enough problem in this country. Like 50% of all adults report experience.
Speaker H: Social media already, market already at a macro scale.
Speaker G: I agree with the problem.
Speaker F: The individual I agree with, there's always.
Speaker G: Variation among those individual will be the people who are extroverts connectors who really want to go out there and do this kind of stuff. So we get to enjoy human connection or do what I do or try to do, like call up on a friend and check in with them and have a calendar block out for that. And I think as much as there will be disappointing human experiences, that makes me feel like I want to spend more time with AI. But I think there will be that nagging consciousness. As long as you're aware that it is an AI system, that it's not a real human for a long time. And that will make us either lonelier and shrivel up and go into the four chants of the world or get out.
Speaker F: And also, it's a design thing, right? When I say AI, I want to clarify something. I'm not talking about AI as the technology, because you can also design the AI product to actually enhance human relationship, solve the loneliness epidemic. We're creating more of human interaction and social. Building social skills and all that. It's not like AI as a technology, but the way we use it.
Speaker B: I would say, yeah, presentation. I don't know. I can't remember the exact number. But there was a large research done where people were asked, would they rather give off their phone or their partner of a current relationship? Okay.
Speaker D: The result.
Speaker B: Result was over 60% will give up their partner.
Speaker H: Oh, my God.
Speaker B: It wasn't like if you had an iPhone. If you'd rather get rid of that and get a Samsung. It was like getting rid of your.
Speaker G: Phone forever for the rest of my life.
Speaker F: Or getting rid of your partner.
Speaker G: But I could go get another partner.
Speaker E: No, you cannot. But then you have to.
Speaker B: But it says something about, if you were in relationship with someone, budy, you'd be like, I'm not giving up my phone, but I'll get rid of. I get rid of the partner so we can get an apartment.
Speaker F: Even if it's not a specific partner, if you're with them, you hope that you have, like, a bond that you have.
Speaker B: My point is that maybe this is not a strong correlation between the two, but once again, I'm trying to have my mind very open. Like, if I was a five year old, I think about relationship with an alien.
Speaker F: Yeah.
Speaker B: And I think that you will learn over time if you can trust. It's the same with humans, even with community environment. We're sitting here, but we don't go and talk to random people on the street. But here we just walk in and be like, do you have that short contact? Yeah. And, you know, with human, like, there are certain people you trust. And even if you made these devices, you were saying, like what you can make it, for example, there's certain way your eyes, mouth, everything is. They would trust the person. What?
Speaker E: I would directly disagree with that.
Speaker F: Okay.
Speaker E: Because I think it's exactly because of what you said, that why we cannot trust. And I said that the last time I spoke as well. We tend to answer the one vice we think, oh, you know, it shows.
Speaker B: The most I'm saying today. But I'm talking about in the future.
Speaker E: In the future. But then in the future, your AI could something you cannot understand, you cannot relate to it in any way.
Speaker B: That's what we're going to create here.
Speaker H: Here's a question. If we are able to create AIs that we are compassionate, we can relate to, we train to five year olds. Well, let's not. Let's just say our age, right, when we become parents, our generation struggle was to say, oh, I'm marrying a same sex partner. What if your kid comes home and said, mom, I'm marrying Mary. How would you react to that?
Speaker G: As long as it's not like such a system that you can. My life Mo, one of my many Mos is try everything twice. So I'm totally open and obey with whatever.
Speaker B: Try everything twice.
Speaker H: You can only die once.
Speaker F: One time.
Speaker G: You never know your initial reaction.
Speaker B: Never. You're going to try death twice.
Speaker G: That would be awesome.
Speaker E: This year, the ad companion is like if my son brought home a boyfriend. At least I know that boyfriend is living in his mask, in his body. I don't know where his virtual boyfriend servers are sitting.
Speaker D: It had some sort of a body.
Speaker E: Someone built that robot. Yeah, someone built it. Someone manufactured, some corporation built it with.
Speaker G: Incentives issue around whoever you want to marry.
Speaker H: And I'm sure boyfriend could be an undercover agent.
Speaker E: Yeah, exactly.
Speaker G: I think my concern is for my kid. If the AI is something that is so easily disposable or configurable that I'm not worried about the AI part. I'm more Worried about what happens.
Speaker B: Makes him happy.
Speaker G: As long as it makes you happy, it's fine.
Speaker F: But at one point, if the happiness makes you.
Speaker G: Yes, there's a lot of building 10% randomness, so it does interesting things, too. But I'll be more worried about someone who can't deal with just conflicts not working out or even more troublesome to me. Maybe for the next generation, this will be totally normal. Tree heroes, customize your own bot. Don't like it, reset it, change the parameters. But I want my kids to be able to interact with other humans that you can't do that to.
Speaker D: If you get used to only AIs that cater to every whim, then how are you going to have a relationship with other people who literally.
Speaker G: Exactly. That's what I'm worried about.
Speaker D: I don't know.
Speaker B: It's going to do what is best for you. Meaning that it's going to teach you to be resilient. It's going to teach you to handle conflict. The AI is 100 times smarter. He thinks ten step ahead of you that you haven't even thought about. This is what we're talking about.
Speaker H: But we are okay with the girlfriend.
Speaker E: But the AI is bias as well. The AI will have bias as well because what if. Made by a human?
Speaker A: Yeah.
Speaker E: What if your kid's AI boyfriend started.
Speaker G: Upselling you an ad?
Speaker E: Is that any different from my girlfriend driving home from work relating to me?
Speaker H: Maybe not. And maybe your girlfriend is the IA agent. You still know that.
Speaker D: Yeah.
Speaker C: Would we have the wisdom to prompt in a way that has a medium to long term benefits? Because we might have some short term, like, I want you to help me in this way. And in short term it does that, but we cannot see far enough what it makes us. And then it changes so much of us that we don't even remember who we were when we started.
Speaker G: And do we have AI wealth?
Speaker C: Like, how much can we allow to front ourselves?
Speaker H: This is such a live conversation.
Speaker F: Yeah, that's the issue with the incentive piece of it. You want to say the last of the products supporting.
Speaker H: Okay.
Speaker B: All right.
Speaker E: With interfaces that are more hardware, like physical, to address some of the neurophysiological disorders that exist today, like dementia and all these diseases that you can only treat with prevention. No, but we're able to take noise that come from neurons and turn them into actual meaning. And so people with ALS, they can no longer control their physical bodies, but their brains still work perfectly. And so if you can create that bridge using the AGI, then now you're addressing real world issues.
Speaker F: Define it as going beyond human.
Speaker E: That's going beyond literally.
Speaker D: I would call that person a cyborg.
Speaker E: I think we are already cyborg. We all have them. They're not part of our bodies, but you have to use one of them. If you don't use them, then you're kind of left behind in the white person. As long as we live in a capitalistic system, you will have to adapt to technology if you want to be competitive. And that is why we have to do, because we force each other to kind of like a prisoner.
Speaker H: I'm so curious to actually talk about the body occupantation part. I feel like we didn't talk about that.
Speaker E: Great.
Speaker G: That's how we.