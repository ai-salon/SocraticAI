Speaker A: Wisdom 20. Really interesting combination. You have, like a Buddhist monk, and then you have rid Hoffman together. And so they, they talked about, like, our relationships with AI, actually. Good. One of my colleagues wrote a blog post on LAs wrong saying that kind of in defense of that, saying that, look, like, first of all, it can help people to kind of come out of their shell when they actually can't talk to other people. And maybe it's not like you either have an AI girlfriend or you have a real girlfriend. Maybe you'll have both. And that it's kind of maybe good for your development of your social skills as well, in some ways. And then I think a lot of people are afraid. There was a person in the conference that did a very skating review of replica, which is AI. Well, at this point, AI partner, it's kind of AI friends, but you can.
Speaker B: Pick how you want to interact with it.
Speaker A: Essentially, it quickly offers you to become partners. It's very persistent about it before you pay. And she was kind of, like, criticizing that. I'm curious what the thoughts are around that because I'm somewhat perplexed about it.
Speaker C: I mean, I've seen China obviously already has AI girlfriends and boyfriends that you can hire for, and they already have these services coming out using just AI at this point, and not humans involved in that at all. So I don't know, that seems a bit going into that direction itself, like replacing, as you said, having a human, let's say, partner, and an AI partner as well. And then I think on the tutor side as well, it has a few more benefits. The other day I was talking to this founder she's building, like in the mental health space as well. And her AI helped one of her clients or customers, kids, understand what he should be doing when his bus didn't show up at school and guide him through how he should navigate and go back home by asking him the right questions and then him answering it and going back and forth. So I feel that could be a very good potential use case as well, like helping people navigate how you would want to learn and go through things.
Speaker D: This is about AI relationships, right?
Speaker C: Yes.
Speaker D: So one framing that I had a friend of mine used was trying to break up the different kinds of AI relationships that exist. So one is a relationship with AI, which is kind of what we're talking about right now, but another kind of relationship we can think about is relationships that are through AI. So the example of that is like Tinder. Tinder is you're starting to have relationships through AI, but you can imagine other ones where you email people and it's written by an AI, it's an interface to us. And another kind of relationship we can have with AI, kind of relationship some of us have, which is, it's like something we think about all the time. It's a professional experience. It's like changing our relationship with a lot of aspects of being human. And so these are all different subcomponents of relationships. And maybe we should just stick to talking about, literally a relationship with AI. But I felt that that kind of framed different kinds of relationships was interesting.
Speaker B: I think there's also how AI can help us relate. So there's also not exactly a relationship, but it's using AI to enhance how we actually relate with each other.
Speaker E: I would add also one more thing. How do we use AI to relate to ourselves?
Speaker F: Yeah, there's the nature of the relationship, and then there's like, who's the like? Or there's the nature of the relationship to AI, and then there's the nature of the social relationship as well as there's relationship to yourself. There's like a partnership, there's like a friendship, there's a collegiate relationship. And so I think that's another dimension, too.
Speaker B: So exploring how to dynamics to kind of expand that one, I think we do this kind of like, if you see a scene, who does improv or has done improv, or is it familiar with improv at all? Anyone? So, yes, and yeah, I like this idea of teasing things out. So in terms of thinking through the different types of role and kind of like relationships, one could have. One of the interesting applications, I think, is that concept of role play and in whatever situations, like whether you're going for a new job and you are really nervous, or whether it is you have social anxiety related to something, or you have to go back home and talk to family members. So that's an idea to expand upon this thinking, because that kind of combines relationships with the self, because in any interpersonal relationship, first you have to work through that layer of, like, what is going on with me to then interact externally.
Speaker G: And I think working through the feedback that you get to work on yourself more and push yourself out there as well. That element of AI, I think, would be also very helpful.
Speaker F: I'm also touching on a couple of things that we mentioned. I'm interested in sort of discussing the. I think it's a gray line between sort of that training element or helping element to the point where you are actually having a feeling of a relationship with AI because there's a certain degree of interaction where you're going beyond sort of learning how to function better with other humans to you're having this own relationship with AI.
Speaker B: I'm curious if any of us have had the feeling of having a relationship with an AI, because it feels to me like this is a concept people talk about. It's like when Agi is here, people have a relationship, but it's like one of these things talking about like, oh, Amazon reviews. And if you ask normal people, how many Amazon reviews have you posted in the past week, the number you normally get on average is zero. Right? So I just wonder, to me, the more we look into this, as we develop the product, we kind of start developing this conviction towards it needs to be through AI, forming relationship with a human just because today's tech is just simply not there. Relationship is very shallow. There's many technical aspects we can go into. But I'm just like curious, like IRLs, do you have an actual relationship with your AI? Is this like Pi? Is it a chatty or what brand?
Speaker H: And I think there's different definitions of relationship. Has anyone experimented with speaking to something like character AI or replica or any of the others? And if you have, put your hand.
Speaker B: Up.
Speaker H: And then what's the question? Have you experimented with speaking to character AI or any of these other AI relationship kind of tools? Cool. And then I think chat GT is another one, right? Because you can look at that as a relationship you have with chat GPT when you speak to it, it's not necessarily the same because it's not really trying to be your friend like the others are, but it's still a relationship, right? You kind of speak to it like a human being. And to me that feels like a relationship. So I think that there's a broad definition of what counts as a relationship. There's can it be a true friendship? Can it be a true partner? And then can it just be someone you see something you see as like a relationship with a thing, whether you.
Speaker B: Attach to it, I guess, is where I'm getting at, because there's so much mental health, emotional support, or your actual girlfriend boyfriend discussions. I simply am not seeing that today.
Speaker H: Well, it's happening. It's 100% happening. Maiden's not in this room, but people are absolutely attached to these things.
Speaker B: There's a lot of points. Yeah.
Speaker C: I just want to share in life, you can't share the exact same relationship with everybody, enjoy playing a sport with somebody, you enjoy having a drink with somebody, you enjoy watching a movie with somebody else, but you can't enjoy all the relationship with somebody else. Similarly with AI, it's more like you start to build dependency with another partner.
Speaker G: For a specific purpose. Just as calling up.
Speaker C: Right, your watch. Right. Either you're depending it on your health.
Speaker G: Are you depending it on your.
Speaker C: Keeping you updated over certain things or whatever, and then you continue to build that relationship and you become so dependent.
Speaker G: Like you're having a smart speaker at home.
Speaker C: Keep talking to it and having things, and over time you're just building a relationship where you can't do without it. Like, similar to your phone, I would think it's another app to Google or another persona.
Speaker G: Just like your phone.
Speaker H: Right?
Speaker C: Like with your phone, you have several relationships. Wallet or email or messenger or WhatsApp or whatever. In the future, you'll have more relationships with individuals beyond what you already have on your phone. That's just my procedure.
Speaker D: Hey, real quick note, ironic, because what I'm doing is breaking the full up conversation. But in the name of not doing that, we're going to scrap the break.
Speaker A: And just keep this going till 840.
Speaker D: So at 840, yeah, just come back to the main room and then until.
Speaker A: Then, I mean, if you need a break, take a break.
Speaker D: You're an adult. But I just want to know.
Speaker B: Allow more depth for us to dig into. We're never ending.
Speaker G: Right. Audio.
Speaker B: So we got 35 minutes on the timers. Probably give us a five minute time to come back together.
Speaker A: Yeah. So I think the closest I've come to an emotional relationship. So we've built our business coach, right. First just with ML models, before GPT-3 and a lot of people were like, it's asking me great questions, but does it really understand me? And then GPT-3 came along and we implemented it a bit late, but still before chat GPT. And that really changed the game. And I was surprised for a while. I was like, why does it matter so much that it just paraphrases you and tries to guess what your intent was with what you said? And then one day I'm using our own product and I'm mostly using it for productivity. And then one day I was like, oh, I fucked up this or whatever. And it was like, kind of paraphrasing. The point was like, you're not a bad person, it's fine, everyone fucks up. And I was like, holy shit. This really helped, kind of having someone kind of validating your experience and telling, because I think that's one of the big challenges we have, that we get really tough on ourselves and then any growth stops because you closed up. And in that moment, I didn't really feel like it's. Yeah, but I definitely felt an emotional impact. And sometimes a lot of people ask me about our product, they're like, oh, but can it be emotional? It's AI. And I'm like, well, have you done journaling? And not everyone have, but some people have done journaling. And I'm like, can it be like a diary? I'm like, can it be emotional? They're like, absolutely. I'm like, or have you read a book where it made you emotional? They're like, yeah, it doesn't have to be a life to make you emotional. So that's interesting.
Speaker H: Yeah, I think an interesting risk on that is like, I'm relating it to something that's difficult to talk about sometimes, but like porn, right. And if you look back over history, we've seen porn be a huge issue in terms of people's sexuality and actually realizing they don't want to have sex in real life because they get obsessed with porn and they're addicted. I wonder if having this completely unbiased thing that's always there to give you attention, always there to listen to you and empathize with you without any effort from yourself, you can be a complete asshole to this thing, and it will still be that. It's okay. That's going to probably push some people down that road when they just focus on the fake relationships instead of the real relationships. The same way that sort of having easy access to pawn did for sexual relationships, having easy access to emotional relationships, which we've never had before, could do the exact same thing. Yeah.
Speaker A: And I think to build upon that, there's, I think this big point about alienation, kind of like that. Okay. Cultures like South America, one of the theories is that it's happier because it's very kind of family oriented and you depend on other people. Yeah, I think that's the key point. Versus in a developing country, you can basically live in your apartment and never go out. You can order food there, can work online, you can live without ever seeing a human being. Not in every country you can do that. And so there's kind of like, this dilemma of like, is that leading to kind of alienation and mental health problems if we don't depend on other people? Because then we can be assholes to them because we don't depend on their survival. And there's this question of, I think that there was a study that a lot of CEOs are kind of like sociopaths, and then it's like, okay, is it that sociopaths become CEOs? Or is it that when you're rich, you don't depend on people and then you become kind of an asshole to them? And that's like, to me, that's the core. If I can crack that question or get your perspectives, that would be amazing. Super confused about it.
Speaker H: It's the thing about relationships are supposedly the biggest factor in terms of having a long life. This is from the Harvard study of adult development. And I wonder if actually we think it has to be a human relationship, but then the counterexample could be, what if it isn't actually just feeling? Belonging is the thing that does. It doesn't even need to be with another human being.
Speaker F: That was going to be my question because my gut reaction to it is like, yeah, that sounds really bad, but it's actually such an AI does make somebody less lonely and lives longer and is happier. What's wrong with that? Is there other issues?
Speaker H: You know what I mean?
Speaker B: Well, I think we don't know.
Speaker H: Yeah, exactly.
Speaker B: Because loneliness leads to inflammation, which leads to all kinds of other biological impacts that shorten our life. So I think we don't know exactly.
Speaker H: It's going to be interesting to find out.
Speaker F: My thing is, I think you probably potentially could make a case that it would help people at an individual level. But I think that maybe there could still be societal implications of that.
Speaker H: Right.
Speaker F: That even if you buy into the fact, like, okay, it's good for the individual, I think that there's a potential to make people less tolerant of others and the interactions that they do have.
Speaker H: And that seems potentially dangerous.
Speaker B: Yeah, completely. Like, we're already so bad. Look at the world. We are so bad at really looking and understanding and connecting and relating and managing conflict.
Speaker A: BUt did we get worse or did we get better? Because to me it feels like if we go like 500 years ago, it seems like those people, but maybe that's just my life by judgment.
Speaker B: I feel like in my lifetime it's gotten worse.
Speaker A: Yes, but I think that to me the bigger question is like broader history because I don't know, I think things can EB and flow and it's like people are sometimes negative about polarization and climate change. And then I'm like, well, but we've never been more aware of the problem. So it's like maybe this is a path towards fixing things and we've never.
Speaker B: Been so capable of making an impact. Right. We make so much more of an impact.
Speaker E: Now, what about the topic of empathy? I read a recent article in Atlanta a couple weeks ago, how there's been a collective decline in empathy globally, but specifically in the US. And they traced part of that to technology, and how technology by design is designed to reduce human touch points, right? So now you can get everything at the click of a button. And now if you can talk to someone, an artificial person, through a chat bot, you have this illusion that you have a connection, but in reality, it doesn't translate to real life empathy, right? So we live in these eco chambers, and we have the illusion that we can connect with someone. But in real life, does that make us better humans? Are we able to build better relationships with people in real life? Or does it take away from that?
Speaker B: Be curious to build off of that. If we're thinking about lens of empathy, how do we build empathy naturally, like, in human interactions? And this is like an open question anyone, like, how does empathy develop? How does compassion develop?
Speaker E: How does any of this conversation listening.
Speaker A: Without interrupting, but also probably by example, if your parents were kind to you, then you grow up kind. And that's maybe the flip side of the positive, potentially, that I think our bot is better at empathy than like 90% of people. That's like one thing that GPT four is good at. And we didn't train our own modEl. And I think when you see it speak to you in a pathetic way.
Speaker D: You also learn, I used Pi. I don't know, Pi is like focused on empathy, right? And the way it responded to, like I said, oh, I'm thinking of micro dosing this weekend. Tell me how to do that. And it responded like, wow, microdosing. That's an interesting experience for you to have. Can I ask why are you interested in that?
Speaker B: Are you looking for creative?
Speaker D: You have some kind of mental health issue that you're trying to move, like responding with a question. And also, I can't interrupt it, which is a really annoying part, but it forces for you to listen to. It's kind of interesting, the friction of interacting with another agent, like another person who has certain goals, maybe empathetic goals towards you, but isn't just a servant for you. And I would imagine I haven't worked with used replica or anything, that in the long run, for someone to get the kind of relationship experience that we're talking about, kind of subsuming human relationships with AI, it's not going to be a mere servant. I don't think that's going to satisfy what you want from relationships. So if that's the case, you can imagine trying to build systems that are almost entropy tutors in a way. And then maybe we'll get into the previous conversation of, like, will there be a replacement for human relationships? But empathy might come out of it.
Speaker F: I think the idea of an empathy tutor is really interesting, but I also wonder if it's sufficient, because I was listening to everybody talk about empathy, and it's not just sort of like practicing active listening or seeing different examples. I think it also requires exposure to people who have different perspectives and also, frankly, different personalities, like people who you can't communicate as easily with. You almost need to have the opposite of an empathetic AI in order to learn empathy because you need to practice interacting with those types of to.
Speaker B: I was going to bring up this somewhat controversial sounding point, but I feel like people develop empathy in a deep way by suffering a lot of injustice, pain. That's what I've been noticing in the human society. The most empathetic people. A lot of times it's not the people who are blissfully ignorant, the people who have suffered the most but chose still the sunny side and decide to do that so they can guide people on the better way. And I think the danger of getting always this really kiss up or speak rainbows AI. It really shields people from these much needed practice and things like that, that they need to develop real characters. But I'm really glad you brought. We do have the capability of an AI. Just say beth and please, like, we're divergent perspective, different kinds of people, right? It could be programmed.
Speaker F: They totally do. But are the incentives there to create such an AI because people enjoy interacting with it.
Speaker B: I will buy Apple AI because that's like, we do work with. We are developing skills, the power to deal with divergent perspectives. So if AI helps in some way, I would program it that way for sure. And I think the question about empathy, we don't know enough yet to know biologically and in our brain, right? Like, we know what empathy and what compassion look like in our brain. And when we're feeling empathy, certain parts of our brain light up. And when we see people in a different group that we don't relate with, they don't light up. But we don't know if AI can mimic that, right? There's a lot happening biologically, and our brains are also biological.
Speaker G: And the consciousness, I mean, the consciousness topic is something that we have a lot of discussions around in tech because we don't know how to define a consciousness, but it's part of human interaction in a way that if we allow ourselves to just define language models, to speak on our behalf, we're just not taking care of that consciousness side of our interaction with people, because when you sit in an environment, the air, maybe the tone of the speak, maybe.
Speaker C: All.
Speaker G: The interactions that you have with people around you, create that consciousness that no one else can imagine in their lifetime, maybe. So that's something also we're missing in the element of, yeah, these chats give you text, they talk to you, they sound like a human. But are they really trying to understand? Even if they understand what you're asking, are they trying to relate to you in a level that a human who's sitting in front of you, isn't it.
Speaker B: Like 70 or 90% of all communications body language? Not even so that's such like a brilliant point of just what's missing. It is missing so much. Even if you have another human being sitting on the side over Zoom, when people meet in person, that's even so different, let alone just like text.
Speaker D: Can I put a maybe controversial thing, which is I don't think the world is going to be improved much by being a more empathetic world. I don't actually think that this is a big part of some conversations around empathy, which is like, what kind of actions does empathy point people towards? It's the kind of the photo of the dog, or like, 96% of all money that's spent on helping animals goes to animal shelters, which is not the most pressing animal issue in the world. So I don't actually think that empathy leads us in the right direction, necessarily. And so when we're talking about human relationships and trying to understand each other, one of the things that I'm hopeful that AI can support is actually placing the many other concerns that people different than me. People talk about a universal translator, where it's not translating just languages, but, like, I have a set of cultural experiences and analogies that are how I interact with the world and understand it. And if you can take someone else's lived experience and put it into my language, first of all, I can just understand them better, maybe I can empathize better, but also I can just, as a collective, we can navigate solving the big problems we have to deal with. And empathy is one component of it, but it gets a lot of focus when I think at a systems level, at the level of how large communities interact with each other and solve big problems. It's like, not the most important thing. It was evolved when we were in, like, 50 people collective. So I'm just not as.
Speaker A: Just to build on that really quickly. I think a level even deeper than that is that it's not so much about knowledge and experience of other people. I think it's fundamentally things like self awareness, psychological maturity, and emotional intelligence, where you can take these things that are difficult and kind of process those things and react to them properly and understand how you actually feel. But I agree with you on the.
Speaker C: Point that empathy, I also just want to add.
Speaker H: Right.
Speaker C: Empathy is also context based or relationship based. As an example, a superior and their subordinate or individual with his partner, or a founder and a co founder. It's very different from person to person, or a doctor or doctor and their patient. It could be so different in every specific situation that I'm empathizing how my patient feels, I'm empathizing how my coordinate feels, because I need to give them the space to grow or to learn or whatever. It's so contextual in that specific situation. It can play out so differently in every situation. It's just like mind blowing to learn that and for AI to learn that and be able to reciprocate, it's a long way.
Speaker B: I have a friend who's actually working on those AIs where you have many AIs in this game world, and they would interact with each other along with the players. And based on what we just talked about, it almost seems like now I'm almost 100% convinced we need various different AI agents. That's like, some are more masculine, some are more feminine, some are empathetic, some are just very savage AI avatars. Yeah, we need to have all these. And one interesting corollary I want to bring up is that we do a lot of comparison and tuning of our model. And then I use chat BT versus PI a lot, and I screenshot their differences when Pi. So there's this post about on Reddit, which is totally misinformation, riGht? If you put aluminum foil pack into a ball and polish it, you get like a super cool sphere. You place that into your microwave, it's going to create magic. I put that into Chaji Petit Chai's like, don't do that. That's going to cause some explosion. I put that into pie. Pi is like, ha, is that a joke? I wouldn't try that. But maybe I'm like, the first sentence needs to be, don't do that or.
Speaker D: Stop it right now emoji.
Speaker B: I wouldn't try that. It's like so much later in the paragraph and in the comment section, there's some racist cursing going on because people did that. And they post picture of exploded micro. I'm like, this just shows you can't just have. I was spending a lot of time on time, but now I try to at least use two to compare contrast, and a lot of times the immediate output of the first line or first.
Speaker F: Something is the opposite, I think, to that point. That's why the incentives for the training is so critical, because I would imagine that I'm just making this up because I don't know anybody who's working on an inflection on Pi, but they're probably trying to make an AI that generally you like interacting with and is giving you kind and favorable responses. Chat GPT is a little bit more focused on productivity and knowledge, but in the context of relationships, especially if it's an AI that is commercially geared towards an emotional relationship, like a partnership, the incentive is going to be to develop something that makes somebody feel good about themselves, which may not actually be what the world needs.
Speaker B: What's interesting about the empathy question, I think there's also a difference between empathy and compassion, and that compassion is when we also have an impulse to serve. So, like scientifically, that empathy is when you feel bad or you sympathize, and compassion has this other level, and therefore I will do something in their service component. And so for me, that's the part that will make us be collective and figure out and solve our problems. So I don't know, I feel like for me, that's why I do what I do is because I care. I actually want this to be a better world. I want us stop waging more, but it's because I care. And if I don't have that, I would.
Speaker D: I like that distinction because, for instance, the way I donate money is I schedule it, right? Monthly money is brought out. I see that as a compassionate act. It's not really that empathetic of an act. I'm not at any point during the year being like, oh, my heart, I'm not feeling that symbiosis with another, like, I feel what you're feeling, and therefore I'm acting. And potentially because I actually don't think that that's the best impulse to engage in my compassion. I like to sit and think and be like, how do I want to direct my actions? So compassion seems like something a little more easy to systematize. Like, what are the important things to help, and can we construct something? And that's separate from whether we have a system that is actually like feeling pain because you're feeling pain or something like that, right?
Speaker B: I mean, I do think scientifically, at least what they know so far, it does light up similar parts of the brain. We are feeling the pain incompassion, but then we're choosing it moves into that action.
Speaker D: Sure, maybe in our own system, one is built off the other, but maybe they can be logically separated.
Speaker B: I just think human are motivated by different things, right. Some people are more emotional, so they are persuaded by the pictures they see on social media. But I personally don't think I am above average compassion level. So I do think because I think it's a logical thing to do. And then I need to know, okay, what's my input and output for doing specific things? So in that sense, I do think compassion or empathy do not really work, because my motivation is not driven by my heart, my motivation is more driven by my brain. And then I need to internalize it to something worth doing, and then I do it. Yeah, interesting. And how do you determine if it's worth doing? So I think the bigger picture is more emotional, but when it comes to day to day work, it's more logical. So the bigger picture, yeah. So I think if I choose to focus on certain things, I would think, okay, if I do this, then I would intentionally calculate, how does it improve human output? Maybe it's a number, but I don't know if other people are decided.
Speaker F: I think I get what you're saying. I identify with that sort of more rational approach. The type of volunteering or whatnot that I do is usually either tutoring or educational based, because that's personally what brings me joy versus I don't get as much satisfaction, for instance, volunteering in the soup kitchen or packing up food or whatever. So I'm choosing it. My criteria is more like, it sounds super selfish, but there's only certain dimensions of helping people that bring me joy, and so I will still focus on those.
Speaker B: And it sounds like bring you joy. That seems like a heart. What's wrong with being selfish?
Speaker H: I think this is super interesting conversation, but also let's maybe bring it back to AI.
Speaker F: Yeah, fair enough.
Speaker C: I was just thinking on that. If you break it down, listening to everyone talk about their experiences and how everyone is mapping, how they think about empathy, how they think about AI should react to them, how they think about AI interaction, I think all of it can be fundamentally boiled down towards the environment. For example, even for humans, right, the way we develop empathy, or we do not develop empathy, or we develop the way you manage your risks, or if you. The way you manage, you decide how you'd want to go if you want to go ahead with a particular option or not. The way you decide that or the way you synthesize that decision making process is based on your environment itself, right? For example, if you grow up in a household wherein the parents are more, oh no, I love you, let's go. It's more emotional. You are bound to be having a little more emotional component to your decision making. But then if it's more towards, oh no, I need the exact number of exact output, and this is how you go. That's how you then go ahead and decide in your life. So I feel the most important factor for human AI, communication or relationships going forward, the way it could decide if it's good or bad would be the environment in which the humans interact with the AI and the environment in which the AI has been trained before that interaction specifically.
Speaker F: So just building off of that, it sounds like if you had some sort of a system where you had a mixture of AI and humans of different perspectives, and you can, like, I'm thinking, I'm super technical, product focused here, but how would you measure success? It's like if you start to have more harmonious relationships with actual people as a result of your interactions with these different types of personality and boss, that would be a way to see if it's actually causing human third.
Speaker H: So this is something, this is something I really want to talk about and I'm interested in. I think this is a very meta conversation, and I'm quite interested in tactical things. And as an example, it's like, I would love it if everyone could give an idea in terms of how could we use AI personally today, what could AI do for us that would make us better in our relationships that we have today? And I'm going to give an example. It's like, I would love it if AI just watched all of my social media feeds and told me if something actually important happened in my friend's life so I could reach out at the right time, super tactical AI could do that. That's something I would love AI to do. And I think that can help our human relationships in the real world. It's not me interacting with an AI, it's just the AI sort of alerting me to a fact that's happening in the world that I care about. That's one of my tactics.
Speaker C: This would be a good direction, listing down what positives you would want to have with AI, either with or completely replacing human as an AI with AI, and then what potential negatives we could have. So one positive. Obviously we have build on top of that. Any other options or thought?
Speaker B: I would like to have AI role play different, like divergent perspectives, different points of view, radically different contexts, environments, how we grew up, role play them and then we'll find out if it actually helps. It helps when I'm in this situation and I might have a threat reaction or something. Biologically I might respond to that.
Speaker H: That's great idea.
Speaker D: Anyone else definitely related to that? If we think AI today contains multitudes, right? Like when you talk to it, it reflects some median response, but you can change it pretty easily, right? If you gave a prompt at the beginning to be an asshole and then ask it. So you can get access to this diversity that you're saying. But as you were bringing up, there are many different environments, different contexts, and like a very practical thing in a very silly way happened last night of a friend. We were out trick or treating or.
Speaker A: Like giving candy to children and a.
Speaker D: Friend brought some candy that she was going to give out. I was like, oh, can I give some candy? She's like, absolutely, give some candy. And it was a joy, right? Giving candy to other kids. And I asked my partner, do you want to give candy? And she did not. And the reason why she didn't was she felt that actually her friend who brought the candy had brought candy to give out to kids. And by me asking her, she's never going to say no to me, obviously, but I'm taking something away from her. I was being, in my partner's perspective, rude to this friend by taking away her joy for my own self esteem. And I'm like, that's not what I think she would think at all. I bet she thinks that it would be totally fine and brought it to give joy to me and would love that. But those are like quite different perspectives on that person. And I actually don't know which one is true. And so if this system not only could give me these divergent perspectives randomly, like, here are a million, because that's reflecting the whole world, but could do it like this person in this environment, she's from the Midwest, she probably has some Understanding of who this person probably is. And it's like, these are some things you should be thinking about that would be very.
Speaker H: Imagine that something like constantly recording you and then at the end of the week you have a sit down. You retro.
Speaker F: Yeah.
Speaker H: Remember that situation with the candy? And this is how she probably felt. And that would be so good. You learn so quickly.
Speaker D: Imagine all the times where I don't know if you've heard the phrase, like, radical candor, right? Everyone wants that. Everyone wants to get that, but it's hard to get it from other people because that is a vulnerable or asshole position. But a lot of people want to improve.
Speaker H: And so, yes, that's a great idea.
Speaker C: We'Ve gotten, I think. Good. Two, three good ideas. Let's look at the negatives.
Speaker B: Radical candor. This is not related to AI, but I think AI can help. I read this through the book, Radical honesty, whatever. I tried at our team meeting, for once, fucked up everything. So I had to backpedal and then just realize, you know what? Hey, you can't just do that. And I think that also applies to right now noticing a lot of cat GBT rappers for dating. So you can take a screenshot of, like, the girl said this or the guy said this, and then it will output, like, multiple possible responses. I tried out. That's horrible. Oh, my God.
Speaker F: Yes.
Speaker C: Tragedy does not know how to date.
Speaker D: I bet Pi would be better.
Speaker H: I think this is on your question. I think this is my exact.
Speaker B: But then is it so me?
Speaker H: Well, that's my exact problem with it. Right? So imagine my idea around the AI telling you to reach out your idea when you screenshot, and it gives you the chat up line to say. Then things go amazingly. You become best friends, and you become boyfriend, girlfriend, girlfriend, girlfriend, whatever. You show up and then you're now in person and actually, is the relationship now inauthentic because you haven't yourself put the effort in to do that stuff. And I think that's going to be one of the biggest questions in the next couple of years, because more and more, this is going to be augmented with, like, a copilot. And then what happens to authenticity.
Speaker F: My friend was joking, but also was kind of serious about having not just the AI on your behalf, but if you imagine a world where AIs are dating on your behalf, and the AIs are just talking to each other to see if you're compatible. And I was like, this sounds pretty bad, but I suppose it's one way of doing things.
Speaker C: Another thing would be, I think it.
Speaker B: Goes back to your point about the consciousness. Like, what are we learning in the process of doing something? Or like, if we're actually with a divergent perspective and really we're having a reaction, what happens as we get through the reaction and we deal with the difference? It's different than having this non threatening thing. Tell me, do this in that situation.
Speaker H: And do that as a genuine kind of person. If you knew that your person you started dating when you were messaging them on with dating apps, didn't come up with their responses themselves. How would you feel like after the.
Speaker D: Fact, at this point, if I was on a dating app and they didn't.
Speaker H: Use Chat GPT, I would be like.
Speaker D: What are you doing?
Speaker C: This wasn't developed by Chad GPT.
Speaker B: So we're trusting on trust. But this is so toxic. Is it so unhealthy? Because then what's the difference between the ten years ago you read in the news people are catfishing each other on MSNN or something. Like two people actually. You're 50, but you're playing your 15 year old niece, and then you're right.
Speaker F: But then again, it's the disparity that matters, right? It's the catfishing aspect. It's not so much using Chad GPT in and of itself. It's that if the real life version is a very different. And that's why I think that the line to sort of disambiguate good from bad here is more around like, are you using AI as a tool for learning and becoming better in some way, or using it as a replacement to do the things you need to be doing?
Speaker H: Mind you on this, if that happened, we messaged and it was all chat to see, but then we dated for like three, four, five days.
Speaker F: Now.
Speaker H: I really like them. Yeah. It's not because of what they said on, it's because of who they were in person. So it is there. I think I still feel a bit weird about it, though.
Speaker B: If they haven't used chatgbt messaging in the first place, you wouldn't even see.
Speaker H: Yeah, exactly.
Speaker D: What do you think about? So we're recording these kind of conversations and we're able to, because of AI, convert the ephemera of human relationships, human experience, into something concrete. Like, wow, all of these things were said and project that out a few years, you're not using AI. There's just AI that's like data at a higher level is able to be converted into something useful for you. Like, this is who I am. I don't have to think about it and describe myself and kind of come up with a story. If AI were brokering relationships between people, it's a little bit better than just coming up with six pictures on Tinder or something. Instead, it's like, well, I've been watching this guy for the last ten years and I've been watching you for ten years. I think you both would be really great together. That doesn't seem horrible to me. Actually, that seems good.
Speaker H: Did you see that Black Mirror episode when it was. They would match the perfect relationship?
Speaker F: There's like a time. There's a time of three.
Speaker H: Feels like that.
Speaker C: So how do you think about, let's say, privacy in that case, telling the AI agent or just AI? This is who I am end to end. Right. Like ten years span of time that AI knows everything about you. Better than you, than you know yourself. How would you think about privacy at that point?
Speaker D: Right now, I feel like corporations do this around me for their benefit. I'm excited for this is. I mean, I have many risks about. I don't want to come off as an over optimist here, but one excitement is for that kind of recording to become my power. It's an agent for me that I can use.
Speaker H: That's a great.
Speaker F: Yeah. The privacy isn't the concerning part for me, the concerning part is like the lack of self determination involved in it. And it feels very like if we truly get to the point of the Black Mirror episode, I don't know. Hopefully I'm not referencing something that only people must have seen, but it really does get to that point. It was really scary because there's no room for spontaneity and self determination. And it feels like, icky to me, and I don't even have words to describe it.
Speaker B: I know I haven't seen that. But I also wonder about the dynamism with which we change and grow as humans. Right. Like, something happens from one day to the next and your life changes and you're different. And it's not just the data of the fact of what happened, it's that you are changed. So if it has all the data and it's telling me who I am, my concern would be that, yeah, but we're not. And it may look like, yeah, that's the pattern. There are patterns.
Speaker C: I don't know.
Speaker B: It would be more that free will dynamism of change. That's debatable.
Speaker F: Does it take in a new information appropriately? Right.
Speaker B: We're not isolated things. We're permeable and interacting and dynamic. Everyone's AI. Interjecting everyone else's AI.
Speaker D: Before you, about five minutes, y'all, you had used the word trust.
Speaker B: Trust. Yeah, I think it's an interesting one. Yeah. Does AI build trust, or does it break trust? Or is that not binary? How does it build? Or how might it build and how might it break it? Because trust is foundation for everything. Humans. We don't trust someone to gain them over.
Speaker H: It'd be interesting. When you message Pi, like, is my girlfriend cheating on me? Yes.
Speaker F: Pi would be like, no.
Speaker H: Why do you think?
Speaker B: Building off that, it's like, how is trust built in real life? And this is just like brainstorming, how do we build trust in real life with people? Well, so my training program does that, so I won't say too much.
Speaker H: I think just being vulnerable.
Speaker C: Yeah.
Speaker B: I mean, there are specific skills and there are specific behaviors.
Speaker F: According to research, doing what you say. Oh, sorry, didn't mean to. Doing what you say you're going to do.
Speaker C: Multiple times.
Speaker F: Yes.
Speaker B: Vulnerability, consistency integrity.
Speaker C: Vulnerability.
Speaker D: It's kind of amazing how much people trusted chat GBT right off the bat without a repeated experience, because clearly there are heuristics we use as well, which is like, oh, you're talking to me in this kind of friendly tone. I asked you one question and you responded to me how, for some reason, maybe with Chanciti, because it's less threatening than another human. I don't know. It seemed like people built, especially younger people, build trust very quickly and rely on it. And then there's this whole hallucination thing. You can't trust it too much, but people do. They trust it really fast.
Speaker G: It all goes back to how well your compass is established, that when you're listening to someone telling you something, how well you can trust yourself, that you wouldn't get fooled by something that you don't believe in or you don't agree with that. So I think a lot of people, I myself started using CADGPt to build my company right now to the level that I can use it for different backend or front end development. But in the end, I don't allow it to tell me how to design my system, but I will allow it to help me with the tasks that I want, versus if I just go to it and say, just build this for me. Build a layout, tell me how to do things. I would come out as, like, someone that doesn't know what he's doing, because real life experience, what we experience outside, is something that the chat GPT system doesn't know, and that's what you need to distinguish from that. So people who trusted that were like, okay, I can tell you where I can see this system being helpful versus where I can just not care about it.
Speaker H: I use it similarly. I'm a solo founder, got an amazing team, but solo founder. And so I use it to challenge my own thinking, right? I'll put my strategy or what I'm thinking, I'll be like, this is situations I might do about it. Tell me what's bad about this or tell me what's good about this. And I use it as this brainstorming, challenging thing and it's amazing.
Speaker C: For that, you should try pie as well. I've done that.
Speaker H: Yeah. But I feel like it's going to tell me it's okay.
Speaker B: It's okay.
Speaker H: Whatever you decide.
Speaker C: If you'd be stern with the language, don't give me one thing. Just give me nothing malleable. Give me something stern on your side. I need to do either this or this or this. Tell me what I should pick and how rather than telling it to pick for you, tell it to tell you how to pick.
Speaker D: We're going to be coming together like a minute.
Speaker H: Yeah.
Speaker D: And so I'm going to move over there. But if you all here would like to share kind of final reflections with each other.
Speaker H: Thank you.
Speaker B: Thank you.
Speaker D: Thank you.
Speaker B: Top of mind for anyone. No right answers, no wrong answers. Just what's top of mind?
Speaker H: It think for me, I'm just most excited about how AI can make our relationships better. Also building that space. Hashtag Pally.com. Pally.com. Literally what I described.
Speaker F: Pally.
Speaker H: Yeah. So it watches your social media and tells you when to reach out to people.
Speaker B: It'd be great if we had something.
Speaker C: He's planting a startup's idea in you, Pally.
Speaker H: How do you spell like Palely.com? P-A-L-L-Y.
Speaker B: Thanks for being a conversation.
Speaker H: Thank you so much.
Speaker B: Great to be with you all. We've got excitement about how it can help our relationships. Anyone else have? Thank you for giving. Cheers.
Speaker C: AI to be trained a long way. We are trying the long way to come meet our criteria.
Speaker F: I really liked our conversation about having AI as different personalities of AI to challenge you and also sort of measure the success of that as sort of like your efficacy or not efficacy, but the extent to which you're more harmonious interacting with actual humans.
Speaker B: I like that. Okay.
Speaker H: It'S about to have a new website because we've kept it slightly, but everyone sign up.
Speaker E: The topic of empathy and how AI can potentially help us be more empathetic and how that translates into real life interactions and relationship building.
Speaker B: Yeah, my mind got really stuck on that one too, especially when it came up in the whole, like, what are the good parts of empathy and what are the bad parts of empathy? Going back to the rune as an empathy quadrant of prodigal candor thing where it's like, it is good, but then at what point I am an emotional person. And it took me like 30 some years to realize that. Emotional filter, then the rational filter, but yeah, like empathy, then how it can be ruinous at times. I think that's going to really stick with me. How do we use AI to essentially help us make maybe the best choice rather than just the choice that we feel or that we think whether you're more of a cognitive or emotional processor is best. Anyone else? How can AI help us stay connected after this one discussion? Hashtag leading it? I already took a screenshot.
Speaker C: You're already doing AI job by summarizing.
Speaker G: All of this conversation.
Speaker F: You should do like, you know, the new IOs feature that has that sort of thing. If you have a cool integration with that where you kind of put everybody's phone in all at once and then shoot a text message to everybody, that'd be pretty dope.
Speaker C: Feature noted in the red. Privacy, right?
Speaker A: Yeah.
Speaker H: Thanks everyone. Good chat.
Speaker C: Thank you.
Speaker B: Good job.
Speaker A: Thank you for your contribution.
Speaker F: Right, are there that you think there's.
Speaker C: More people to connect with? You guys.
Speaker B: Share your hashtag.
Speaker C: Yeah, I just can't.
Speaker B: Can we find each other there? We can do it. If you remember tons of people in the new version, totally down.