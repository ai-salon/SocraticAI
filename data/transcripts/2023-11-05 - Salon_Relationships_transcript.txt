Speaker A: In both, like, social relationships, but also romantic relationships. I think relationships is very broad. I mean, like, all of society is built on relationships. And it's interesting because the first way that compelling AI is manifesting in our life is as a conversation partner. And that's, like, the essence of every relationship is really people you talk with, discuss ideas with, share information, so forth. And so it's interesting that we're, like, kind of personify them right away, and I wonder how much we can become liable to become attached to those things or let them influence us. So it's very different if it's like, this AI system is just like, it spits out numbers, right? That's not really what a person does, but instead, we deal with them in a person format, and we like to anthropomorphomorphize stuff anyway. It's like, if I draw a sad face on a rock, you look at the rock, you think the rock is sad, right? So given a computer that talks to you in beautiful language, it's like, okay, I think this is a person. Yeah. I think the big question in my mind is, like, are people optimistic, or are they pessimistic on where things are going to go in the near term future? Right. And overall, where are people the most optimistic things that this could really help us with, navigate relationships or form better communities, and then also things that this is going to be a threat to us having healthy human lives. So I guess that's the two big things in my mind. This way, maybe here, move your chair in a little bit. And then we are all in the same circle. Hey, guys, I'm Haas. I'm from London, and I'm super interested in how AI can augment relationships. So the relationships we have between one another, whether that's dating or friendship or work or anything. And I guess where the line gets drawn for when that relationship becomes inauthentic because of the use of AI versus it just helping you have that relationship in an authentic way, but still supporting it to replacing the relationship altogether. So I guess that's one stream, and then the other is just purely AI. Friendships and relationships and dating and these characters that you can just build attachment to. And what's good about that? What are the risks about that? I think that's an interesting topic as well. Yeah, that's me. Hey, I'm Ralph. I am particularly interested in how AI can provide emotional support, because I feel like we kind of all heard statistics about the lack of therapeutic support for both individuals, couples, groups, due to just lack of people wanting to be a therapist, or the economic conditions that cause therapy to be so expensive. And I think there are a lot of negative examples of AI ads for therapeutic purposes. But I'm wondering, what are the different directions that it can grow in the next 1020 years? Cool. Josh. I think I just started thinking more about the relationship side, actually, my most recent relationship interrupts. Right. Let's add to the beginning of our intro, just like where you're coming from in the world. So we kind of know like, I'm a developer, I am a teacher, I'm whatever. So just add that in. I come from a couple of blocks away. Nice. My background, kind of all over the place, data science for a while I had a startup, then I did some blockchain stuff, and now I'm back in the space. But I'm most interested in longevity mortality as it relates to AI, a subset of that as relationships. And then in particular, I myself am going through a lot on the relationship side of it to try to figure out. I think authenticity is a great way to comfort if I'm being authentic. So what I was saying is, most recently in relationship, we were having a conversation about, would you co parent with an LLM? Because you can't provide all the solutions to an answer. At what point did the child rely on the LLMs? That caused some issues. And then just recently I was sending personal emails as thank yous, and I was like, I think this language could be tighter. And then I use DVT for it and I changed it, and I felt like, I almost felt like a fraud doing it. And so there's like, where do I draw the line of my improvements? I also think that we need to revise the way the training tests work and a bunch of other questions. But I have a nebulous approach to the topic. Hi, I'm Joel. I've been an AI researcher for a while, 1012 years. And the topic is really close to my heart because I've been working more kind of like fundamental research for a while and wanted to kind of do something more socially impactful. And I think that one of the big problems in the world is kind of the degradation of community and relationship. And adults have less friends than they used to. And there's a strange paradox where we get so much more stuff, so much more as possible materially, and yet we struggle find meaning. I think relationships are just one of the absolute core parts of human life. So the pessimistic side of me is that AI could just be an accelerant for those existing trends that you see, things like replica and just the dynamics of capitalism giving us these really convenient intimate partners, either friends or romantic relationships where you just get exactly what you want as opposed to maybe what you need or what actually will help us come together as a society. But on the positive front, I do think that it's possible that AI could help make relationships more convenient. There's something just intrinsically uncomfortable sometimes about intimacy, but if you imagine we're playing cards with a couple of friends, but there's this AI that is just sometimes just interjecting things, it's a perfect host to subtly, over time, get you become more emotionally open or something like that, or something like that. A flavor of that. You're on a date and there's something just helping you to kind of like lean in a little bit more. That could be a little bit naive, but I just think it's really interesting to think of the positive possibilities as well.
Speaker B: Hi, I'm Molina. Yeah, it was really nice to hear about positive sides because I was actually trying to focus on that too. My background is mostly medical diagnostic laboratories. I'm binance. But in relation to AI relationships, I was thinking, how could it help us actually understand our partners better and develop more organic, full relationships where we can work to build it better way and yeah, how can it be executed and what we need for that? Hi everyone, I'm ridula. I do not have a tech background. I'm really a humanities professor and I teach rhetoric and communication ethics. So it's the communication ethics part of me that's deeply interested in this. One of the things that I personally believe, but is also part of the discipline that I teach, is that being relational is really what defines us as humans. And I think my concern has to do with something that's already been named and that is the propensity for AI to deliver what is perfect because actual human beings never are. And there's something about learning to adjust to somebody else's imperfections that I think makes us really human. And so that's what I'm concerned about. I've also had the experience of the inauthenticity. A colleague mentioned that he used Chachi BT to write recommendation letters. And I had something in me had a violent reaction. But then, you know, it's not like he just sent it off. I mean, he read it, he inputted the information and it turned out a letter that I'm assuming he felt good enough to stand by. So I'm just in this question space and I have to admit more on the dystopian side, but I feel like that's because I'm not intimate with the technology, so I really don't know its potential. I'm happy to learn.
Speaker A: Yeah, I'm Eric from Sweden, eternal optimist, generally so heavily biased towards optimism. But I think we can integrate AI to make a much better society and relationships and all those things. Particularly I like all the topics about. I prompted chat GPT to get extra discussion topics and all of this is good, basically, but I think I'm particularly interested in the short term apps use cases, which, for example, like a moderator. Right now we're using the transcribe AI, but like an AI moderator, someone from different culture can translate it to another culture dating. They're translating what the person is saying. Work wise, I'm building a new kind of computing experience, which is kind of like a super app like X, but also integrated with futuristic features, and thinking a lot about how AI can make this map out how we coherently see the world, map out a world of our own reasoning and our beliefs, et cetera. So particularly interesting how AI can make sense of the world for each of us, and then as groups as well.
Speaker B: Hi, I'm Jen, I'm a potty designer. I've worked in legal tech and AI for the past couple of years, so very new to this AI and relationship space. I think I'm interested primarily in parasocial relationships, so I haven't really given that much thought to it yet. But it would be interesting to kind of start thinking about what effect AI might have.
Speaker A: And also in general, parasocial relationships. What's the definition in your mind?
Speaker B: So how I define it is like these one sided relationships where you have some semblance of a relationship, or you feel you have a connection to somebody who doesn't know you. So it would be like with celebrities, I'm seeing a rise in YouTubers, or people have these sort of interactive, have these interactions with influencers on OnlyFans, for example. So I think that's interesting. Hi, I'm Wenchi. I've been in tech for over a decade. I'm a data scientist by trade. I've been thinking about this more from the social. So Vivek, the former surgeon general, I think he wrote this whole book about the loneliness epidemic and how a lot of older Americans don't have close friends. And I think during COVID as well, I think what they observed was the lack of, not your deep connections, but a lot of the shallower stuff, even like saying hi to someone at the grocery store, that was actually very important to, well, being some curious, like, how this will pan out. Do we become more isolated if we're talking to a bot? Does that Actually give us the same fulfillment as talking to other people? And then even with other technologies, some other AI conferences, they were talking about, like, self driving cars. Right. That's great for society as a whole. But are you losing some sort of interaction if there's no person driving the car?
Speaker A: No. I'm Curtis. I'm a software developer. Kind of bounced around different industries. I think the thing that really sticks out to me about AI is just its potential to change the world in kind of an unexpected way. And the way I would kind of frame this is, I grew up with the Internet. I don't remember a time before it, but when I think about my parents and just people older than me, I don't think they really expected the way that the Internet would profoundly change their life. I think they would have thought, oh, on the optimistic side, it would supercharge research and kind of help bring people together. But on the other end of it, it's kind of enfeeble people. You have a worse attention span. It's harder to just kind of filter through stuff. There's more disinformation. And I feel like as we go through with AI, we're going to see these same effects. And there may be really positive things, like being able to see different perspectives and interact with kind of models of people that you would never see in your own existence. And the other kind of maybe pessimistic view is like, oh, it could kind of destroy human relationships as we know them in a very profound, negative way.
Speaker B: I'm Dina. I am a data analyst and have a deep interest in systems design and community and relationship design and biology. And when I think about that aspect of design, of community design and human interaction, it is super relational and is super embodied. So I always am like, embodiment and AI, what does that mean for the systems we're creating and then kind of on the opposite side of it? But what does the functionality like, the very literal functionality and structure of AI, how does that limit the system and constrain the system in which we can have relationships in AI, with AI, and then how that comes back to actual human relationships? And then on the scale from optimism and pessimism, I'm probably somewhere right in the middle, going back to that embodiment piece, how can AI recognize emotion and then be able to feed back against that emotion the way we do when we're with people, but then there's like neurodivergent people who can't do that anyways. And how does the recognition of emotion kind of be encoded? Can you capture that accurately? Is the thing I did for.
Speaker A: Jackson ML Engineer. One kind of AI ML thing I did that was related to relationships was like a few years ago, worked for a sexual wellness startup, but did AI ML stuff for them. But yeah, interested in the topic because I used to have a friend, it's kind of interesting guy, but him and his girlfriend are trying to build like a dating app that's sort of like AI heavy data heavy like how people fill out in depth personality tests and then also maybe use LLMs to figure out people's compatibility by their textual data. So talk to him about that, it's kind of interesting. Also I'm curious about speculation about farther future, kind of like singularity type stuff, brain computer interfaces. If we start to merge with AI, what a relationship like then. Hi everyone, I'm Fabio, I'm from Italy. As you can maybe hear my accent and my background is kind of mixed. I started a software engineer, but then I shifted to management consulting. And also that I'm working B two B enterprise us. I'm very generally optimistic person, but when we talk about relationship and technology, there is something like that. I see a little problem here and especially what happened with the social media in the last ten years. And also because I value a lot in person relationship, but also authentic relationship. And seeing that this technology is so accessible, that can become very addictive and we are going to abuse of it. So it's very hard, I think as a human being to don't use it when we need to be real Somehow. And yeah, that's it. So I'm generally positive, but there is something that we should think about. I'm Tony. My background, well, right now I'm working at an AI startup, but my background is in education. I've taught through middle school, high school students and college students. I'm really interested in a lot of what people have been talking about today. But what was on my mind, the top of my mind coming into this conversation was particularly how AI will influence developing humans like these students. All this time spending with young people, especially the impact social media has had on them. And that was like our first really broad integration of AI into humans lives. And we see what happened and now I think like, oh shit, we gave us sticks and stones AI and we've done this. What happens when we give a nuclear AI easily distributed to relationships especially as people are learning to have relationships. Nice. Okay, cool. There's a lot of interesting things to dig into here. I think there's kind of, I noticed sort of a clustering along maybe one axis here between what I'd call authenticity, or ideas about authenticity in terms of can AI models satisfy human desires and does it mislead us? And so forth, versus maybe what I'd call instrumentalism, which is recognizing they're obviously useful for helping us with tons of stuff. Right. And it's like we want to use them for this useful stuff. We're concerned, if we make them available everywhere, are they going to disrupt or threaten this concept of authenticity in human relationships? I think this is kind of like something that's kind of emerged from what people have said so far. So I want to start us just with a question that's very open, and anyone can chime in any order, whatever. And I think it's this. If your robot looks like the Dalek from Dr. Who. Dr. Who, that's right. Yeah, totally. It's obviously a robot, right? It's obviously a robot. But if your robot looks like a person android human thing, then it's easy to get this authenticity mixed up. So I think it's like in this question of how do we get the benefits of this thing without maybe misleading people emotionally or so forth, or whether that's even good or bad, or could it help us if they were authentic? I think that really is the question. It's like, should AI models and our interactions with them be dressed up as human interactions with things that look like humans, or should it be obviously something that's not a human? Right. This is actually an explicit point made by OpenAI, is that we called it chat GPT, because it's just not a person's name, right. Versus Siri and Alexa and Claude and stuff. It's like, easy to get attachment. I'm curious if anyone has thoughts on this. It's like, oh, they should be human, like for this reason, or that's bad for another reason. Kind of open for. Is it fair to say you're saying, like, tools out of the spectrum versus an authentic human that interacts with. Definitely. That's kind of one way of teasing apart these perspectives. But I'm curious about your personal opinion, where it's like, me, as a user, I want this kind of experience for these reasons. Do you want it to have a person's name? Do you want it to remind you every other sentence that I'm a lover's lens model? I think you brought it up to. Even when you were talking about this in my other face in Iraq, I brought up an example last week where to your partners, to the person who wrote the letter of recommendation. I wrote a thank you to someone. And at first I felt very inauthentic. Then I was like, yeah, but it has a lot of my texture. And so I started to definitely transpose my emotions on top of the model and the output. And so even though it was entirely a facade, I felt very connected, especially GBT. It seems like it very quickly goes from what is a tool to the moment. It comes for your personal characteristics. It can become like an empathetic agent versus. If I'm just like, yeah, create me a spreadsheet and help me understand blockchain D versus blockchain C. And I think it's the moment you start to have, for me, at least, it's when I had my personal language that was tied to it versus just like a thought that I was probing it with. That's where I kind of crossed the line between two, which is very narcissistic. I guess that's really the question here. It's like, do you want your tools to be empathetic? What does that mean? I think in terms of how it looks, I don't know if I'm in the minority. It feels like it. I kind of want it to be like a human. I think that there's a lot more powerful stuff that comes out of it. If it looks and feels like us. So many different scenarios when that matters in the real world. And so I think that will be. I do think there'll be two types of AI, right? There's AI on your device, and you won't see anything. But I think in the real world, it'll be cool to have something that is genuinely human like. It should have a distinguishing feature, human like. I think it being personified is an inevitability. If it's talking to me, people are going to name it. I talked to a lot of users, and us being an AI application, it's so much shocking how many people. Yes. I want to call me by my name, and I give it like a special nickname, and it refers to itself by that nickname, even if it's like a business tool. So this is a really strong drive, I think, for humans to just give names to things, whether or not it's a sentient thing. Right. We used to have, like, god of thunder and stuff, right? Just because we didn't know how thunder works. So we have one additional newcomer we all introduced ourselves a little background. I mean, I know you already, obviously, but background kind of where you're coming from. And then in the context of AI and relationships, what kind of things beyond your mind as questions to talk about. And so far people have expressed a lot of ideas around places these can be useful, but then also concerns around authenticity of the relationships people form and how that's misleading. And does it lead to a social media 2.0 of people having negative experience as a result of a new technology? Hi, I'm Joshua. Sorry for being late. I came from a class. Yeah, the background in computer engineering, currently with a master's in engineering management. Generally more interested in hardware aspects, but AI is a significant topic and participate here in the. Yeah. Regarding relationship with AI, I'm actually currently watching this Apple TV series Foundation, the robot that is serving Empire. I don't know how many know the story, but I found that actually quite an interesting relationship since the robot is certainly an artificial intelligence and how it's kind of supporting, providing, guiding information, collecting information, assessing them. But the ultimate control is still kind of with empire. But I found that quite a symbiotic relationship. But I feel like that's certainly something that I wanted to go. Two other aspects where there's this high AI tool, kind of like chatty. The Pi One is a bit less known, but it's very much focused on an emotional support and kind of. I had a few instances, not too often, but kind of where I was asking this situation, can you guide me here? What is kind of an approach to maybe take a conversation? So found that I think this one movie, Interstellar, was it right where they had this walking robot, the blocky thing. And what I found interesting as another aspect was kind of the percentage how truthful the AI was. So they kind of actually turned it a bit down because it was kind of the context, but I think was creating too much anxiety or was not beneficial for the overall situation. So I guess something along these sort of relationships I would want to have a. I move to and form a supportive, authentic relationship for the greater good of myself, I guess just that. Yeah. So just to tie back to what we're talking about, let's ask you directly, would you prefer to have this AI companion or robot like in the interseler movie? It's very clearly not human. It's a weird blocky thing. Right. Would you prefer to have a human form factor or obviously machine embodiment or representation? If we have that manifested in a physical form, sure. Or even just as a conversation partner, where it's like speaking in a human like way versus a non human like way. I guess the mascal version, just this insanely advanced where. Yeah, well, the Empire also has Internet relationship with the robot. I guess if that reaches that point of sophistication of AI, I think a human form seems reasonable. I would be open to that. Or maybe prefer it, I think, in earlier stages, maybe to keep it separate so that it is. I guess this block robot seemed heavy. The block robot? Yeah. Then. I don't know. Also, the physicality wasn't super utilitarian, so that was a bit strange. I guess the other opposite is maybe a help without the probe, where you have just the steam. I guess for a spaceship, maybe that's appropriate. You have a companion. I guess you want to feel forward. And I think an humanoid form seems to be good. I guess I have to give it.
Speaker B: I was going to say, when you talk to a help desk support, I think it's. When it's not clear, are you upset if you realize it's like an LLM rock talking to you and not like a customer support agent, how people feel? Because I think one thing, when you're, like, masquerading.
Speaker A: I think on that note too, as well, if it is something that is like, yeah. Either reversive to you or maybe get reversed to mass genocide, I think if we have any AGI that has a human space and it's talking about launching nukes versus looking more like the robot interstellar, it's going to be much easier to develop less empathy towards one. Is that a risk to having empathy for so, one thing, we think they have empathy, but are you saying you don't want to have empathy for them? Actually, there was this awesome talk. I don't know if we can reference this part of your rules, but ten or 15 years ago, MIT Media left, did this like the director did a talk at Aspen about developing empathy for AI. And what they did is they formed a relationship like it was with Alexa or with Siri or something, the equivalent, and the child developed an emotional bond towards it. And then over time, they turned the bot off. And so all of a sudden, it seemed like it didn't have emotions and the child would cry. And the whole point of them trying to say is like, robots don't have empathy back then, whatever they call a robot. And then the point that we give them empathy is like the point of no return. And their also assumption on that is like that empathy can only be between two humans or between maybe two organisms. Or something like that. So I think the tougher question is not to open it up too much, but how would you define empathy? I used to have a ton of empathy towards my dogs, but obviously, they did get the same amount of empathy.
Speaker B: I think thinking of empathy as your example of hell, there, all it is is the voice, right? So it doesn't look like a human, but it sounds very human, like, a little bit creepy. And I think the moment of apprehending that technology as close to human is when you start to turn it off and it starts to protest and say, are you really turning me off? Are you really turning me off? For a moment or two in the film, I remember when watching it, being invited into that experience of losing your life, right, what it might be like to lose consciousness. But at the same time, I think the movie was set up in a way that, as a viewer, you very much identified with the humans in there, and you identify with their, in the moment, adversarial relationship with this technology, where it's kind of like, manipulating them, and so he has to turn it off, and so you end up on the side of the human. But I'm just recalling that brief moment where the so called humanity in hell was apparent, and that's when it began to show some fear. So I think it's possible. But to answer your question, I think it's neat when tools like this show personality, because as a user who doesn't really know what goes into the development of it, it's always a moment of delight. Oh, that thing responds a certain way, and it talks a certain way, and it's got these affectations that make it sound human. So I find myself saying yes, because I think the experience would be more pleasurable. But I would want a certain caveat, always right, to know that I'm engaging with a non human, even if it's.
Speaker A: And what are people's thoughts on the risks of having something that doesn't have that caveat, that has a compelling moment of delight? It's a humanlike experience and stuff, but it will never tell you I'm a robot. It'll actually just keep responding to your text messages or something. Any thoughts on that? No. See how I can go wrong? I mean, ultimately, if we get a Halloween thousand something where it's serving its own ends. Has anyone here read the book Superintelligence? Yeah. There's, like, a story in there where he talks about the potential for a rogue artificial, super intelligent system to just go online and start chatting with someone who has, like, a wet lab and says, oh, I need you to just size A, B or C. And the idea of building up to how a super powerful non embodied AI could destroy humanity, being completely unembodied. And this idea, if these systems like how chat Chief T will constantly tell you it is not a human, if these systems don't do that, the potential for downstream chaos and negative impacts. But I think the counter or one of the other questions around this topic I think about a lot, is what happens if instead of it becoming like this evil Terminator robot, what happens if it does devalue like sentience and it becomes like a humanesque level intelligence where we think about giving it rights, humans are already really bad at giving rights to other humans. What about another? You were saying the moment we empathize with how 9000 is, whenever it's about to die, what happens when that's not just how 9000, but our iPhones whenever we try and turn it off.
Speaker B: The question of form feels super important here because for a lot of reasons, because if there's maybe a bad example, but an AI cop form is going to be very different than an AI sex bot or the form matters a lot. And also, what is the point of giving it a form feels super, why would we be embodying AI? And then the other piece of that is the digitation or virtualization of relationships in general in social media, where we can kind of easily forget that there are people on the other side of it and like pro culture just sort of immediately developing because it's like, that's not a real person, this is just something online. So there's like, I guess two parts. The form feels like it matters a lot, and what we're doing there and then already AI right now isn't embodied. And if there is sentience or whatever, what does that mean for things like rights or like. Yeah, there's just a lot of things.
Speaker A: I feel like actually the embodiment is now just reflecting on what I said, the embodiment is the critical factor. So I feel like actually chat GPT is kind of just an intermediate version of AI because it's just this chat that's kind of the quickest fix, how we can put the features into one interface. I think in the end we want to have it more specific to a specific application. I guess. For example, if I for to have a spaceship, a car, or some other control center where we wanted to have it more, a bit less human, I guess, because we don't need to interact physically with it, we're just going to get into the car or in the future on a spaceship like with HaL. And we just want to have a voice interface basically with this technical object that works. And the human language seems to be then the easiest, I guess the combination of something visual. But if I would have, for example, a household robot that should be much more optimized for human interaction. So if you have guests over that should have, can be able to read all those nuances and maybe, I don't know, let's say bring water or refill the cup right at the right moment because it reads that micro expression or these are all kind of not features that a car would need. And that manifestation, I think should be certainly increased human form or similar to human form, but it's still guided by the practicality. I guess it's a design question at some point, maybe you want to have it so human looking that it's kind of not awkward to be around it, but maybe still so much robot looking that it's not being mistaken by someone who, let's say that tech savvy. So I guess overall more human, but with other things, certainly the human aspects or these micro things, these fine ones are not so important. So should be kind of maybe more technical. But I think the guiding factor will be then the embodiment of AI into some form that is then surrounding that intelligence. Nice pause here for a second. We have a newcomer welcome.
Speaker B: So sorry for being here.
Speaker A: That's okay. So we all went around. Oh, yeah. Also it has to go to the washroom. We're getting a drink. They're just go ahead anytime. So we all kind of just introduced ourselves briefly, kind of like a couple of sentences on our background. Just like, to understand where I'm coming from and the world of ideas. And then also a couple of questions that you were really interested about, like to talk about in the context of AI and dating and relationships. Relationships very broadly, too, just as social relationships, friendships, collaboration, all kinds of stuff. And that stool has a handle that will make it go higher up so you don't feel like you're sitting behind.
Speaker B: So I'm Ishita. Ishita Jazz. I have an AI startup, and, like, a quick story about the startup. We started with building something else, but then I think in January, in June, we want to pivot and build Samantha from the film herd just for ourselves. And in trying to build Samantha, we built a lot of in house tech. So we fine tuned our own models to be conversational, build a prototype, build some context compression, context management, because she's posted memory so now we have APIs for people building companionship, apps, personal system, anything with a human. Anything that needs to have a human conversation. So that's my introduction. I have a background in data science, product finance, and now I'm doing this, planning to step up as a company. Some of the questions I worry about or think about, right. I think a people talk a lot about regulation. I feel like in building. I feel like when Internet was invented, if it was regulated, would the world be what it is today? So in terms of AI, I think relationships specifically would be really important. And alignment has to be in human values. So one of the things I'm really curious about is what are the values like if you don't see AI relationship of any sort?
Speaker A: Okay, well, that's interesting. Yeah. I feel like we've kind of come to this point where take off, where Joshua left off, is that it seems like the degree to which these things have a human like interface. Because this is the question we're talking about, right. It's like they can be very useful, but there's this missing authenticity. There's this risk of misleading people emotionally, of getting into compromising situations. And it really seems to depend on how like they are to being humans. It's one thing having some server room that just does air traffic control. It's like no one's going to get attached to that emotionally, right? Maybe the lonely radar operator. But where if it's like a person that looks realistic and is like making small talk and stuff very easy to now have our emotions get assigned to that. So it seems like the extent to which we want these systems to interface with the world of humans places demands on them to have a human form factor and a humanlike method of engaging with people. And for people to be placed in positions of responsibility or trust, we typically like to have some assurances of their competencies, whether it's a driver's license or medical license. I'm curious, people here, if you were to think, suppose we can make compelling lifelike robot humans, right? That could be good conversation partners. What would you like to see as assurances so that you would feel comfortable or safe to have them in your environment? And basically, how good do they have to be for them to be more accepted as social agents and not just abstractions of mathematics or something? You know what I mean? Yeah. One assurance I would want to have would be to understand their incentives.
Speaker B: Exactly.
Speaker A: If you had a conversational partner, it's suddenly trying to teach you to buy some more stuff on Amazon or whatever human mind is susceptible to influence and just having a clear bill of like, this is what the system is designed to do and I want to him. Or if it's like the term is like a fiduciary relationship or something where its interests are my interest and maybe I pay upfront for that or something. Do you get that with all of your human relationships? Yeah, exactly. Because I started to bring up an anecdote again, but I had an ex and we lived together very close physically, and she moved and all she does is travel, but we talk. I don't ever see her anymore, but she's literally like a bot in my mind. I don't even know what her incentives are for talking to me and I refer to her for life advice. Even when I felt like when I wrote that thank you note, actually I kind of felt like I was cheating on her because normally I would send it to her, but I was kind of, well, she's not in my life, so I guess. To what degree would you have to note? Is it that you can probe something for the first time and you'll get an exact answer of what an incentive is? And that's how you would define like, okay, I can now work with you. Yeah, I think there's like different levels of analysis, but I think my greatest fear is more that when you have an agent that's in everybody's living room, that the emergent effects of perverse incentive affecting a billion people could be really unpredictable. Like coming back to social media or something, Facebook as a whole or something, and what that does to society. There's like good things and there's also bad things. And so it's true on an individual level. We never sure of anyone's incentives that we're talking about. Maybe we know people more deeply. Maybe we have better sense of trust where we align. We don't. It would be at least nice to be able to know what the thing is affecting a billion people. What is it designed to do? It seems like a part of the accountability that we'd like to ask the public upon that. It's like basically a piece of public infrastructure at that point. Like you'd like Alexa to be able to say, what's your incentive to get in like PLI Five language? Well, realistically, if you pull into the Amazon ecosystem, they have you buy products. Yeah, I do get very nervous. That's where marketing jargon can come into play. Like you could get some long disclaimer that would loosely.
Speaker B: Access to information and AI. What does it know about me and everything else? Where I'm talking to a human, it's like squishy, and they forget things. And there's a level of chaos in that reality where I'm not like, this person knows everything. This person has all of this knowledge about my buying history and about X, Y and Z to then actually be able to manipulate me. People are manipulative, but every internal action isn't necessarily like this person knows everything. And so that feels different. And that's why incentive feels like it matters more. Because in any given moment, my incentive with one other person can be one thing and then be very different the next day. Whereas there's a persistence in AI that.
Speaker A: Feels different, kind of, kind of expansion. On this question, is there a way to make the incentives not for corporate interests? Because who are the people funding this? Those with corporate interests? And it may just be a pessimistic attitude, but I think it's more about how do we as individuals evolve to counteract these use cases, rather than like, oh, we should just trust anthropics constitutional AI, that'll be okay. They have other reasons for developing this other than pure interest. Is there like an open source decentralized model anywhere? Like Llama Two created by Facebook? But it is open source. There's a fair amount of open source models out there. I think opening ice model of capping profits is really interesting in terms of long term incentives, where they become non profit at some point and become a public good in some sense. While, yeah, most other companies, like Amazon, whatever, would just infinitely maximize profits. I also wanted to answer the other question before, even in long term, where we would have robots that look exactly like humans and we would have neural implants that basically have AI for us, like helping reasoning or total reasoning, there would still be a difference between a robot that is looking like a human and a human that has a neural implant. There would still be the scale of the investment. You can replicate the robots much more while the human is much harder to breed. I think even in that case, you want to have a disclosure basically forever. You want to have a disclosure that it's a robot. I feel like honestly, the disclosure might kind of have to come from the other direction in that most of my relationships, and I say this in the most general sense, like work relationships, just strangers, like cashier. Most of those start online. And I wonder if it'll just be easier if part of that relationship and the starting of that is almost proving that you're human and giving and imbuing like this human with a level of trust, or maybe just like maybe you just trust them because they're maybe less.
Speaker B: Competent than the AI.
Speaker A: They don't have the capacity to trick you at the same level, and they don't have the same kind of unknown incentives. Because I think if you rely on the AI to tell you its incentives, the first bad actor just strips that out of the AI or tries to suppress that and you're back to square one. I think, yeah, that might be the very kind of first step of any relationship, and it might almost bring human to human interactions more in the forefront or more into the foreground, because in order to really trust somebody, you actually have to meet them in person. And I imagine we are a long way away from the embodiment to the degree where there's like a physical avatar that a person cannot distinguish between. I like the word use trust. I think trust has a lot to do with having a good world model of someone else in your head, like your friend, that you probably really understand what they would say in different situations. How would they act in different things. And that's what trust kind of comes down to. It's like given there was a conflict of interest or position of vulnerability, I reasonably expect this person to act in my best interest, right? That's kind of like an operational definition of trust. And so, so long as we don't have insight into these things, how they work, it's kind of like you don't know what it's going to do, right? Is it going to order me that thing from Amazon? Or is it going to order some gain of function research fucking bacteria to be produced or something, and then that gets delivered to my house? Yeah.
Speaker B: Another aspect of this is replicability, or how generic something is, even if it has a human form versus how unique it is. So even if the source technology is the same, can it then manifest as multiple different personalities? I think the reason that's interesting is because, for instance, one of the ways we understand relationships as ethical choices is the framework that's associated with philosophy. Martin Buber, who distinguished between I it relationships and I thou relationships and I iT relationships are primarily instrumental, right? So it could be a cashier. And if all you're doing in that moment is completing a transaction, then it doesn't matter if it's cashier A or cashier B. You just want a smooth transaction. IV. Relationships are where the partner is irreplaceable because of their unique attributes. And so it would be, for instance, in your case, is it comparable? Because on the one hand you're using Chachi Bt which is sort of generic versus the ex is a very particular person. And so while she might have been replaceable in that instance, because you're getting the same kind of advice, doesn't really take over the actual person. And the influence that she had continues to leave faces off in your life. And so I think that's when we grapple with a different ethical question. These things can be disguised, but I think another level of deception is when they start to become so customizable that you develop this kind of bond with them, where you start to see them as unique. And I don't know whether that means good or bad, but it takes us.
Speaker A: One. We can definitely get more supplies, tools. But, like, with the cashier, if you see this same cashier every day for 100 days straight, at some point you're going to have a point of empathy, right? Yes.
Speaker B: So then it transitions more if an.
Speaker A: Eye Dow realistically, it's a tough thing for us because at some point, you're just viewing humans as tools, the same way that we're talking about this. And you can't have relationships. You can't trust everyone. You just don't have the emotional bandwidth. Right. Because I guess that there was this homeless guy that was always by us as well. We used to live together, and he was always there. At some point. When he was gone, we were just like, oh, my God, what happened? And you develop all these narratives in your head, but there was, like, this trust for this what was personal, very negative relationship. Never really had a role as a tool, but then became like an emotional attachment. Again, I don't even know if I care if it has a proof of humanness. To your point, it sounds really selfish, but if I can have a very quick connection and just trust that there's, like, it'll be consistently there, it's almost like it's more important to me than because humans are fleeing, right? Like, we're short, we're gone. Relationships come and go versus something that has a certain sense of permanence. And so I think that's another way of asking, how much do I care about this tool? Well, this tool is always going to be there. I care about it more than I care about a lot of relationship.
Speaker B: There's something interesting about the source of AI and what you were kind of talking about, because it's kind of this question of, is it one or is it a multiplicity? And you kind of had that interaction with everybody you meet, too. Like, are you one thing or are you many things? And it's like. But we have words like too faced to kind of describe, oh, you're not the same thing. Like, you are lying to me in some way. And that is the erosion of trust to an extent. So if AI is one thing from their one source and then shows you different faces, that feels like a different thing than every version of the thing being unique and separate from the thing. So it just feels like a one versus multiplicity, both of the AI, but then also really in human relationships, too. And I don't trust people who have actually one way with me. And then very, very different in other places been like, yo, fuck that.
Speaker A: I think that's, like, a popular fear, too. It's like that there's this humanistic veneer on some kind of thing that is a complete psychopath. Or, like, the AI, right? It's, like, absolutely a monstrous alien intelligence that has no regard for human stuff. And it's just, like, been trained to have this nice little mask on top. I want to pick up on a point that was brought up just now, because I think it's interesting where we want people to be reliably performant in some role. Like, I go to in and out burger, and I want the same service or whatever every time. And so it's nice to abstract away from that as a person doing that. But at the same time, it turns out those interactions at the bank teller or something are a part of civil society, and that we are increasingly lonely in life, and that even this was apparent even in the 1840s, when Karl Marx wrote about the alienation of the self through the commodification of social relationships. And I wonder now we live in a fully commodified society where a lot of my relationships with other people as agents or representatives of other organizations are in the form of commodity exchanges or financial transactions. And is that really a world we want to live in, where it's like, oh, I don't like AI because, I don't know, they're too personable or something. Or I want people to be more like tools, because we have now this thing, we have this interesting landscape where sometimes for convenience, I want people to act like tools. When I'm trying to cancel my New York Times subscription on the phone, I don't want them to ask me how my day is going, right? Because I know it's fake. I know it's part of a sales pitch. At the same, why? The extent to which we can depersonalize people and dehumanize them is the extent to which we feel justified treating them poorly. Because if you truly empathize with them. You would not yell at them about how New York Times sucks or something. Yeah. I'm curious what people think about this, whether it's like, should we reel back our tooling of people as it is anyway? And is there something we can learn from that on? How we should make tools more human? Or do we think this is an opportunity? If we make AI all of those jobs and things, tasks to do that are, frankly, tasks don't need a human interaction, all AI does that then mean then the relationships become way more important that aren't those conversations. So when you're not calling a call center and you know it's an AI, it's quickly done, whatever, you're probably not even on the phone anymore doing that, it means that the people you do interact with, you are going to put more effort into that relationship because it isn't transactional in nature, it's purely relationship driven, which is, as we've said a couple of times, like, the core of what humanity is. Maybe it can have a positive effect in that respect. I love that. That's great. Yeah. We can unlearn this thing where I abstract away people's personhood for instrumental purposes, because I know, I quite like this thought of, like, if we really take this to the extreme of the future of AI, and let's assume that we have AGI and it can do basically everyone's job better than we can ever do, and you get to this world, there's so much resource in the world because we don't need to work, because there's just enough for everybody to go around or whatever. What do humans end up doing? People argue, people will just find new work to do. I think that's one way to view it. I kind of view it as like, maybe we were never meant to work in the first place, and all life should be is hanging out, building relationships, or doing stuff that you find entertaining. You don't need to do it because you earn money from it, because it's a job, but you just live life. You have community, you go to bed, you wake up, you eat, you hang out. And maybe that's a future where all we have is relationships, because everything else is just taken care of. Human being, not human working. Yes, I do think it's like a tension of our era that we evolve for a world that's much different than what exists. And the capitalistic way that we abstract people doesn't suit us relationally. And there's this question of how do we reimagine society in the future and in this future world, it could be that relationships are enough, it could be that there's some deep parts of nature that wants to achieve and to do as well, and how we manage to satisfy that need. Maybe the same way we embedded football or something to kind of sublimate our aggressive impulses, we'll have something that's like the folk part of work, of the football of work is my mind immediately went to what I would want to.
Speaker B: Do if that happened.
Speaker A: I'd want to build and I'd want to continue stuff, because I find a lot of interesting problem solving. And then I see my kids, my friends who raise their children, who are like putting little labels together to close an airplane, and they could do it in like 2 seconds. And they watched their kids do it over the course of like 3 hours. And then as long they applaud them. And I just imagine that's kind of a situation where we built something or whatever you push like trendship and development, it writes it better. Like, look what you did. Although there's some the future you talked about, that's one fair amount of people think could be possible. Maybe there's another potential future that still has building. You could take it as like a darker or more positive, but I guess you could think that maybe there still would be conflict or kind of thing, meaningful work to be done by humans. I guess maybe the kind of future where brain computer interfaces, we become part AI. So it's not like maybe there's a lot of ways that might not work, but if that happens, then we wouldn't be as sort of subservient to AGI become part of it. I guess the question is, but then what would we do? Obviously, probably having relationships and hanging out like leisure, I think is if you didn't have any, if there's no work you need to do just to survive, then I would think, yeah, leisure all the time is good. Although it could be things about once you become part AgI, maybe you're exploring the cosmos and then there could be future conflict between, I don't know, you're this cyborg spaceship thing that's know, flying around, there's other people, there's other beings like that. You can imagine a future where we are still building things. And it's not just like the kid Lego thing.
Speaker B: Does it feel like relationships are the work or is that not?
Speaker A: Yeah, it could be both. But still, I guess if you're like, maybe one kind of frontier for us to do after AGI is explore the rest of the universe, I guess that wouldn't be related to relationships, but I think you, you would still like, you know, want, like to have. Well, those are interesting questions. Like if like we start to actually change what it's like to be human, will we still want romantic relationships? I forget who wrote it, but there's like a SCi-Fi short story about in the future where there's some kind of a kid who's like half cyborg and he's able to change his sexual preferences manually, pick what he wants him to be, and then he goes down like a rabbit hole. They end up getting super strange. He's turned off by inanimate objects. Because Once you start the kind of morals, once you start changing your preferences, if you can change what you actually desire, then you'll desire new things. And then once that new being does new desires, you might want to modify your desires in a way that you initially wouldn't have wanted. So that's also maybe worry like a hedonic stairstepper rather than a hedonic treadmill. We're always trying to take his next step up, but we're staying the same place. Or we morph into something totally different. Maybe it's possible that we'd morph into something that maybe doesn't want companionship, which could be bad. But this is also, I think that type of scenario is what called the Star Trek scenario, where it's like all of a sudden we have infinite wealth. I mean, you go to explore the universe. I'm really interested in the in between now and then with this conversation around. Yeah, we're too far. Well, no, I mean in particular in the next 20 to 50 years in terms of how AI replaces the means of production, because already from the last 20 or so years, we already see a decline in relationships. And a big part of that is like more automation, more standardization. And whereas my dad, he knew the grocer, he knew the people in the mailbox, there was a much more sense of community through local businesses and who these people were. My generation, there's not that I don't know the people at Walmart or Kroger nearly as well. And I think that's like a step further of a lot of us. Like the reason I'm here is that I work in AI and I'm in San Francisco for my work and I'm meeting all you all kind of through work. And I have somewhat of a social network already. I think about the 14, 1516 year old going to a world where they don't have as built in personal interaction through work. And they have really easily accessible discord servers, Twitters, Instagrams that are already kind of pushing out a human relationship. But then that's not just a discord server. You have Obama talking to you. If you want, you can have whoever you want in those conversations. Like, would we even be able to focus on just human community in a world where AI is that advanced to take out production? I think thinking about the kids is a really interesting thing because I relate it to. This is something I discussed with some friends the other day, which is like a sometimes sensitive topic to discuss. But how we saw in our generation, internet porn change how kids then perceive sex growing up. Because when you have this thing that's instantly accessible to you and you can have anything in the world and you start to build these expectations that are completely detached from reality, you grow up. It's caused all these problems we've seen in society. Now, if you have kids who don't have to form real relationships at school because they can go online and web before they were on Xbox or Discord or whatever, and it's still kind of a real relationship even if it's. But it's still with another human being even if it's online instead of face to face. That has caused some issues itself, but fine, now you've got a fully fledged emotional support person thing, their AI that can manipulate you, understand you better than anyone, better than yourself. Of course you're going to build attachments to that thing and then now you have this at ease. Are you going to just give up on real life relationships? Why am I even going to bother, right? Because I've got this 24/7 on tap. It knows me better than anyone else can. Maybe kids then go too far down that road and it causes even more issues. I think the counter to that is people saying the AI can help train the kid in the real life relationship. Fine, lovely, if that can work. But I think the reality is it will be a bit of both. So I think that's a big risk.
Speaker B: So I have a point there. I think to your point, you talked about how somehow all our relationships, human relationships, have sort of come transactional. So thinking of that one point that I know, just thinking about LLMS right now, I don't know if you guys have heard that when we promPting, if you're polite, then chat, GCP would give better answers. So that's one point. Another thing that we've seen is when people are using LLMs and the LLMs are trained to be kinder and more empathetic, then people talk more. Just thinking of a very flipped situation where we're just talking about text right now. But this would expand into so many other mediums. Maybe alums would teach us to not think of humans as more transactional. Because here, imagine if you're trying to just tell Chatty, just get me this, and it's responding in a certain way. You're hoping for a certain result, but you would get a better result if you're more human almost, or if you're nicer. It's a potential that AI actually teaches us to not think of humans as being transactional and teaches us to be more human.
Speaker A: On that note, how are you seeing culture? I think the way we've been talking a lot about, do we give empathy to AgI?
Speaker B: I guess to that point, the best way to do that, I felt, is if we gave them background story. So if you give the LRM itself a background, I mean, if you give the way you're building it a background story, then it's harder for people to sort of change the way they use it and for it to have one personality. So if you kind of imagine, let's say there are ten LLMs in the future, right, because there are so many. I mean, they're not going to all be there ten years later. Let's say eventually there are ten or 100 LLMs, and they all have XYZ personality, like you know it before you use it that way. You know what their intention is, you know what their personality is.
Speaker A: Like, developing a story, like a moral system, and then designating it towards different LLMs.
Speaker B: You could give them different personalities. Literally. What we did is there's something called a principles you test. And we took that test thinking, if, let's say the LLAm is a human, what is the personality you want the alum to have? And took a principles you test, it gives you archetypes, and it gives you all of that. And we use that as background story for the alumnum. And we thought we realized that it performs much better, it hallucinates less, it digresses less, all of those things. And people who are talking better, I mean, people who are talking longer and having better conversations. Can you have empathy and morality without being embodied? Do you think you're coding in a backstory? But does that kind of guess at emotion? I don't know. Right now these are black boxes, right? Boxes. I don't know how the space will shape up. I don't know how much you'd be able to tell about that. But right now you're able to do all those things with prompting. But again, this is getting technical, but I think maybe taking away from the conversation we were having, but right now you can give a certain amount of input to elements, which is context, right? And I kind of think about it like if I talk to human, if I'm having a human interaction with you, for example, we would start with probably exchanging names, what do we do? And then ten minutes later we would be talking about, what's your favorite hangout spot? What's your favorite musician? And then probably if we meet again, the next time we'd be talking about, oh, I was just dating this person and he said this and whatever, right? And the next time we'd be talking about, let's say we meet one month later, we'd be talking about, oh, I'm doing this new thing, it'll be completely different. Right? So, but the elements have a limited input, right. How do you, or like even AI, right. I think the biggest thing to sort of, if you want to give them the story, the biggest thing is how do you get them to remember the inputs the way sort of humans remember it, right? Because suppose we spoke for 30 minutes, you don't remember the exact words used. You have feelings from that, you have beliefs from that. You might remember certain sentences used from that, if that makes sense. Well, identity feels kind of important. Not just the input, but inherent identity feels super important to that experience because even if the words are changing, they're still like, exactly what did you get from that? Well, the technical part feels super, super relevant because it creates the constraints. You were like, oh, I'm taking away from it, but I feel like it's super relevant.
Speaker A: To steel, man. That's the current constraints. Within less than a year, we went from 1000 context to 16,000 context to 100,000 context. And I am not technical more than I read a lot of papers, but I don't have a very deep, deep understanding. But I would just assume at least for the next 20 OD years, it's not going to slow down. I've heard more than once the announcement on Monday could have some major upgrades. Like just the context went up and that's like not even taking into account like vector databases and expanding what all these systems can do.
Speaker B: So far we've relegated relationships to interaction, like verbal interaction. And I think you've mentioned earlier how important embodiment is. And so when we think about the spectrum of relationships, especially when it comes to intimate relationships, there is a physicality and I don't just mean sort of sex, but even in interaction, like, for example, in this room, we're all sitting here embodied, and there is an energy that is present here. Then there is touch, like, plain and simple, and how important that is, whether it's a handshake, a hug. And so it brings me back to this question that was present in the room a little bit earlier, and that is like, what is the purpose of relationship? And I'd really be curious if folks are willing to articulate, what do you see as. So we're all assuming that this is a good thing that we want to maintain. I see it as essential to humanness. But what do we see as the purpose of relationship?
Speaker A: Human to human relationship?
Speaker B: Yeah, human to human relationship. I think just knowing different sides of yourself, right? Like, different relationships bring out different sizes of one person. I think that's the need humans have to have different relationships.
Speaker A: I'll give, like, the most unromantic answer I could possibly give is that human relationships have conferred an evolutionary advantage on mammals, who have a large cost of rearing young. And our ability to survive in herd societies is how we've navigated difficult and hostile environments. And so the purpose of human relationships is to confer survival advantage on different sets of genetic codes and also mimetic codes if you think count culture. But it's also true, right? If you look at the Homo sapiens, like, the reason we became the dominant species is because we were the first species to really be able to coordinate on mass as a counterpoint. I agree. That's, like the originally true. That's where it came from. But I guess one beautiful thing about humanity is that we get to transcend the evolutionary incentives to some extent. We get to make our own meaning and make a relationship what we want. It does seem like it's like the positive psychology literature. Like, relationships are like core to our well being. But I guess we get to decide somehow what relationship. Another angle is that as we have transcended some of our biological constraints, like, just, I don't run up to the sperm bank every day to maximize my fitness. We have some higher level that we get to decide. Our moral systems, we leverage as a society. Relationships in our institutions, like community, democracy, all these things are relational as well, and they serve a purpose for helping us to secure our future well being.
Speaker B: I'm going to go the complete opposite way. You're a very literal thing, but like relationships, we're being very human to human relationships. But the quote of we're just the way the universe looks at itself. And I don't know, it just feels like more magical than the base biology because evolution and single cell to multicell to the way we understand anything, is in opposition to something else. And so there is something about looking at somebody and understanding yourself and the other thing and this kind of feedback loop all the way from the multicell up to human and human relatiOnships. But just like, I, as an embodied point in time, can look at this wall and have a relationship with this wall and be like, oh, I'm looking at a thing. That's fucking cool. I don't know.
Speaker A: So I think good wool. Yeah. Yes.
Speaker B: You can be super science about it and be like, this is the literal thing that has happened, but then embedded within that feels like magic. And honestly, the conversation we were bringing a religion a couple of weeks ago, also God, there feels like something in between relationships and being able to be embodied and experience the world that is like the universe experiencing itself, which feels cool and not as literal as what you were saying.
Speaker A: I love that, like, being human is to be with other humans. If you grow up as a human without being with other humans, you become very dysfunctional. I think that would be true with AI as well. Even though you would have good natural language inputs, you probably wouldn't beat those humans. I think of the analogy of AI and humans as humans and dogs. That thing where if we go outside and walk a dog, they won't bark at any human that they walk past. They won't really care about humans. But whenever there's another dog, it gets really interesting. What's this dog? Right? And same with us. When the AI is treating us in the world and tells us what to do, we won't really care about what AI does or all the things that we see. But when we're going to meet other humans, we're going to get interested, and I think that's going to remain because of this reason. You say we like other humans, and being human is to be with other humans. That's good. It's just kind of like slower bonding, though. I actually really agree with the origin that you're talking about. But then over time, we obviously domesticated the dogs. At first, dogs were not just kicking up the humans like that, and then we domesticated them. So it became a human to dog relationship that we optimized. So we tooled the dog. And so it's kind of like, at what point how fast can we tool, move past human to human relationships and turn it into a human to Agi? Relationship. And in that capacity, it's kind of like we're just a widget for the advancement of whatever the tube part is, right? Like you're just figuring out what can help, what two bonds can form to quickly advance whatever the next stage is to more quickly create. Sorry, it's a completely unbaked thought.
Speaker B: I had the same. I was like, human connections kind of feel like a glue that holds humans together in pairs or groups or larger groups, and innate hold people to move collectively together as a whole for whatever purpose. I think in the AI world, there's like agents now, but I wonder what's next after agents.
Speaker A: Yeah, that's kind of where I was thinking, too. So relationships, I mean, they have lots of functional roles in terms of our experience of what it's like to be human in a society, in terms of meaning, making, community, belonging, understanding ourselves. There's also this sort of, like, biological evolutionary argument, but in a more functional sense, too. It's like relationships enable us to conquer things that no one person could do on their own. And so society has these interesting emergent phenomena that could really look like agentic behavior by a group. By a group that has, because everyone has these set of aligned incentives, and they're in relationships communicating with each other. The net effect is that it looks like this group is acting with some agentic identity. And that's actually a useful conceptual heuristic to lump around it. Just like an ant colony can solve a problem that individual ants are not aware of. They're following some simple pheromone protocol. And then I think in that sense, it's like the emergence of artificial intelligence as distributed, large computing systems is the next evolution in sort of an emergent phenomenon of property that seems to have a holistic or distinct agentic behavior to solve problems that are far beyond an individual person's ability to solve problems like in the. In the construction and the synthesis of new molecules or diagnosing diseases. I mean, medical science is very complicated, so it's one region where no one person can know everything. So in that strong form of this future AI, then I don't even know if actually, if the AI is good enough. I don't know if you need humans for relationships, really, because it'll do everything. What am I like? Let's hang out. Let's watch Seinfeld. It's like, great. I just have a quick add on. It doesn't have to be better than us. Like, social media is a thousand times worse than the scrubber bear I coffee with a budy of mine. But it's right there. It's easily accessible. Like food we throw away. I think it's like four tons of food a day in the US. Like vegetables, fruits, Whole Foods just thrown away. No, only four tons a day. I may be wrong, people maybe 40 times, maybe off. Anyways, point is, just because we have the best version of it doesn't mean we'll take it. We are pretty lazy, self focused individuals. Also, in general, I wouldn't want a relationship with an HEI because I feel like it's not a real relationship because it's more like it's a parent. To me it would be like, can a two year old child really have a friendship with know Albert Einstein? What is he getting out of that relationship? So I feel like there's part of relationship that has to be like a two way. Well, it's street interesting because Albert Einstein had this image as like a paternalistic uncle, right? And why do grandparents play with their know? Because there's other motives going on there. I think one thing that just came up in my mind is at what point does the best AI relationship outcompete the worst available human relationships? Because already there's going to be a use case there where it's like I had a friend group that was toxic and critical of me and not supportive and so forth. And probably even at this point ChaI GBT will give better life advice than the bottom 20th percent of friends or something.
Speaker B: The key there though is what is true about human to human relationships is that the other human places demand on you, which currently, like Chatterbt and other interfaces, don't. It's just one way. You ask a question, it gives you an answer, you ask it for advice, it gives you what you want to hear, presumably. I think it's the premise so far in terms of relationships is that this thing will be the thing that gives. And in receiving, receiving, receiving something is atrophying, right? And so I completely agree that there is a biological explanation for why we banded together and developed all of these complex social constructs for our relationships and that might still be the guiding force. Nonetheless, our consciousness has evolved to tell ourselves, even if it's narratives, about why we get into relationships, but we've evolved to do that and that's still our experience, our conscious experience. So we get sad when we get rejected and we get angry when we're rebuffed or somebody disagrees with us. And I think all of that, that kind of bumping up against stuff that doesn't meet our expectations does something to our human consciousness, which unless, and it could very well be that the AI is coded to do that, right, and that becomes part of its humanness as a replacement. I think there's that question to address. Is it perfection in how well it meets our needs? So, for instance, somebody might be painfully toxic. Someone might also just be challenging us in our views. And we experience that as uncomfortable behavior, but we can't get away from that person. Then there's something that we have to do in that relationship to address the relational rupture or to change ourselves. So there's that work that happens. And if we have AI that we conceive of is better because it doesn't do that right, it doesn't push up against us, then we're following one particular path of defining the best relationship. And that's why a little while ago that question came to me. It's like, what do we see as the purpose of the bare bones of.
Speaker A: A relationship then to provide, to your point, the need. So isn't the need in that capacity literally sustenance? If you think of food and water? If you can't help the herd get food and water over wild, you're not going to matter to the herd. And so I say that because now I think especially your thoughts on this. And once AgI is like, well, shit, I really need electricity to survive. I don't need you to just feed me these tokens. I can develop my own positive affirmation. I need to. I feel like that's kind of the relationship is, to your point, what's incentive? The baseline incentive is existence. I need to exist if I want to exist. I have the luxury of thing, of wanting to exist. So if you're helping me eat and drink and survive at a bare minimum level, cool. I think everything on top of that is going to gravy. And that's, I think, where a lot of the conversations, if you think of what an AGI demand is going to be, it's not going to be for me to fill in a context window. It's not for me to give a token. And after a while, it's time for me to give tokens and be like, well, shit, I just need electricity to be running myself until there's some kind of embodied component to it or something. And so in that capacity, what would be the point of 8 billion people? I mean, you're not all providing me the energy I need to kind of continue to learn in more information? Yeah, I don't know. What are we incentivizing what are we providing to AgI? Again, I'm trying to go steps ahead, but I think when I think of bare bones relationships, that's kind of like the most basic piece. And I think on top of that, we're talking about a lot of other very good human flourishing components that are luxuries. But from a bare bones perspective, it kind of boils down to.
Speaker B: I'll just say one thing to hopefully keep it in play. When you think about NASA's hierarchy, right, is belongingness. I wouldn't even go to self actualization. I think that is a luxury. But that sense of belongingness, is that truly something that we can let go of to survive? And I'm thinking of human despair, right? And when we act on that dEspair, and someone might take that ultimate stab of ending a life, ending life, often that stems from a psychic despair, it's not because I'm starving, but it's because I feel abandoned, right? And I think that is deeply coded within us, that sense of belonging. So just wanted to say that one.
Speaker A: Of the beautiful things I think you're pointing out is one aspect of relationships is how we align each other. So, like, the feedback you get from other people and suicide, it's kind of like, maybe you're getting a lot of signals that you're not useful. But Tony just seems profound about that in what you were saying. When we're interacting with each other, there's some things that we give. There's like a ratcheting kind of vulnerability or things we give each other, but we replace kind of this kind of way that we align each other in conversation and society and parents and children and all that with something that doesn't have that same motivation or a different motivation, then what are we now aligned to? What are we becoming? I think this is a good point. I'm not sure if it's like, will actually be a long term issue in the sense that we can always train them to be more challenging of us. And even if you've seen, like, the original Matrix series, right, they talk about how the first simulation was this post scarcity world utopia, but all the people just died because they were like, there's no challenge, right? So in the absence of adversity, you atrophy and die. And so growth is necessary. It's in the face of challenges. And so for. I feel like that just pushes the goalposts to say, we don't want AI relationships or relationships and capacities with things that are just enablers and pleasers of us. We want things that will still demand this growth and challenge us as well to become better. But then that just becomes like an expanding definition of what our ideal relationship partners look like if they are machine based. Except. But maybe also one reason I wouldn't want a relationship with an AI is like the power imbalancE. Because it seems to me that somebody has so much more power over me. It's like I'm their pet. Even if the AI did not literally, there's maybe a risk it actually would make me like a slave. But even if I wasn't literally like a slave to the AI, it just being so much more intelligent than me, it could manipulate me. Or in terms of on the relationship basis, I am closer to its slave than somebody in an authentic relationship with it. I feel like there'd be no way to actually get, no matter how it changed itself. Probably couldn't escape that. What's funny, you say pet, because we treat our pets very well, actually, and our household pets, we don't ask to work or do dangerous things, and we often spend tons of money and time making their lives work. So actually I would like AGi to do that. I would treat me like my cat treats me, or in some ways, maybe it's just using me to get the house. I think I would respect it, but there's some relationships. The AGI would be more like a parent, and I feel like you would miss something. I could see like an AGI as a parental relationship, but I think it couldn't see it as a romantic type thing, at least for me.
Speaker B: That sort of brings it back to the work becomes relationships and human relationships, where if all of our needs are being met, then maybe it is working on it. We're really dysfunctional societally because of things like Internet porn and all of these other issues. And if our needs are being met, then the work becomes like, yeah, how do you be a better person to the people in your lives? And how do you have normal human sexuality, whatever that can mean? And what is ideal humanness and how do we use AI to that? Is the work kind of to help parent or to help. We give a lot of structure to our pets and also children because they need it. And then that helps them be like actualized adults who can be happy and not unhappy or whatever the fuck it is. So maybe that's the gold standard of what we want AI to be. Give me the structure, and then I.
Speaker A: Can be a better human to the.
Speaker B: Other humans in my life. Isn't the goal, like, relationship biologically procreate? And we intuitively get attracted to someone who's like us, we don't get attracted to horses or dogs or someone else, right? And when we think about AgI, isn't it like something so different in nature? And so it's more like a toy that it's hard to think that we can have romantic relationship with it. I mean, even if we all do, then what aspects of our relationship it makes better? Because there's something very natural about being attracted to other human. And our brain developed for so many years, millions of years, to actually become what it is now. And when we think, hey, we are at this stage, we figured out this computation stuff, and we'll create a super thing that will understand us and we can interact with it, can probably solve certain needs, right? But at the same time, what part of my life will make better what human or another human being cannot give? Japan is already an example of a country that is already so sort of cultivating as a loneliness culture, that a lot of, like, statistically right now, I think 36% females, Japan has fallen in love with the main character. I think 22% male have done that once. You only imagine how many times have they done that. And it is culture that's already declining rapidly in terms of population. And so in some sense, they are already example of culture where you have tools and you can see this and like, why did this happen? Turns out that there are games in Japan in which you can create anime, these anime games. And it's a challenge of trying to build a relationship with this female or male. And you work the game is you try to get them how well, give them gifts, save them from that bully that's being in the corner. Stand up for that old lady or something when she's being young, like a chip, you go through these sort of motion of, I have to work for this relationship so I can gain the relationship, for example. And already this formula has done several things, which is maybe tap into this concept of how we form relationships, which.
Speaker A: Is we kind of have to work for it.
Speaker B: We want to feel like if it was just given to us, it might be that we don't value it. And so by working for it, you get the relationship. And a lot of people really like that in Japan so much that they prefer that over having some human relationship already, amongst many other factors. This is not the only accurate. In a way, they experience different stuff. I mean, they experience really good, fulfilling relationship with a human, and then they experience fulfilling relationship with the thing. But it's more of like, oh, I don't have anyone now. So it's my last choice to talk to the AI. And then it's also not the right. I mean, it could be right decision and it fits the purpose and it solves some problems and pain of some people. But at the same time, is it the priority that everyone, I mean, preferential choice? And is the government, like a lot of governments, trying to increase the population and work on that, then it's creating more division between maybe. I would love to think more about it more in a way, how we can use this tool to actually support our relationships and make it better. How AI can help us to understand our partner and how we can build sustaining long term relationships. So maybe use different in everyday life, but yeah, it feels a bit inorganic, but it's very interesting. I think if it solves some people's pains, then it's great alone, probably, but it's interesting to see evolution of our relationship.
Speaker A: I'm really interested in that point. And we were discussing it before this, and it was that, like, at what point does it become inauthentic? And if we think about three scenarios, one could be, you have no AI helping you. You're sending messages to people, you're speaking to your partner or friends, and you're acting like we do today, completely authentically is who you are. An interim is like you have a chat GBT or an AI assistant telling you, you should maybe say this, or you should maybe think about this. MAybe they're going to react in this way, and then you're adjusting your style accordingly. Maybe that's okay. Maybe people are unhappy with that. The third could be like, it's giving you a whole thing to say, and then you say it. So you're basically reading a script. Then there could be like, okay, it's telling you what to say, and you just press one button and it says it for you. And then the last could be like, it just does everything and it's telling if it's basically you as an AI, at what point along that journey is it like, now it's too inauthentic. It's not you anymore. I was thinking the same when you were speaking to a lot of fourth level, which is Apple is releasing the vision Pro. Soon we'll have the glasses, lenses, and if I'm with a partner, I can just have these lenses on and like earbuds. And it will give me an AI version that is better of this partner, but it is a partner that I like, but it's a better version. And that enables, instead of seeing a game character, it could be someone that say, like, Facebook has all these billions of people and can say like, hey, just meet this person, and it makes it easier to connect with each other. Then you can take these off and be like, what the heck am I doing? Why am I taking these off? Put them on again. But after a while, you might be able to take them off and actually appreciate the realness, the authenticity of that person.
Speaker B: Isn't there also beauty in imperfection? Like, we want friends and people, right? If everything is so predictable, it may not. Is there a value in, like, I think it's a rom.com with, like, hitch or something. With all these dating. Coaches who play the role of human can do a lot of this editing. It just takes more work. So we put value on, just like, the friction that we've overcome or, like.
Speaker A: The idea of a therapist. Technically, you're going to a therapist and they're helping you communicate better. So where I may be like, you made me feel so bad, it was your fault. They're like, okay, they were a catalyst to you feeling bad. Why did you feel that way? That's an intervention. How does that change if it's in text and it's an AI rather than a therapist? What if it's live? Or what if it's life? Like, hey, they didn't do that to you. Authenticity virus probably correlates quite a bit to accountability. In your third scenario, if I were to just shoot off an email and I just didn't even prove it, and I just sent it and somebody came back and they wrote this, it'd be like, well, I didn't technically write it. I feel like there's your first scenario. We're all wearing clothes. It's all layers, right? Like, we're all butt naked. It'd be a very different conversation. And so I think, like, maybe a different party that starts important. These are all, like, exceed areas, right? So, like, we're not. I mean, are we. Are we all showing up as our authentic selves right now? I mean, I would argue, like, from a very basic animalistic sense. No. We have these intermediaries of clothes and sound language, et cetera. And so I think if you have to be the one that says that feels like it's an actual extension of your effort of doing something, and then you're held accountable for it, that's how you would measure it. I would gauge if you have being.
Speaker B: Authentic, there's also, like, intentionality around some of that, too. Like the authentic or inauthentic intentionality of the person to begin with, if you're like, I want to fuck this person. And that is the point of this communication where I want the shortest path from A to B, that's super manipulative and should aid. Whereas it's like I want to have a real human connection and relationship and want someone to coach me through it that feels very different. And one is manipulative and brilliantly inauthentic and one is like, I am trying to better myself to connect in a real way with someone.
Speaker A: So let's assume it's a positive intent. I think that even in the in, if we do assume it's positive intent, I still think there's a line where if you're just like repeating what something tells you to say, you mean, well, you're doing because you want to build a relationship. But then is that really you building the relationship?
Speaker B: I hear the point and I think that is a layer to it. But I just don't think we can assume positive intent and that a really important layer is the not positive because.
Speaker A: It'S going to be two things. One, saying isn't the road to hell is paved with positive intentions. And then also I like to think about a potentially abusive relationship where someone's learned that, oh, if I communicate a certain way, they won't react negatively. Now all of my things are intermediated between that. So it's not just about trying to be nice. It's like, oh, I'm not able to stay in a toxic relationship because the AI is making sure I stay in that toxic relationship because they gave it that goal. I would surely get into relationship with AI. I think it's actually what's kind of missing with chat GPT is that it doesn't have this relationship aspect and kind of have to rephrase things and remember, be more human and remember kind of whole depth of the relationship or your personality, then that would be obviously much more alcohol. Right. Which I think is coming out tomorrow, by the way, like the memories piece. I think as of tomorrow, like Chapter Two is just going to have one stream of consciousness between you and it. Yeah, that would be great, right? Yeah, exactly. I wanted to have it in a relationship, obviously that it serves me and not the other way around. And I think that's then also kind of the part how you handle that relationship. Just left with the master enslaved. That's the human choice. And how we put ourselves in that relationship and how we treat the AI, at least at the current stage, still in the foreseeable future, we still have the control. We can still take off the electricity. And if we cannot make clear that we are the dominant entity here and it needs to serve us, then, well, power is pretty clear. And with regards to intimacy, I guess I would be also open to that. But then the other aspect is, because attachment was so often, that's kind of the human story, how attached we get and not up to the AI. And some people get very attached in very short amounts of time and other not. And that's kind of your personal growth and development to work on that, how your attachment style is and if that's healthy or not. And when you get attached to, I don't know, a sex job, or then maybe you should consult some AI, because that's not very healthy. But, yeah, obviously that can also be, I guess, let's say, for people who have very little sexual experience and then to have a kind of more conscious sex rover that they can actually kind of get some learning out of that practice, that can be surely helpful. Yeah, I think there's two good points there. One is this, like, they're both good points where. Well, that stood out to me. One is that people seek out these main character video game relationships because their lives are so lonely, right? Because the actual genuine human connections are lacking there. And so it's really like this question of like, well, what's the risk of us pursuing relationships with chatbots? It's maybe more of a reflection. Why is our society so messed up that people live such lonely, depressing lives, right? Because they wouldn't go for that impulse response or whatever. There's the experiment with, like, yeah, well, let's not get into the psychology of rats or whatever, but there's experiments with, like, does the rat want to play with the rats, or does it want to just do drugs? It'll do drugs when there's nothing else to do, but it's not really what people want deep down, right? That's one thing. And I think the other thing that stood out was somehow the depth of a relationship enables a much higher bandwidth of communication, right? Where I can. So Joshua and I went to burning man together. I can just say monkey love to you, just two words, and it carries all these connotations and meanings and so forth. And same with an AI colleague, work buddy, right? If I have this AI agent that I've gone through university with it, and it's just like my second memory and stuff, the bandwidth can become massive because it can just be very short little snippets that are queuing memories like that. So in that sense, I would be like, all for. You realize it's like, yeah, there's this kind of thing. The more human, the more deeper the relationship, the more useful they can be in our lives, the higher leverage they can give us to solve the problems. And then it's kind of like a wine lap. That's what we want, right? If I have that robot in our house, let's say I grew up as a child and the robot is the nanny, and I wanted to learn and build a relationship with it, and it should be good. The robot should have good intentions and take care, like, higher priorities and obviously the family unit, maybe the housing unit. And that the more it knows, the more holistic the relationship is, the more useful thing with Dimascal there, that's a foundation who kind of continue not only in a relationship to the empire, three empires, but to the whole, literally empire that cares for that. And based on all the access of information and knowledge, it can be the most helpful one. One comment with regards to society, I'm kind of always a bit with this kind of as a reminder, like the critique as society always society is just a set of people. And the critique on society, why is society so messed up? Is maybe because we have a whole set of people who haven't done the work they should be doing and are not doing their development. When we put that now to attachment styles or, I don't know, within relationships, and like in Japan, they've shown a lot of, if they would simply ask kind of an AI, hey, what is needed to build good, healthy relationships, then they would give answers that, well, you should maybe socialize, you should maybe talk to people, you should check in, but in a genuine way. And if you don't do that, well, then you will have not good relationships in the end. It's easy to say, hey, society has these attributes, but, well, you are in Control to change your immediate society around it. And that you kind of select and develop those relationships in a way and put them on a level where they're actually going in a good direction and you lift the people up.
Speaker B: That assumes a great deal of human agency, which is one way to think about it. And I think it's very useful to sort of shift paradigms every once in a while to say, what if we radically change? We could change the structure. I think the other way to think about this, and this goes back to the biological imperative, right? We're socializing and building Social groups, that there is a structure that is Constraining our individual choices. And so to this question about loneliness, one way to look at it is to think about all of the Nontech factors that have contributed to an excessive Loneliness. And then we could also ask, what about our existing technologies have created these conditions, which then make people even more reliant on artificial intelligence or these avatars to connect with? And I think a couple of moments, people have mentioned Social media, and I think we have evidence of how the ubiquity of social media is Constraining individual choice and agency. And there are moments where we can extricate ourselves from that and say, yes, I have radical choice, right. I can disconnect from social media, et cetera. But to the extent that we stay connected, it has become our kind of the Environment that we live in. And so every time the phone buzes or you see that red notification, you are conditioned to respond to it in a certain way. And so there's an amount of effort that you execute each time you say, no, I'm not going to look at that red notification. Right. And so I think when we have these conversations about artificial intelligence, in what way is it going to change the landscape such that our individual choices then become more constrained? And what extra effort might we do to then combat that?
Speaker A: What do you mean by more constrained? Or how has social media made our options more constraIned?
Speaker B: So, for instance, I think this idea of perfection, which I think has been named so far in the conversation, there is something that having 500 or 1000 or however many friends you have on whatever social network you use, has changed the meaning of friendship, right. There's something about having that endless scroll, no matter what time of day or night I pick it up, can always flip, flip, flip, and there's no end to doing that. That is fundamentally different than before we had those kinds of media available to us.
Speaker A: But that is just a new option. The set of options has only grown. I think there's kind of like an illusion there where if you look at the statistical mechanics of Facebook or something, what is it doing to us? And is it developing agency or not within us? I guess if you're looking to the point of view of agency and how much we understand the world and how things are impacting us, I think it's true. It gets definitional, gives us just more options, but it also directs us, in a way, through those options. You're talking about personal responsibility. We want to live in a libertarian society where people can develop that libertarian society, but there are causes and conditions whereby you get that in education. And so I think I had a really negative response to saying that only because I find it difficult to navigate this world and to have the agency that I want. And I think part of the reason I feel my agency constraints is that there's just so much opposition against my own interests going on about me. It's such a complex world, and I wasn't evolved to this world. This world is like just fucking crazy. Algorithmic capture, making the illusion of choice. Yeah, true choice, literally. Is there also too much choice? Could be bad for you, actually. Maybe we want to be constrained more. We want to be able to constrain ourselves. Might think that the option to do heroin is like an extra choice, but if I ever pick that choice, then I can maybe become addicted, and then that can constrain my choices further. So maybe that's also maybe kind of what it's like. You might start off with a whole bunch of choices, but once you start, you kind of maybe change yourself or become. Addiction to social media is kind of maybe like a loss of freedom or how it changes your breakdown story or whatever. Yeah, I think it's like you have more options, but they're just more similar. So if I keep searching Italian food, Italian food, Italian food, Italian food. Next time I search restaurant, the likelihood of it being Italian food at the top and other cuisines below it are just higher.
Speaker B: I think we're down a lot about individual agency. There's a lot of societal and economic changes that are not like one person either. The fact that people live further away, I think people are generally younger, but it's like the support network of your family and your grandparents and aunts and uncles and that whole community. And with work shifting to cities. And I think those social economic stuff is very hard. And that also causes, I think, some of the loneliness epidemic especially. I think it's actually starkest in older middle aged men. Outside of their partner, they don't have any close friends they really talk to anymore. So I think a lot of that shift is not just like, oh, I don't have that much personal agency. I think they want probably, I think.
Speaker A: For a positive outcome. I really like that you bring up that middle age, and I also see it a lot in late age men is just that this withdrawn, this loneliness epidemic strongest with them. And I think the topic of AI is really, really important to that, especially when we talk about human purpose, because right now, boomers in general, as an aging population, are retiring later. One part is economic pressures, but second part, a lot of people don't know what to do and not to work. I know we talked about a little bit early and we can circle back around because this isn't necessarily the topic we're talking about right now. But yeah, I think this loneliness epidemic has AI can really increase the overall loneliness from a lot of different perspectives. But I've also been a doomer for most of this time. So I'm going to go to the other side of this and say I think AI has huge potential for growing relationships. So the idea that if I'm searching a lot about Italian restaurants, maybe instead of it just saying, here's another Italian restaurant, here's a meetup. Going to a new Italian restaurant, and I think that's not talked about quite enough, is just like, social media has done this really a lot of awful things. It also one of the things that I'm talking to people here in San Francisco in particular. Yeah, I met so many of my roommates through Twitter, and then I was in New York for this, and everyone I met was like, oh, how do you know? Blah, blah, blah. You seem like really great friends. Oh, we are on discord together. And technology, when used correctly, can have this massive social good. I'm really curious about the ways AI can help us get there. I'm actually now optimistic too, because I think Internet has given sort of like a floor on the minimum accessible information, which is quite high. We can search how to tie a tie, all this kind of stuff. And so in the same vein, where there's now a kind of floor in the minimum quality of available relationships in some capacity to get advice, to get therapy, counseling, coaching, whatever, which is, I think, already scarily enough, better than friends that you shouldn't be friends with or something like that. Right? And then that can actually help people get out of this cold start problem. I've always had shitty relationships. I'm inherently distressful. I'm reclusive, I'm sad and depressed. And how do I get out of that trap when I can't afford $250 an hour therapy, right? And so $0.10 an hour therapy sounds pretty compelling. I think also the human empathy, hopefully a lot of this is us improving the human empathy. As a result, I find my email, I keep bringing up Canada. It's much easier for me to just work with GBC because I don't have any judgment. I don't have to face any, no bullshit from social norms or anything like that. So I think there's another side of this that hopefully we can start to be considerably more humble ourselves. Like, oh, we get the opportunity to engage the human as opposed to, I forgot who said it when we started. Sometimes when you talk to someone, you might want them to be perfect. And we have this perception that we are perfect. So obviously the next closest to us is going to be quite perfect or some kind of weird perception of that. And so I feel like there's obviously always a pressure when you're engaging with a human that's fairly inorganic, and it might be much easier for us to just have a lower standard. I think one of the better things that's happened, I think, in SF over the past like seven months, is it seems like the city as a whole has become much more inclusive as I'm in conversations. I think Andrew's been great out with this law resistance, for example, where people come in and you naturally open up a group. There's just a lot of very common social psychology norms that we could have about cultivating relationships that I think we largely have lost because of the hierarchical structure that we have. And so I think one big piece that might kind of come with this is like it make it less difficult for humans to engage with other humans as well. And in turn, hopefully, or use empathy as a way to improve those relationships.
Speaker B: I'll also add that a lot of why relationships are so challenging is that there is a two way street. You are dealing with someone which has generally a different personality structure than you. And one of the defining features of being in 20s, within your twenty s and before, is that you generally don't know yourself and you spend a lot of time making mistakes, or let's not call mistakes, but you accumulate experiences throughout your time in order to in some sense make it as a conduit to explain you. Like the amount of times you make friends with someone that probably shouldn't. You shouldn't be friends with, for example. Right? And it's just like, well, because you don't in some sense know who you are exactly yet work to understand what are those best relationships that are actually for you. And so to say, sometimes I'm also optimistic because I see AI as a good conduit to understand yourself better as well, in ways that oftentimes you can only do a short spy experiential. But what happens with short circuit app? What happens if AI does have a more personalized features? Understand you in ways that you are not able to do that in a way therapists, that's part of the Java therapy and therapy is able to see you objectively, just understand and explain some of those neuroses to which might be challenging for things like relationships.
Speaker A: I like the idea of therapy, but even better, the therapist is only getting your explanation of what's going on. Oh, yeah, I exercise three times a week. My Fitbit says absolutely not. You did like 20 steps yesterday. Go exercise. And if you start having this integrated approach to therapy, it can start flagging. Hey, I saw you spent 20% more time on Twitter the last four days. Like, you okay, you need to talk to someone, you want to text your buddy with the right incentives. But seriously, we're already giving these companies massive amounts of information about us, our attention, our psyche, who we are. If you put that in a package that's already really intelligent and proactive in helping you, that could be massive. It could be detecting not only physical ailments like Apple's trying to do with this new watch in the next couple of months, but mental illness, and especially for young people, being able to have such a proactive, positive force in their life, would be huge, because as a teacher, you're a professor. You see these college students, they're only giving you one little peek into their lives. But if you have something they're talking to throughout the day, depression is already shooting up in Gen Z that doesn't have to be permanent. And when applied correctly, AI could do huge amounts of good in preventing stuff like that, because it wouldn't just be a spoken experience, it would be an embodied experience, almost like a technologically embodied, like your embodied self, your mental self, and then like your technical self.
Speaker B: There's something there, though, that feels very like my depression can look very different than someone else's depression, and then it just becomes a change in behavior. But change is also very human. So it's like, in my twenty s, I was doing very different stuff than I'm doing now. And maybe signs of mental illness were different then, and so it has to be able to do that on a very individual level. And also women's body cycle every month. And my mental state is related to that, too. So are you pathologizing? What are you pathologizing by identifying certain behaviors? And then I actually also have a functional question for people who are more technically based around that question of growth and change, which is, can AI grow separate from its conversations with you outside of the inputs you put into it? I won't get you, because I know that you have question for. I think that's something people are trying out. I'm not sure how cheap that is. Just take a step back from that question. Right. I think I'm hearing so many conversations, I personally sometimes wonder, and I'm also amazed. What is AI really? I think somebody wrote a paper in which they said that they figured out that if they compress a lot of words together, they have a lot of mathematical meaning. That was the foundation of elements. Right. I don't know if we understand AI right now. Do you guys think that we understand what AI is? Or if you could define, like, even before we go to AGI? I don't know what AI is, to be honest.
Speaker A: So I don't think we understand what consciousness is, but it's something we all have and use and practice every day. So I don't know if understanding what it is in a functional, descriptive manner is ultimately like a bottleneck to using it in ways that are effective. And I wanted to touch on the point you brought up. So there's this risk of pathologizing individuals, but I don't know if that's, like, the direction you were kind of suggesting. I thought of it more as, like, a cyber nanny, right? Where it's just like, I'm depressed today, and it would tell me things like, oh, yeah, reminder, you haven't worked out recently, and maybe eat a salad, go for a walk, and suddenly you won't think your life sucks or something. And you're like, you know, it's actually probably pretty. That's probably pretty true, right? And then also as a governor on your behavior. And I want to share this quick little anecdote. So one of Plato's dialogue is called Phaedrus, and in it, he depicts a model of the human spirit in contrast to gods, which is that humans have a chariot pulled by two horses, and one of the horses leads to sort of all the worst things in social media and just short term dopamine, salt, fat, sugar diets get all depressed and stuff like that. And the other horse leads them to sort of a virtuous life of good conduct, good citizenship, and so forth. And the difference between humans and gods is that gods have just two good horses, right? And so the extent to which this sort of AI companion we grow up with over the course of our lives, that knows us intimately, that can act as that regulator or reminder or whatever, helps us to, at least in Plato's conception, become more like gods. And not just gods over our physical reality, but gods over our worst selves, too, which I think is pretty interesting. So we have ten minutes left. So I just want to open up to takeaway discussions or takeaway questions, and also questions that came up that weren't really answered that you think would be cool to talk about in a future session, too, because I'm always like just trying to brainstorm and what are we going to talk about in two weeks? So kind of open for things that you thought were interesting comments, things you learned, perspectives that changed, or kind of open questions for next time. IncentivES is like the root of the whole alignment. And if we would trust, if it will feel authentic, how much we can integrate it and a lot of other things. It feels like a discussion around how incentives affect all kinds of things, how we can align, create good incentives short term, long term, yeah, I'm particularly interested long term, but also the short term things would be. Yeah, and I hope we can circle back to relationships as well. Had an additional focus, society in incentives, but especially like business based incentives, because I don't think as long as there's an incentive where a few people make a lot of money on the expense of other people, that one won't win. What are alternative business models or incentive models for the people building these tools? AI and capitalism. That's going to be tricky one also, have you guys done one on AI and evolution? Maybe those because some of that stuff came up about the purpose of relationships and maybe more abstract, but yeah, that could be interesting too. Sorry about that. I feel like that's kind of one of the big contrasts that I see is like, if you frame AI from two different perspectives, we look at kind of the collectivist evolutionary model of relationships. AI seems like it will completely destroy kind of humanity and collapse it. But on the individual level, I don't see a great reason why an AI couldn't replace all my personal relationships. When we look at the kind of recluses of Japan who immerse themselves in kind of an anime world, we view that as pathological because we don't think it provides the same quality of relationship that we can get from humans and that they are getting a subpar experience from life. And also, it doesn't perpetuate society. You have a plummeting birth rate. But from an individual perspective, if that relationship is good enough, who cares? It's good enough.
Speaker B: And that's like, maybe one of the.
Speaker A: Problems that I see with AI is so much of our progress in humanity has occurred because building relationships and experiencing, going through life aligns with moving humanity forward. AI has the potential to cause those to diverge in which I can get my personal fulfillment. Like, from my perspective, life is great. I'm kind of like a dog. I don't know if my dog has a really good life, but maybe the wolf would not look at my dog very fondly. They would think you're not living life, you've never hunted. That's so essential to who I am.
Speaker B: Actually, along the same page, I feel like a lot of the Phantoms are made by super workaholics too, right? Like, if you talk to their family or if they had kids, they probably ignored them all in pursuit of some other intellectual idea they were working on. So the micro versus the macro, totally interesting.
Speaker A: I think most people would go for the super chill out, let's hang out, let's have fun. There'd be like 5% people like, no, I want to maniacally work hard on stuff. I think that I agree with that as a takeaway. And it reaches an interesting. I don't know if I like the future that that is, but I think it's a super interesting thing where I think, Eric, you said about if a kid was growing up completely isolated right now, they'll grow up without these social norms. They'll probably die sooner. Based on the research that relationships make you live longer, I wonder if you had two kids growing up, one that had only AI relationships, one that had only real relationships, would it have an.
Speaker B: Effect on their lifespan?
Speaker A: Would it have effects on their quality of life? I don't think we'll have an answer for that for a very long time, but it'll be an interesting thought experiment.
Speaker B: I guess my takeaway kind of bit of evolution and actually the point earlier about the black box, and you don't.
Speaker A: Need to know what it.
Speaker B: I actually think it's very important when we think about growth. And the reason I think it is important is because I play a lot of tabletop role playing games and games in general. And part of the joy of playing them is a mass of chaos and randomness. Like you need a little bit of randomness in games to be fun. And if AI is capable of growing outside the input I'm putting into it, then it is more closely mimicking human relationships where you're like Ping pong. I can have a conversation with Josh and then different conversations with Andrew and put those together and learn from that, and then bring it back to a relationship with someone else. And can AI do that? Because if it can, then there is an amount of chaos that my brain and randomness that my brain very desperately seeks, and I can come back to the AI and have a real relationship where it's growing separately from me. But if that's not the case, and the inputs are the only, the initial inputs are the only thing it's capable, then it's very one sided, then it's very parent child. If it's not growing and changing alongside with me in a way where there's pushback, then what's the fucking point? A little bit from your. .2 Humans can see the same exact thing and have different takeaways. And so it's like, can two AIs do that too?
Speaker A: Right? I think the gaming industry is just really interesting. It's how they're approaching NPCs as a whole and the models that they're putting into them, so that the next time you play a game, it's not the same role experience. I think the gaming industry is going to lead a lot of that. I think two veins that we haven't talked about, I think are really interesting in this conversation is one hallucinations in relationships, especially for interpersonal relationships, where you say, you called me fat last Tuesday, and then he has like, yeah, he did. And that wasn't true. And then also like international relationships. So what happens when we already have this massive wealth gap between developing countries and virtual world countries today? What happens when one of these are AI enabled and the other one isn't? Do you get like an Elysium style situation? What does that look like when the norm is for someone to have is like AI Socrates, Ai nannies helping you develop from a young age and then some people on the other side of the world aren't getting that. We're already there, man. Yeah. And it's going to get worse and faster. Or it could be better because you could get like an AGI teacher to cheaply. It equals out. Everybody has like an amazing teacher and then what power dynamic result because of that. So a knowledge economy and everyone's equally capable.
Speaker B: It could be an equalizer, but also make people very similar to each other. Yeah, because one of the examples that I heard is that for know, if you ask how did you celebrate your birthday in America? It might look quite different for somebody who's sitting in a really smart small village somewhere in a developing country, and they might not be eating a chocolate cake, they might be doing something very different. But if they start using AI.
Speaker A: Just a fun addition. No, I don't know how fun it is. Essentially, after party, I said it starts at 40 on there. Terrified. With TikTok already, you see a pseudonormalization of language and behavior. Like in my teaching practice, I've taught literally from California to Texas to New York. And these teenagers will all do this when they're talking or I've noticed whatever I see on TikTok, I literally see that no matter where they are, it could be a small town in Texas to a suburb around New York. So what happened to make. I had a lot of takeaways. I want to make sure space for other people to talk about their takeaways. I've been thinking. Yeah, all right, monologues at 430. I just want to make sure space for anyone else to.
Speaker B: Along the lines of the international. This might be too technical of a discussion, but like open source versus proprietary, like walled gardens, which way is the way forward? I think we talk about AI as it's like one thing, but it's not going to be right. Different companies, different open source models, different implementations, how does that lead to differences based on who's using it and all that?
Speaker A: That's a really good topic. I was talking about that last night, actually. I think the idea I'll be leaving with kind of is related. Something you Said just kind of keeps reverberating my head. Just relationships as kind of like the infrastructure of human formation. You often talk about AI alignments, how we align AI to serve us, but that relationships are how we each align ourselves with each other. I think it's easy for me to neglect how much social feedback just impacts me. I don't know, it's just like a thought that just releasing this interesting, exciting thing. Another aspect is kind of the learning experience, not teaching, but for example, if we have the AI nanny or an assistant that help me to chat with my girlfriend, whatever. Right. And I'm going to do everything right, especially in young age, I'm not going to do mistake, I am not going to experience a lot of, let's say, bad feelings, I don't know how to say, or like, oh, fuck it, that now I learned something. So going toward the perfectionism, we are going to kind of probably learn less and experience less.
Speaker B: Failure. The value of failure.
Speaker A: The value of failure. Yeah. Especially in young age. I believe it's important because even in interaction with a friend, there is a sort of variability. Right. Maybe you ask for an advice and you don't know what your friend is going to tell you. And we like this unknown, let me say, yeah, we can train the system to be random, but in which degrees. So based on what they are going to give me as the advice, based on what? Sorry, based on what they like or based on want to tell to my assistant that I want to become the best in whatever thing. So please behave in that way.
Speaker B: But there are multiple paths, like intentional malicious actors.
Speaker A: Yeah. So I think we have to keep.
Speaker B: In mind what failures do you want kids to experience that AI will make not possible.
Speaker A: I don't know if I'm talking with a friend. I'm not just a good manner, right? But sometimes, okay, our parents, I mean, the way we are as adult, also because we got educated something from our parents in a good and a bad way. So even if my parents was telling me, don't say bad words when you go in a shop, I did, and they learned based on that. And the eye cannot, I think, be effective in let me understanding something only by hearing that I need to practice it. And I think it's hard to practice something only within the system, I guess.
Speaker B: Is it because AI wouldn't tell you not to swear? And then what is a.
Speaker A: I think a really tangible use case is being lonely. Being by yourself is so important for new ideas to learn, to motivate you to feel active, meet people. But if I can just talk to my phone as a robot, I don't.
Speaker B: Know the first time you're lied to, right? I don't know if we're deliberately programming the AI to lie to us, but I think there's value in knowing who to trust, when to trust, like a framework of the world, that it's not all leaving this conversation, noticing that the sort of potency that I have about the impact of the relationship has transformed a bit into all because some of the optimism is infectious. So I believe in the potential for this to help us strengthen our relational capacity. And it's reminding me that we are at a place where we are getting to make choices about how we code this stuff in a way that it's never been that deliberate before. I think it's almost like we're at a precipice of becoming creators in a way that we have been. And speaking about becoming gods, right? I mean, this is a moment that I don't think we as humanity have encountered before, because we've certainly written social rules before, but it's sort of emerged organically, or some tyrant has taken over and said, here's what we're going to do in my kingdom, et cetera. And there's been a lot of bumping up against each other, and that is what has created our social fabric. And so the question of what is ethical, what is good, has always been an open question, right? And so the extent to which we can maintain that openness, even as we are trying to infuse these systems with norms, is the question that I'm leaving with, because there's some intentionality in saying we will give it this personality, we will give it this ethical system. And that assumes a certain universal norm which we've never had before. And so can we give these entities ethics and maintain that kind of heterogeneity that I think we've always had as a society that works as checks and balances? So that's my question.
Speaker A: That's great because it's like coming from a fear of how things might turn out to more like a practical line of action where it's like we have to figure out, this is actually an engineering problem. And I guess it's always kind of been like social engineering. How do we choose to live in good societies if we get to have this incentive discussion? That would be a really nice topic to talk about how we keep incentives open. Yeah, I think so. ToPIcS I'll do in the future is evolution. Certain economic modes of production which would capture capitalism. Video games be super sweet, cultural creation open source for centralized and these are spanned from pretty philosophical to pretty technical in the weeds. But I think we got to kind of wrap up now. Hang out for a couple of minutes. But yeah, I'm super hungry. I'm going to go to this place called Bird nearby and get a sandwich. Anyone's welcome to join, right? Keep hanging out or you can go to your next things, clothing optional, whatever they may. Yeah guys, thanks for coming up. Was recording actually, but.