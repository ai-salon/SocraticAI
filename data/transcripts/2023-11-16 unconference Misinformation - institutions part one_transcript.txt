Speaker A: The recordings are not preserved in their raw form, and they are also washed of any personal identity. And the whole theme is called chaplain house rules. And it goes for people here, too, which is feel free to reference things you discuss here, but don't attribute it to individuals. And so that way, we feel a little bit more free or open to share whatever ideas we kind of like. And same with those recordings. We'd like to, in the future, make an interactive place where people can engage with the ideas shared here. But again, in a way that's not attributing anything. So it might say something like an artist or an architect said something along these lines, but no names. No whatever. That's cool.
Speaker B: Okay, sweet.
Speaker A: So, yeah, this second topic is really about social institutions, right. And misinformation, as it might manifest in them, affect them, as well as understanding things like market forces among companies that are for profit and whether they can be incentivized to be truthful or to reduce the amount of information or society. And I think also really touching on things like upcoming elections and democracy. Right. Which I think is kind of something I think about a lot, too. So we'll do the same kind of format. We can just go around. If you've already introduced yourself, I guess we can just kind of talk more about questions you'd be really interested in talking about in the context of social institutions and organizations. If you have to leave early, it's totally fine. Just get up and go. And hopefully we can kind of weave a path that touches or answer some questions or bring some new ideas. That's the goal. So I'll kick it off. Sure. I'm Andrew, and yeah, I mean, I kind of mentioned this previously, but I think for me, it's like really thinking about how does our government work and how do we make decisions in society, is by being informed voters and having what we think is a reliable way of expressing our preferences through democracy. And that's kind of like, we like to think that's why some societies are successful. Who knows if that's actually true? It might just be because of geography or whatever, but I think certainly for the expression and freedom of thought and what we like to think of as a meritocracy, it's the ability to discern things like talent achievement on a fair basis. It's the ability to vote for things that are in your self interest, to not have other people's interests imposed on you in an unjust or unfair way. I think all of that comes down to what we think is true or what information we have access to as well. So I think I'd be really interested to talk about maybe in a pragmatic sense, like, yeah, what are actual threats to election or democratic integrity or things we can learn from the past and that kind of stuff. And we can just go around with your name or a pseudonym or whatever you want and just some things you're interested in discussing in this context. Do you want to go this way?
Speaker B: Um.
Speaker C: No, go that way.
Speaker B: All right.
Speaker D: You had almost saved me from the pot seat here.
Speaker B: Okay.
Speaker D: So, yeah, my name is Sam. I think in the last group, we were talking a lot about the role of media and understanding news and what's happening in the world.
Speaker B: And.
Speaker D: When you talk about things like the elections, I think a big question is just what can we do? And then kind of building on what the group was talking about earlier is what can we do that actually matters because it's not just a matter of reducing misinformation, but it's a matter of reducing misinformation based persuasion and misinformation based social division, things like that.
Speaker B: Hey, I'm Matt. So one of the things I'm really interested is the whole science thing, right? Where traditionally science is if you understand something, you can predict what's going to happen. And so when we're talking about information, it's misinformation. First we have to understand what does information sphere currently look like, and then how does information influence it. And then to do that correctly, you have to have predictions of what the information ecosystem is going to look like and then be like, okay, now that the misinformation or this information narrative has been introduced, how does this change our expectations of what's going to happen? And so there's a couple of things there that are really interesting, where one is, how do you even create those kind of forecasts? You got cool stuff like Gdell that is going and getting information. But as we're seeing the information ecosystem clamping down because of AI, where Reddit, Twitter, all the newspapers are like, hey, don't scrape me. It's getting a lot harder to get that information as well. So how do you make sure, are we actually losing visibility into figuring out where information is making impacts, even though we really don't understand in the first place? And that's kind of what I'm interested in. What's the problem?
Speaker A: The discussion topic is really understanding what impact do we feel AI will have in the near term on some of our social institutions? In the broad sense, these could be things like democratic elections, these could be things like, are there more market forces that we could pick out or delineate that are driving commercial entities towards more or less truthfulness in their portrayal of events? And are there things that we could do as individuals in the context of members of social institutions to guard against us? And it's kind of broad, I guess.
Speaker B: Social institutions basically.
Speaker A: Yeah. Also just like, what do you think would be key feather points or potential sort of antidotes as well?
Speaker B: Yeah, I think it's not right to just present an antagonistic relationship between people and the social institutions that are around them. Ideally, the symbiotic thing we work with their social institutions, which use political and social ends. And one worry I have is social institutions being overactive with trying to incorporate AI as a means to understand the populations that they're serving without really knowing the statistical limitations of these technologies, or even having good or clean data and ending up making bad predictions or coming up with faulty policy recommendations. That would basically kind of be my worry. Let's drop an lom onto everything and see if we solve problems. And I think that attitude is dangerous both in business and for social institutions and in life. So, yeah, that's what I would say.
Speaker E: And you just getting back to the conversation, Matthew, alternative government structures, truth alignment with profit, place to storytelling.
Speaker C: Yeah. Picking back on that, I'm kind of interested in terms of the incentive for these companies to the profiting from the engagement model, how it actually contributes. There's all these problems and new ways of approaching this that I think picks a lot of different social issues that we are experiencing, especially, I think late to the elections as well. It's going to be very relevant because I don't know how to fight it as long as that is going to be benefiting from the, benefiting from the happy stuff that I assuming against the current. It feels like.
Speaker B: Eric, I'm trying to move to this country, but the 24 election is kind of worrying me very much. But I want to live here and it doesn't look too bright right now. And I think short term, we really want to make sure that 24 election doesn't turn its whole world into Turing. I think there's three things we could do, which is try to push OpenAI to make some election website sort of work, because chatability is kind of rational in a way. And if they could help you with making elections like someone would lobbying OpenAI, that would be nice. Other thing is making sure that adding some penalties for misinformation, which a long time x, for example, is on the verge of getting banned now because they don't care about misinformation, but something like make sure that companies actually care about misinformation because it's in their long term interest to care about misinformation. People don't stop using it, but short term, it's good with misinformation because it increased attention and, yeah, we got to make sure that we don't lose this election thanks to x turning into this massive.
Speaker C: Anya and I grew up in Russia, and both personal and professional interests, I've observed different types of misinformation interference. I'm interested in ethics and parameters of the models that we use and how it can be information warnings or prevention. I'm Remy, I'm a software engineer from the Bay Area. So I'm very interested in. I'm very concerned with the widening gap between the rich and the poor in the US. The standards of education are extremely large between the two. I guess we have a tier one system just like China. It's concerning to me. I just don't see the middle class coming back. So it just seems inevitable we're going to lose our democracy or we're going to have. I really want to have a democracy. I want to have an educated public. I have very bad feelings about what's going to happen. I see the educational graph getting wider and wider, and the quality of news media, even in the tier two cities is just really scary to me. So in its selfish interest, I want the US to be democratic and have a well educated population. And in turn, as a world citizen, we have an undue influence on the world. It's healthy for the whole world if we have a well educated populace that isn't crazy. Little wide gap between.
Speaker B: So I'm Chris. I've been thinking about instead of how AI could be used to fix the problem, how it could be used to make it worse, it hallucinates really well. And that actually might be a really good way to spread stories. That's misinformation that spreads faster, because that's sort of something people seem to glom onto emotional paranoid stories. And I'm wondering if hallucinations might be one really good way for people to do that.
Speaker A: I'm Jason, I'm a product designer. And just to synthesize what I heard from this last group, where there were ideas around how to some solutioning around, like having truth verification, but to go along with that, that comes with this idea, some sort of institution than that. So the other part of it is also education that was mentioned. The intention is that everyone has a right to vote here. So could AI be used to provide both sides or multiple sides of a story so that someone could make the right decision as opposed to just getting one sliced through social media or some other way?
Speaker F: Hey, I'm Murray. I run a human data platform. I'm really interested in the additional vectors AI provides to counter misinformation by providing correct information at scale. I kind of personally hold belief that it's almost impossible to remove all misinformation. So I think it's important to counter that with proper and wide distribution of the information you see fit. Yeah, I'm also interested in relation to future smart cities as well, too, how distribution of information and collection of data can be played in those as well, too.
Speaker A: I'll go for it. Eric and I have two sort of thoughts. One is that maybe blockchain will finally find the use in politics and proving things to be true. And the other thing is that I think the UN is something that we've forgotten about, but maybe we can sort of reinvigorate the UN with AIun and sort of have utilized AI to sort of work with government structures around the world, maybe through this UN structure that helps deliver better governance to all the different countries. So that's a utopian view.
Speaker C: I'm Charlote. Really interesting in learning, and my dream is to teach people, definitely children, how to learn themselves, to give them a confidence and a resilience to go about the world more curiously, and I am trying to start a company in which we use AI to teach children.
Speaker B: Jack. I work at a social media company. So very interested in the whole defenses of governmental, political, as well as monetization. Make money by the same time. Data misinformation. How do you implementable solution?
Speaker G: Yeah, my name is Michael. I'm also pretty optimistic about the future. I think most information on the Internet right now is false. I don't know how you would measure that, but I think it's true. So I'm not too worried about mass dissemination of text. I'm a little more worried about images, but also optimistic about that. But I am also worried about public figures misusing the current trend toward sort of alternative facts or whatever. And really it's about just like existing conspiracy theories. But I do think that existing tools and future tools will make it easier to identify those problems and mitigate them, especially if you're a big social media company.
Speaker C: I'm Anya, I'm a software engineer and a concerned citizen on the planet watching this AI emerge and also having a lot of personal experience, being from Russia and seeing people's realities being distorted with AI, with misinformation created by AI. And I find it very philosophically peculiar that we're going to have AI being a gigantic actor as the creator of this information, as well as trying to come up with versus how AI can help us solve the problem. At the same time, and I'm not sure whether humans are particularly variable in the process or not because we've seen RHF actually make models dumber and things like that. So this whole philosophical aspect of how this all plays out is very interesting to me, how humanity is going to catastrophically self destruct or not, like optimistically create a better utopia. But the worst case scenario that I see is that we all just going to deconstruct back into some tribal society where the only truth is what I can see and touch in the immediate.
Speaker B: Environment, everything else will become escapes.
Speaker A: Metaverse. Yeah, nice. One last fellow joining us. Do you want to give your name and things you'd be interested in talking about here? Questions we could hope to get to as a group on the topic of how AI misinformation, its ability to defend or produce it, might impact some of our core social institutions. So far, some of the topics have been things. Education seems to have come up a few times and training people both for beliefs about the world, but also meritocratic achievement in society, as well as a lot of concerns around integrity of government, elections, and dense the democratic process. Oh, just, you know, my name is, and yeah, a sentence on your background if you want. It's optional, but yeah, really just conversation topics you'd be interested in discussing or bringing up or questions to resolve. This group.
Speaker B: My name is Mohammed. I led the forex strategy on edited disk aspects we talked about is how can AI be used to help us better navigate the Internet.
Speaker A: Nice. Okay, cool. Well, there's a lot on the table because I guess social institutions are very broad and amorphous and so forth. But it sounds like there's actually a couple dividing lines. One is, say, education and government. But I think maybe the more tangible one is it sounds like some people here are actually quite optimistic on the ability to use AI tools for defense and countermeasures and as antidotes to the spread of misinformation. While it seems like there's a clear and imminent threat on their ability to produce misinformation, either deepfakes, photos and hallucinations and. Would you like to introduce yourself and add any questions to the agenda?
Speaker B: Hello, everyone. My name is Leo, and I'm not sure which part of the discussion we're in today.
Speaker A: Just intros really. Context is AI misinformation as it intersects some of our social institutions, namely government selections and then education, and some of the thoughts on whether it's going to help or hinder our ability to seek truth in those areas.
Speaker B: Oh, great. Yeah, I think about this a lot. I myself am working on a project to decentralize social media, and one of the core issues there is surrounding this information. And for me, I worry about same as how today people select influencers that will basically say things that they want to hear rather than telling them the truth. Similarly, people will select AI tools that will simply tell them things that will confirm their biases or hold you tribal being rather than the truth. So I think a lot about how we get people to care more about the truth when there's a direct consequence to being wrong about that particular topic. How to hear the truth.
Speaker A: I guess maybe also related to that is how do you get for profit companies to care about the truth when maybe they make more money just by stoking cents.
Speaker B: Exactly. Two sides of the same.
Speaker A: Totally. Yeah. Our brains are so backwards. Yeah. Okay, well, I don't know. I can kick it off with just a discussion, I guess question, but I'm curious. So last time we started, very philosophical and people hated it, whatever. But to ground ourselves this time, I'm just kidding. Is there anyone here willing to, or maybe someone that has a lot of subject matter expertise to actually lay out a case for AI tools as providing an asymmetric advantage to offense information production or defense in detecting lies and things like from a practical perspective. Because to me this is kind of a black box where it's just like, I don't know, the algorithms in use that could actually stop the spread of misinformation in real time through social networks or how those algorithms operate?
Speaker G: The first, you probably would have done this.
Speaker B: What do you mean by algorithms?
Speaker A: Let's suppose there is like a malicious actor that wants to spread fake news about a topic for some malicious purpose, and then how would people defend against that?
Speaker F: Beat them more information? Quite fair. You're not going to be able to tamp out all bits of misinformation if you do. I mean, it will have to come from a platform directly themselves. I think the best way to fight misinformation is just by providing information at a greater rate and greater volume. Quite, frank, going back to what you said as well too, I think you're almost looking at from a tail wagon dog sort of perspective. I think what actually happens is social media influences a lot of what people think already. I think we can change a lot of what people think based on what we show them, what we drive them into. If we know, number one, who they are, how they think, what they currently do, we can start to formulate a plan of how we can adjust what they think over time. That's what Facebook and a lot of other organizations have been doing for quite some time. I think the best way, if we want to influence human behavior with the truth, whatever that may be, we should go ahead and counter with as much information distributed in the way we want targeted to who people are and what motivates them.
Speaker B: So it's complicated because freedom of speech and censorship. Right. And then information, especially, like every minute, and there isn't any evidence around the debug, we can take that or reduce some distribution. So what companies take preemptive measures. So if you have a classifier that can contain information, you can reduce its distribution by that. What it means is it's lowered down your newsfeed, your Instagram feed, or Facebook. So we're not censoring, but it takes others longer, much more time to find it. And what that does is it prevents problem. Isn't that go viral and be viewed by many people and then also elections? Jackie Chan's dead. That wasn't really consequential to the elections, but it was something that's gone viral. So all misinformation is consequential consequences against millions or billions of misinformation.
Speaker G: Yeah, I think that last part is key is that in order for the misinformation to be bad, a lot of people have to be saying the same misinformation. If they're all saying different misinformation, it's very unlikely that it'll actually cause harm. Right. And so you can just identify a relatively small number of things accurately, and then if you downwrite those or remove.
Speaker A: Them or whatever else, then you could.
Speaker G: Pretty much mitigate all the damage that would come from something that would affect.
Speaker B: The election because we have limited resources. Right. So fact checkers would sometimes over some of their nonsensical. Their argument is that when you shift their information and that's how they. Is that true that if you have different views that it doesn't have an impact because it's not about views. Yeah, different angles. Right. So if everyone's talking about the same thing, if something is important on the agenda, that means that issue is important, that people are thinking about it versus other issues. So I'd imagine that people don't actually have to be saying the exact same thing about something for it to impact things. So I studied journals in a little bit. I'm trying to think of an example off the top of my head, and.
Speaker G: I can't because it suppose like a million different people falsely claim that a million different celebrities have died, and each one claims a different celebrity. Okay, maybe there aren't a million celebrities, but whatever. To me it seems like the likely bad outcomes from that scenario are far less bad than the likely bad outcomes from 1 million people all claiming that the same celebrity has died.
Speaker C: What it can create is it can create knowing to drawn out the actual factual information that is important, that would.
Speaker F: Have affected, which goes back to volume and now volume distribution of correct information. So meta being a platform in charge, has ability to suppress information that they deem as misinformation, and they have the ability to put on more as well, to distribute more information that is considered correct. So ultimately it comes down to the platform. So I'd almost call open AI platform to distribute information right now to about 100 billion people, but more in the future as well too. Especially more as they move on to other devices and platforms as well too. So if you really want to kind of fight misinformation, you go to platform level. I think engagement with the individual, human level and consumer level is useless and unfair, quite frankly.
Speaker A: Sorry, can you expand on what you mean useless?
Speaker F: The average person does not care about misinformation. They do not think about it. They do not think about their data, they do not think about misinformation. They see, they react, they feel, they take action. It's unfair to put the responsibility onto an average person, especially when considered a global population. It's not fair to kind of put the responsibility on them to kind of go through and seek out information. I know we are here, we're a self selecting group in San Francisco, which is kind of a bubble, quite frankly. I think if you go to anywhere between here in New York City and especially anywhere Philippines or Malaysia, for example, you'll find that most people do not consider anything we're talking about at all, quite frankly.
Speaker C: And even as I think for us there's a danger because there is that information misinformation once othered and imagined in the mind, the way it's registered, if it's an emotional information, for example, very recent example, beheading of children, right? When somebody, a president, comes and talks about beheading of children, there's an image in my mind that my brain creates, and it's recorded in my amygdala as an emotional memory association. And then if it comes at this word like fake news, and that it's not true, doesn't change that already existing emotional memory of that thing, even if I consciously understand it was not true, it already affected me. And no conscious awareness and not my knowledge of how the brain works is going to protect me from it. I'm already exposed to that thing that changed my brain, and there's nothing I can do. And I agree with your point. Even for the most educated understanding, like a quick person, it's impossible to leave it the responsibility.
Speaker F: Another thing I'm worried about well, too, is the wealth of information, data out there. I mean, I'm in the data space. All I do is collect information on people. The amount of data where you can figure out exactly how someone behaves and what they do, serving them misinformation as hyper personalized, their individual psychological makeups, their previous past behaviors, their likes. We take it back to Cambridge Analytica as well, too. If this is your digital life, it's very easy to change way someone thinks on social media. So the way I'd argue, if you want to kind of fight misinformation, especially as misinformation becomes more targeted with all this easily available data, quite frankly, the best way is personalize informative or personalize helpful informational messages to a. And that would have to be the responsibility of a platform or of someone else acting through a platform.
Speaker B: So I think that's a good example. The children getting beheaded is an example where originally children were beheaded by the Hamas, and then there were a bunch of different stories about various children being beheaded. And so the original children beheaded kind of got lost amongst the noise of all the other claims of various amounts of children getting beheaded in various different ways. Biden makes a good point. The damage has already been done. Right? So that's where, in journalism, that's called the agenda setting, where it's here are the topics, because everyone's got limited attention to what they can care about, right? So you have agenda setting for here are the things that are important on what people discuss, and then you got framing, which is how you discuss it. Is it the Israelis ahead of people, or is it so and so? Right, and then what's also important there is that traditionally, when you study these things, most people don't actually read the news or pay attention. And then even what was claimed in that doesn't really matter that much. So I don't know how much the fact checks matter because you're still letting that thing stay salient. And that's why I think one of the things people do is you try to depromote it, right? So you say, hey, we're not even going to talk about it at all. Not so much.
Speaker F: Fact check what's within the best. Sorry, go ahead.
Speaker D: Yeah. And so this kind of thinking about the content moderation conversation earlier, there are multiple kinds of harm that affect different goals or different interest groups. And so hearing and believing something strongly, that believing something false strongly is one type of harm when it comes to how it affects people and things.
Speaker B: Being.
Speaker D: The sort of classic disinformation result of not deciding that truth is unknowable and just kind of becoming apathetic, it's a different kind of harm. And I almost think that there might be like, social media platforms may have less incentive to that sort of numbing effect or that sort of sort of confusion effect compared to, oh, we're making people believe one really harmful false story as opposed to we're making people just kind of stop trusting objective reporting. And there are other kinds of harms, I think, that we talked about earlier as well. But the type of harm that we're trying to prevent, I think has a big effect on what solutions we come up with.
Speaker B: I think one of the things that we talk about in our discussion group is that you can create the kinds of harms that we are talking about here without having any false information, just by selecting truthful facts in a very carefully curated way to an individual knowing exactly what their vulnerabilities are like, what sorts of things get them to believe stuff which potentially an AI in an informationally laden, data rich time could figure out. You can convince them of certain things. Right. And so the problem is more complicated than just there's false facts about the world, and we will chop them. And you brought up a really good point about the Dunning Hoover effect, basically, where people that know very little think that they know a lot more than they really know because they don't have the additional knowledge necessary to check against what they already know. And so you also get into this place where a lot of people know very little about a lot of different things, right. And have a lot of really strong opinions, all opinions based off of, could be based off of correct facts, which is a very limited set of correct facts. Right. And so there's also other things here about what are the other parts of the story that you're missing? A certain kind of intellectual humility, so on and so forth that become really important, not just like false information in the system.
Speaker G: Can I bring it back to with AI, though? To me, the question is, do we believe things are going to get better or worse as a result of the improvements in AI? And the reason I believe it's better is that pretty much everything we've talked about so far. It's hard for me to believe that AI will make it worse. Right. We already have false flags, like the Gulf war in the early 90s. There was like a fake story about babies being thrown out of a hospital. That was like one of the reasons why Americans were on board with it. Also happened in the spanish american war. There's all sorts of stories that are fake that have caused really bad outcomes at scale. I don't think you need AI. I'm pretty good at coming up with compelling fake stories. I don't need a Chachi Bt to do it. So the areas where AI would make a difference are doing it broadly in a personalized way at scale. But that, to me, doesn't seem like a necessary thing to cause the kinds of damage that I'm worried about because.
Speaker E: It'S memetic anyway, right?
Speaker G: Yeah. It's more important to me that everybody believe the same thing than that it be like, individually personalized. The kind of depersonalization, though, that you mentioned, or like distrust that might happen as a result of this.
Speaker A: It's one thing to construct a single narrative, which I agree humans are already good at doing, and that's not an issue. But I mean, are you familiar with these sort of social psychology experiments where you put like 30 people in a room and 29 are plants? And they say, like, what color is the orange? And everyone says it's yellow. And the person's like, yeah, I guess. I think it's yellow too. So I think maybe the risk isn't in the construction of a singular narrative or news article, but the manufacturing of an entire fake community of people that.
Speaker G: Have reinforcing we already have, that we don't need. Like, literally, the chinese government employs millions of people to sit online and just like, post comments, and it requires the.
Speaker A: Resources of the chinese government to do this.
Speaker G: Yeah, that's true.
Speaker A: The thing to think about is now that that is a fractions of a penny token prediction problem. Any small actor can now have an outsized influence in the creation of thousands or millions of artificial voices, reinforcing a worldview.
Speaker G: But will it become easier to detect? And will the balance, on balance, will the easier detection as a result of improved ML offset the lower cost of generating the content, and I think it will, because generation is going to be much more expensive than classification, for example.
Speaker A: That's an interesting point.
Speaker B: I brought up the Iraq force several times. You don't have to do one narrative, please. Iraq was bombed around lies of WMP. You don't have to be in like a communist dictatorship. We rely on certain resources. If you don't need AI, think one thing we talked about was these tools that people use to create these information sources are currently being used for commercial purposes, where I've done a lot of text generation, and we had guys out of Europe doing like millions of generation of spam blog sites, or then later on we saw like guys. And so there's these huge commercial incentives right now to do a ton of generation. And so these systems are already being developed. And what we've seen previously is these systems, when people want to, different actors just pay those people to produce content at the source that they want. So we're already seeing these things being produced, and I don't think we have a very good understanding of what that ecosystem looks like to understand where it's going.
Speaker E: Warframe describes these linguistic models as a sort of linguistic compression algorithm. And so we're speaking about the generation, but they're also really good at just getting the information that you want. So there'll be a lot more content, but I'll be able to discern, and this is where it's good at determining truth, or at least curate. I'll get back to that in a second. Curate the experience that I want. I know that it's about misinformation in AI, but I think the discernment layer is about. In our first group, we spoke about this a few times, is combining AI with other levers to do with verification. And at that point you can get this compression, shown me the verified information. Only you could have a feed of only verified information. It might be slightly more temporarily delayed, but at least, you know, everything on there has been verified and you can see the context in which it was verified. And so combining AI with just one other tool, I think greatly reduces that concern that you have about this proliferation of generation of content.
Speaker A: So you had a point earlier about verifying images being taken using sort of hardware signatures, this kind of thing. Are there other, I mean, the question becomes, how do you verify, I guess more general news stories for things that are not image based, like who to trust. Right.
Speaker G: People don't read, so I don't think it's as important.
Speaker A: People don't read tonight. We're banking on that.
Speaker B: People have never read. This is why I go back to the census, because there is one area where misinformation is not nearly the real problem, and that's the best thing. All the same algorithms, all the same social media, and yet we don't have nearly as much misinformation. And because there's money on the line, right. If you're wrong or you get misinformed by the person you're following, you're going to lose money, and so you're getting fucked. Have you seen crypto or Wall street beta?
Speaker A: You believe in the efficient market.
Speaker B: Despite that, way less misinformation than generally outside of that. So I'm not saying zero misinformation, but.
Speaker A: Just like, hit a point. Yeah, I just want to riff on that top. I mentioned the UN earlier, but I think that corruption, money, financial corruption, is a big problem in so many countries around the world. And I think AI, you can have an accounting AI that looks at the books of every country in the world, and corporations and how they interface and where the money goes, who's getting the pork. And that's, I think, pretty doable with AI. And that.