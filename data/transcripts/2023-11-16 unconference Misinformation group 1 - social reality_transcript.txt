Speaker A: You okay?
Speaker B: Sweet.
Speaker A: So, yeah, my name is Andrew. My background was originally studying sociology and economics, and then I've kind of worked in tech as more of an engineer. And I think for me, so I think that the biggest thing in my mind is, like, the whole web two revolution with social media, I think, had a really outsized negative impact on a lot of people's mental health and ended up undermining some of our democratic institutions around information and the integrity of news media and so forth. I think now more than ever, there's a lot of distrust from sources we used to think are authorities on what the case may be and what the facts of a certain event are. So I think our concern on AI controlling what we see and what we believe is interesting because we already live and die by algorithms, determining the newsfeed, determining what we have access to, and what we think are important stories. So we're kind of already there in some way. And I think it's really just more now the generation of that content as well, is going to be something also influenced by algorithms. It'd be interesting to talk about that, I guess, and kind of like, do we think that's a real threat? Is it already here? Do we feel like we can overcome it? Or is this kind of like a flaw in our programming as people to just key into conflict and dangers and threats and so forth? So that'd be cool to discuss. And why don't we go pull up.
Speaker B: I'm Matthew.
Speaker C: I'm a two time founder. I'm similarly concerned. My current company is called true AI, and it's about surfacing as close to the truth as possible. At the last event, similar to this one, someone gave me a definition of truth, which I liked, which was truth is that which has predictive power. I've built with my company a tool called Avi, or the tool is true AI, and the avatar is called Avi, which is artificial, verifiable intelligence. I say that to show my bias, which is, I think, a huge part of understanding what truth is, is data problems, seeing where it came from, seeing who originally formed the idea, and seeing who disseminated it. So this is what I think is basically the biggest problem on the Internet right now is misinformation. I think it's that big. I'm just interested in helping solve it and hearing from alternative perspectives I don't usually get to.
Speaker D: My name is Balnu. I'm an executive coach for Techstar founders next week. But I also started a society to work on societal implications of AI and other advanced technologies. It's called superhuman society. And I became my co host for AI salons, as you.
Speaker A: I'm sorry I said service guarantees citizenship.
Speaker D: Mrs. Paris, really deep. Like one thing, I'm on humanity level. Something I'm worried about with the misinformation piece and AI is that our sense of what is real and what is not, and then people feeling really disoriented, not knowing how to anchor themselves in reality, and we'll have to change the way we see it and the difference between trust and truth. So you mentioned before, we had these new outlets and stuff like that. We used to have trust in, maybe wrongly, like they controlled the narrative, but at least we had the trust in it. Was it the truth? It's something else. And now we don't have the trust in whatever we're receiving, true or not. So those two different concepts and how they are changing, and what is it that is needed? Is it more on the establishing the how do they establish truth, or how do they establish trust? How do they.
Speaker E: I'm Eric. About five years ago, I was working on another startup, but I was kind of lost. I didn't know really what I wanted to build. And I had a profound insight that.
Speaker C: Really changed my life, which was I.
Speaker E: Realized we can build this. We can basically do like argument mappings of different, like when everyone's opinion, it could really rebuild the whole civilization based on this debate. Graphics and since then, I've been obsessed with that. But I think truth is not binary. I think David Deutsche and that there's no.
Speaker F: Other theories.
Speaker E: I love this quote, which is like, we live in the late Creedru era, that soon we will have this software that knows not true, but it will be a very telling software. Basically AI that is integrated into all parts of society. And super excited to talk about how that software, like all the philosophy and all the things behind that AI. What is truth as a process?
Speaker G: Oh, I'm Romy. I'm a software engineer. I left, worked on healthcare on a big data cast platform with Jason here. I've been going to hackathons and had a failed healthcare startup ten years ago. But yeah, I had a bunch of jobs at R and D. I'm very concerned. I'm a big fan of journalism, so I don't know if that's truth or not, but I've seen it erode. I like to read the Nielsen journalism magazine, and yeah, I'm very concerned. I'm seeing the rise of a lot of extremism, mostly in young men. So I thought that was like a big draw to come here. What I don't understand is why the social media extremism and strengthened politics isn't matched by young women. I thought women would be more good at social media than men. I thought I had a stereotype that they were good at.
Speaker B: Maybe they are.
Speaker G: Oh, maybe they don't. They don't fall into this. And then in different countries that I've lived in and traveling, I just see a lot of stuff affecting elections. So I'm very concerned about it. I don't know what the truth is, but I'm concerned about troll farms. I'm hearing they're influencing elections in the Philippines, in Argentina, in Mexico, in shit like Chile. It's pretty crazy. In the US, of course, this is like the ground zero. So I had friends who were doing a lot of the research on the iranian blogosphere in the thought, oh, this is going to be really good. We're going to do data analytics on blogs. Blogs are going to be like this really cool thing to give open the world and see Iran in a better light. But then Cambridge Analytica happened later on and I could see how it could be used for bad.
Speaker C: So my name is Chris, I'm an old school engineer. I came in with web 1.0. I kind of saw a social media decline over time where it just became just echo chambers and just decline into nonsense. So I kind of feel a little bit responsible for the whole thing.
Speaker F: I didn't actually work at Facebook, but I still feel.
Speaker C: And it concerns me.
Speaker F: I'm Jason. I am a UX designer, roots in visual design, but a huge advocate of user research, and doing user research before rather than after you build the thing. I'm also an artist on the side, and I think what's really interesting is even with traditional art painting, I've learned and understand that individuals will interpret what they want out of the piece and will come up with their own story.
Speaker G: That'S meaningful for them.
Speaker F: You don't have control over that. But what concerns me is the election. We did not stop to think about how social media impact. I think that we understand now that there's a framework for manipulating things, but I don't know if, because we've learned that it's going to counteract the fact that we have this new, powerful AI technology. I don't know if it's enough.
Speaker H: Thank you. I'm Charlote, and my interests are more in entrepreneurship, tech and philosophy. I wrote an essay once when I was in college five years ago about AI and whether it will take over.
Speaker D: The world or not.
Speaker H: And now, finally, that it's getting to a point where it's reaching some significant level in our lives. But my main approach always is how can we educate the youth to use it as a tool and sort of not let it take over our lives. And it's what I also feel with social media a lot. I'm 25, so Facebook and Instagram all happened when I was in high school. So I was an early adopter of everything, but my parents had no clue what all these things were and how to sort of raise me to use these tools and use them as tools. So they didn't know what was going on in my phone because they were not interested in it. So I find it sort of a social responsibility to see how we can educate the youth and make them resilient for all the innovations that are coming. And I'm curious to discuss from that point of view, how can we leverage AI as a tool to sort of get a generation that will make the world more resilient and hopefully in some.
Speaker G: Way a better place.
Speaker H: So that's my point of view.
Speaker B: Mohammed. I, I was there a lot of time, so I think we discussed about how things kind of burge from reality sometimes. So basically, what I'm trying to do is to create a kind of workflow that combines a lot of AI agents altogether in a workflow. So like AI agents one, two, three, and you can kind of combine them in the right way and making them autonomous with our white group so that they can create a business that is sustainable, that works well and that works well by minimum monitoring and basically just enough so that it doesn't go hallucinate too much. So basically you can create a software and you can create this software by basically giving some AI analytics to this software. And basically you monitor the activity of the users and the software itself, photo element itself in a kind of genuine way, in a way that it can work by its own, and in a way also that takes the right feedback loop. You train it for sometimes for every day, for every week. And then at some point you do some point in which you created a cash machine, but then this cash machine can be scaled. You create one cash machine that can create 100 cash machines. So basically you create like a platform for multi entrepreneurs. And these multi entrepreneurs, one person or two people can run multiple businesses. That's this kind of business.
Speaker A: What would you like to talk about in the context of misinformation as it applies to our perception of social reality?
Speaker B: Yeah, in the sense of misinformation. Basically, I feel like a lot of times what's happening is that people can some signals, we can kind of make some signals, like from social media, that are unputful, but you can also do it in a way that is, you can basically leverage AI sentiment. You can use the AI tool to kind of see assets based on the data that it takes from social media. So this is the first level. Then the second level is how to kind of make conversions between various LLM models and social media platforms so you can have higher kind of accuracy. The whole idea is how to not only use the right data, the right social media, the right LLM model and chat TPT models, but also how to make them converge, like how to make them work together in order to improve accuracy. So the level of accuracy, basically, the level of accuracy that you can have is enormous. And this is within the platform. That's why it's a lot. But basically, I think given the white rules, no misundersformation can. You have so many agents, so many tools that are working together, you will be 100% for our company.
Speaker G: Optimistic, actually.
Speaker B: I just, like, created some. You can create.
Speaker A: One of us thinks it's not a problem. I hope you're right. I hope you're completely correct.
Speaker B: Yeah, just if I can.
Speaker A: Sure.
Speaker C: Cool.
Speaker I: Okay, so I'm Anya. I originally am born and raised in Moscow. Okay. Hello. I have background in physics, mathematics, and I'm a software engineer. I have being had a very strong scientific exposure. I think strongly about truth as some propagating phenomena that can be and should be verified in a scientific validated fashion. And I don't know if you've seen this film. I'm going to mention a couple of things that have influenced me in this topic. So one of them is that I read the book by one of my favorite authors, Ryan Holiday. His first book was titled confessions of a media manipulator. And that really was the first moment when I realized just how much misinformation, disinformation is being intentionally created and how much it is affecting society. I was deeply shocked by that book because it reveals the underlying nature of how the society actually is being manipulated quite intentionally, and how much money is being put into that, and how many smart people are working on that. So after that, that was around the time when blockchain also came online as a technology. And I started thinking a lot about how potentially there could be ideas and startups that can be created to try to have this provenance of veracity of information. So I'm also an entrepreneur and I have one failed startup, and I'm thinking I'm about to quit my job next week, so I'm thinking about what's next, and potentially it's going to be a startup, and I would like it to be related to AI and somehow help the world. So this topic is something that I'm, as a potential source of information, inspiration to see interesting technologies can be built that could be useful.
Speaker A: Nice. Also got the one failed startup. That's the way to go. Yeah. Well, on that note, we've hosted maybe 20 of these talks now, and I'll do a little brief recap for people that just tuned in, and all of our conversations are being uploaded into a GPT that's now customized with those transcripts and those takeaways. And so you can go interact with it and mine it for startup ideas. That's the whole point.
Speaker C: Cool.
Speaker A: Okay, so just to catch everyone else up that just joined. Yeah. Welcome. So we had a little brainstorm session with everyone here, and we kind of had a bunch of ideas in the board, and then GPT helped us summarize those into two discussion topics. And the first one is the construction of social reality through information and some of the related themes. There are things like echo chambers, things like personal information diets, things like deepfakes and threats, and education of the youth we've gone around. And I'll introduce ourselves and just kind of like a little bit on your background, kind of where you're coming from in the world. So kind of what's the background knowledge? And then also what things you'd be really interested in talking about in the context of constructing what we think of as social reality with information, or where we get our information from and what we think of as true and how we arrive at truth in our personal lives for our own decision making, too. So I think you came in just now. Yes. Why don't you start? And then we'll just go next, and then we can all dig in.
Speaker G: Okay. My name is also Anya, and I was also born and raised in Moscow.
Speaker D: I gave sort of.
Speaker G: Educational background in international relations and economics, and then had, like, a complicated immigration process. So my professional background is more like crypto and blockchain, on the soft side, sales and marketing.
Speaker D: And then.
Speaker G: Also I have this non commercial interest. I've worked with different activists, with the war of breaking out work with different ukrainian activists, and my parents are also involved, like b, two b side of media in Russian. I grew up and I've watched the whole thing disintegrate, and they also.
Speaker D: I don't know.
Speaker G: Very aware of that. And growing up, I was really aware and saw differences abroad and home. But being here, I'm, frankly, online or something, I just watch how. I don't know. Last seven years now, I'm active in different communities, and I just see how those eco chambers have been formed, how all the hype is generated, and I track specific political topics and specifically, like, how Russia generates different domestic and foreign misinformation agenda through social media, through some political activities and lobbying and all that stuff. And even in groups of people who are interested in it, I feel like it's pretty niche, and a lot of people in american politics are not really.
Speaker D: Well versed in it or not believing.
Speaker G: Or not interested, or there isn't much to do. And it's like, it's just not even an eco chamber, but it's different. People, like, shouting about, like, we should contour this or contour that. But even, I don't know, nobody is really. I haven't found any community or not even like a think tank or anything that's really looking at the problem compromised on Twitter, or they're not opads or something. But I just don't like, I feel like there are specific growing tactics, strategies, but the axis of ebil or so, and there isn't much discussion going. I mean, as much as I could understand, there are different social listening tools, and all the networks that are being trained don't know about TikTok. I feel like it's different tech there, but, yeah, all those things. Like, I'm tracking my personal interest and different groups, and I'm looking for more information. Technical.
Speaker A: Cool. Nice. All right. And over there, Jack. Oh, sure, yeah. And then also maybe speak up a little bit. Yeah. The discussion for the next hour and a bit or so is going to be on misinformation and the context of AI. That's the name of the event. But really, the discussion here is like, how do we construct our social reality through different sources of information? How do we establish what is truth? What do we think the risks or outsized impacts AI might have on the construction of truth and the arrival at things we think we can believe? And a lot of that's to do with trust, the education of our youth, the creation of deepakes and so forth. So really just kind of like, yeah, a couple sentence on your background, but really just like, if there's questions you'd be really interested in talking about, then we can kind of try to navigate there together.
Speaker C: I work at a social media company, especially this coming election. I don't work on the US election, but other, there's one in onion this weekend, so there are a lot of AIt speaks. And then New York Times this week just published. I worked on the Argentina election this weekend, so they call it the first AI or whatever. AI made a difference for this election. So there were a lot of stuff going on. And then even that one of the primary candidates, there were a fake video of snorting cocaine, basically. So that was like face change. It was very authentic. The Dallas video was very deemed that. So in a kind of like blurry setting. So there were a lot of misinformation going on. And then we are internally working. How do we fine tuning our AI, machine learning, detection methodology, all those information before they go. So misinformation is vertical that we focus on, and there's also child safety, illegal content. So all of which. But then one of the misinformation, actually, really, how do you use our elbow to detect. Hundreds of millions of videos were being uploaded daily, even like two weeks. There was so many things like really, the machine learning and AI first defined before we pushed them into human moderation.
Speaker H: Human.
Speaker C: So anything? Technicality from 97% accuracy to 99%. That's a huge difference. How do we get to that point?
Speaker I: Nice.
Speaker A: You're the right person to be here.
Speaker F: This is great.
Speaker A: Perfect. Yes. Give a solution. Okay. Going along.
Speaker C: Hi, I'm Mike.
Speaker E: Software.
Speaker B: Cool.
Speaker A: Perfect. Yeah.
Speaker F: Hi, I'm Ishawn also. And then I heard about it.
Speaker G: Wow, you guys are really stealth.
Speaker I: You guys are really stealth.
Speaker G: Like nobody's sleeping.
Speaker F: Let him talk. So, like you, I used to work at Instagram on ranking, and so I was exposed a lot to civic integrity and misinformation. I worked on comments and stories of feed and comments especially was a big deal right around the 2020 election. So there was a lot that we had to do to try to combat intentional misinformation or even just like brigading and that kind of stuff. So it was pretty interesting. And then more recently at Google, I worked a little bit, very little bit on image attestation stuff. So hardware out of station to say this image actually came from a particular phone, and then it's not shipped or anything yet, so curious. Hopefully that'll get out before the next election, but probably not. So, yeah. Interested to see what's going to happen. And I think opinions and want to hear other people's opinions about the likely scenarios and what we ought to do about it.
Speaker A: And your name?
Speaker F: Michael. Yeah.
Speaker A: Awesome. Cool. All right, sweet. Well, that's a pretty good spread. I mean, so we have a lot of different ideas and backgrounds here, which is perfect, because the whole point is get everyone to talk to each other, and. Oh, I don't know if I mentioned this, guys, to you newcomers, but did you mention that we recorded it? Record the session, and it's transcribed. Okay, great. I forget what I said. Okay, sweet. So, I think one way to approach this is, I think, as we moved along the circle, we kind of went from philosophical, more abstract, down to more pragmatic considerations on near term impacts. And I think that's maybe a good kind of discussion sort of flow, as well. And I'd be curious to hear from people here. And this is in any order. I'm just here to moderate. Do you think truth exists? Do you think there is such a thing as objective reality that can be established through discussions and measurements and experiments? And is that the kind of objective reality that's really at risk for misinformation? Is someone going to mislead me as the charge on an electron, or is it what happened in some other country? And how do we establish. I'm curious. Do you have strong opinions on. Yes, truth is real? Of course it is.
Speaker F: I think self awareness hopefully exists, and I think that we shouldn't see.
Speaker G: I mean, does it matter?
Speaker D: I see it.
Speaker G: Again.
Speaker H: Honestly, does it matter if you believe what you see in front of.
Speaker G: You exists or not? Does it change how you behave?
Speaker D: Good point, because that's what I was trying to get at. Like, the truth versus trust of what.
Speaker G: You think is true.
Speaker D: I think misinformation. Misinformation is not new. The whole history and how we live was full of it. All the governments, all kind of, like.
Speaker G: All the religions, all the groups.
Speaker D: I think it was ingrained in a story. And whenever you're creating, all models are wrong, and a lot of these stories are super simplistic and ingrained, a lot of all political. I think even approaches created these storylines, like, what is truth in that sense? We just tell a story, and then we live by it. So I think, as humans, we are meaning creation machines. We create reality through stories. So, from that last event, I was thinking about that definition of truth that you mentioned, the predictive power. But even in that case, we are not talking about necessarily an objective truth, like the big t in the world, because if I create a story, I believe in it. I create the reality based on that story. So it gives me predictive power. Not because it was, outside of me, objective truth in the world. It's because I created the story. Now I'm going to create things in line with that story. And that's how the brain actually works. What we believe is how we end up talking and behaving. And I think, as humans, how we collaborate and organize ourselves. So in that sense, what is the new form of misinformation? I think we've always been misinformed in that sense, and disinformed in that sense from objective reality. But what I see as also an age issue is that the trust, it may not have been truth, but we believe that particular religion or political ideal, or our parent story of how the world works, what it means, but now with everything, it's a fake. If I cannot trust my eyes, what I see with my eyes, with that aigenerated video, or what I hear with a cloned voice, I already don't trust my news outlets, right? I'm not going to trust anything that I see. I hear it tells me, where does that leave us? How do we collaborate, cooperate as humans, when there's lack of trust? How do we create that so that we have some kind of harmony in understanding of reality?
Speaker F: Truth, it's very personal, and it can.
Speaker E: Be societal like, I would say postmodernism, which is like the ultimate relativity, everything is just relative, has had massive negative consequences on society more than many other things. And I think humans are the only animal we know that can understand memes or information. And since our origins, like 200,000 years ago, we have been able to understand memes. That is the way we know if the fruit is poisonous or not. And if someone says this animal is dangerous or not, that is information, and it's an essential part being human. Regarding what is truth, Hopper famous for, I think his theory of truth is one of the best ones where he's strongly against the inductionism or, like, measuring things. Basically, predicting things is also a type of measurement, and it's more about.
Speaker C: How.
Speaker E: Do we even measure things. We have to have a theory of how we measure things, right? So really what it comes down to is theories and explanations, and David Zoiche's definition of a good explanation, which is that it's coherent with a lot of other theories or a lot of other explanations, and it's really hard to verify. So, for example, if you say that God created the earth, the result is that the earth was created. Did you say that, like, God number two created Earth and Earth was created, but rather if it's really hard to verify, like Big Bang with all the specifics, credit Earth. And then if you change one of the specifics Earth wouldn't be created. That's an explanation that's hard to verify. And I think what today can we do? Where are we today? We had these institutions that we trusted, like New York Times, that we don't trust anymore. And then we also have the Twitter annals that provide truth to some extent, which is a good thing. Anyone can partake in a debate now. But I think what's missing in web two right now we can do in web three is ad some kind of reputation system, some kind of good explanation system to understand what are the claims that are made and how are they coherent with truth seeking, because there's no objective truth in my community. It's the best theory.
Speaker C: The reputation system coherence to your popular idea of, you need to know what you're measuring, and then you're consistently measuring against it, and then over time, you build a reputation. Is that the reputation system that you meant? Yeah. Also said something really interesting about memes, and I only fairly recently learned what that meant before it meant the image with two bits of text. I think it might be beneficial to sort of expound on specifically.
Speaker E: Yeah. Neem is basically an idea that, like, an understanding of universe that is shared among multiple.
Speaker A: That sounds really worldview in some way.
Speaker E: Yeah. About any specific idea, right?
Speaker A: Yeah, sure.
Speaker I: I feel like they are comfortable with putting on the philosophical debate around the question of what is truth and questioning the objective reality of truth, because to me, I feel like we don't live in a world that is like, some sort of a superposition of particles in some decoherent state. The person either said something or it didn't say something. The earth is either flat or it's not. The tree either fell or it didn't fall. Like, I personally have an experience where the distortion of a person's reality and truth is personally affecting me because my mom fell down the rabbit hole of basically watching this whole russian bloggers that are literally translating the QAnon materials. And now she literally believes the earth is flat. And that's very hard for me to talk to her about anything. Now, if we start creating a discussion around it in a style of classic what is art? Question, it's turning things. I think it's not making things better. So I just want to push more on the idea that truth is objective and we should think about how do we.
Speaker E: I think postmodernism. I totally agree. But what I would say is this truthfulness. So the probability that the earth is flat is like, 0.1 or something like that. If you can put a number on it, but there is no 100% certain truth that there is.
Speaker A: I would suggest, too, in the context of why star is so philosophical. The thing I was trying to maybe build towards is to sort of suggest, and we kind of got there, but that the things that are beyond contestation are the least important battlegrounds, kind of tautologically, right? Yes, the earth is round. You're not going to rob me of that belief, but who did what in some. Aristotle started his book metaphysics with this great line, which is that all men love to know. And this is nowhere more apparent than the delight we take in our senses. And of all our senses, our most favorite one is vision, because it lets us determine the difference between things. And so all people are naturally empiricists. And so objective reality, to the extent it can be established by empirical measurements of our surroundings, that is surely whether truth is real or not, or objective reality or whatever did you observe derail.
Speaker C: The Arab being a sphere?
Speaker F: The sun would set similarly on a.
Speaker C: Disc extraordinarily high, not the same as long 18.
Speaker A: So I've seen the shadow of the earth cast on the moon several times, and this happened at different times of the day as well.
Speaker F: It's interesting because I think a lot of the people who believe these obviously false facts, they're relying on a philosophy.
Speaker C: Of truth where truths don't exist or.
Speaker F: There is no objective reality. If anything, they believe more strongly in an objective reality than we do. It's that they think your facts are wrong and that there really is a disk floating in space, and it's that they don't want to trust anything but their own senses or some very select few people. And it's that we're all probably more trusting of the experts.
Speaker C: I think it's worse than that, though. It's emotion.
Speaker F: It's a conspiracy against. You think the world is round?
Speaker C: Oh, yeah.
Speaker F: There's, like, psychosis tied up in it. These people are also not. Well, for the most part, I think it's also relevant to online misinformation, because when people reshare stuff, a lot of it comes from a place of deep mental health issues for some small kernel. Of course, there's people out there who are not. But if you're not able to do reality testing where you know the difference between a knock on your door being real and a knock on your door being an actual person there, you're probably also not going to be able to distinguish between facts being said on Twitter being true versus just made up by somebody trying to convince you of something.
Speaker D: So where do we land with all these defects and voice cloning and all the kind of stuff we're going to see with our own eyes and hear and experience and not knowing actually if they are real or not. More than ever before, not being able to trust any facilities that you mentioned. Oh, I have this internal sense of trusting that. And when we don't have that, what happens? And I want to touch on one thing that you've mentioned. These people are not well, I think we can look at it as an individual, like as if it's a pathology. But I think if you look at it from societal perspective, I feel there's a deeper truth why so many people are pulled to these things. Because humans need a level of certainty. And the way we live our world, that certainty is taken away more and more and more and more. And anything like when you feel at rest, there's uncertainty and the threat system is activated. Whenever you can pinpoint a problem, an enemy, it's much easier to deal with reality. And that worries me, because in a place that we don't know what to trust and we are so disoriented, there's no certainty left. There can be mad mental health issue in that sense. And people are going to be more drawn to things that's going to give them a sense of certainty, and it's open to serious manipulation, and it's going to give a lot of power to certain, I think, bad actors, if not prevent it.
Speaker A: Yeah. An interesting angle to that is that because so much of our lives are built around story making and story sharing, is that we also derive a very strong sense of community and identity and belonging by the stories we subscribe to. And so the susceptibility of people to believe misinformation as promulgated in social media is maybe one of the vulnerabilities is that they are lonely and they want a feeling of belonging, and they want to feel, maybe in the conspiratorial sense, part of some in crowd that has an exclusive access to some hidden, secret truth of the world. And this makes them feel special, because in the rest of their life they don't. And I wonder if this is now in the theme of countermeasures of how do you guard against misinformation and that. And it's like part of it is having lots of bots spitting factual information, but also having communities of people that can help reinforce what I'd say is a healthy epistemology, which is like, what is the metric for deciding what to believe? In, because if we have know, it's almost like your immune system, right? Like you have an immune system. Oh, I've heard that trick before, or I've seen that lie. Know what does George Bush said? Fool me once, shame on you, fool me twice, you don't get fooled again, right?
Speaker H: Well, I believe that the problem with truth is that it has objective parts and subjective parts to it, but it's not really inclusive at this moment. So, for example, in the Netherlands, when Covid was like a big thing, there was a really big movement of people who didn't believe in Covid and who were believing in all these conspiracy theories without a lot of conversations about it, with my friends who kept saying to me, why don't they just believe the scientific facts? Why don't they just believe the facts? But that's not for everybody, because not everybody has raised or has the level of awareness to understand scientific facts and to look into papers. So what I think is very important is how do we get a sort of framework that's accessible for everybody to judge information and to make it your truth or not? And I think that's where you can argue for a long time whether facts are right or wrong. But we are not teaching people how to argue that themselves and how to come to their own truth. And I think that a lot of things that I believe in science, and I believe if I drop my phone that it falls. But I know these things to be true, but I also know them to be a privilege because I have a certain level of education and intelligence that not everybody in the world has access to. And I think the frameworks of truth are very important in that way. And how are we going to make them inclusive for everybody?
Speaker G: Have you guys seen the film Rashaman? By, I mean, I wouldn't want to give the plot away, okay, but the idea is that they have a court, and then the truth is seen through the eyes of five different people in an attack, in a rape, and maybe people. Is there an idea that you could have a whole truth? That was really interesting because everyone tells their side of the story, and then it's told from the woman, from their husband, from the bandit, from a guy who encountered them. Is there going to be a 360 version of truth? That would be the truth, because sometimes stuff is true when you see it, but you only have one point of view. And then later we're finding with improved scientific techniques in DNA and technology, we can actually see what happened before Columbus came. We're learning more about civilization as an Amazon. So that is 100 week truth. But it wasn't untrue before. We just didn't have all the information. Is there going to be some kind of construct or framework? 360 degrees?
Speaker A: I like that as much as I hate postmodernism, too. That was the important takeaway is acknowledging that you can have multiple different valid perceptions of reality that are in conflict with each other. And despite our desire to collapse things into a single coherent narrative of history, this is not going to map well to social reality. And so I think part of this in managing conflict among people is also because one thing with the COVID was that it became very militaristic, what you believed in and what your actions were. And everyone has, like, bad information. We're all trying to make sense of shit. And this virulence, well, it's ironic to say that, but against each other for having slightly different interpretations of events, I think is one thing that's a big risk and kind of leads to entrenchment of beliefs that may not be well founded. But I like kind of what you're saying was like, this 360 degree view of truth is that there may not be a perfect overlap in the ven. Diagram of perspectives, I guess.
Speaker G: Yeah. And I'm seeing with a lot of groups, I'm in. I'm with a lot of left wing groups, and it's true with right wing groups, but the closer you are in someone in beliefs, like, the small differences in reality, they get more vicious. So even working with people who believe that the earth is flat, that can be very interesting. There can be such divided opinions on certain events, and you're not really that far apart from each other. So you're not even going at someone who's right wing or left wing. But, yeah, the whole Russia thing, I'm very confused how Americans have an opinion on it. I don't think they really get. Yeah, that's a whole other topic. I talked to people who are down or rabbit hole, and I studied Russian in high school, and I'm shocked that Americans even know what it is right now. It's just really weird. But, yeah, there's a whole qanon for Russia in the US right now, and it's just so weird.
Speaker A: Like, people who've never gone, what are their beliefs? I'm dying to know.
Speaker H: They are really into the.
Speaker G: I mean, it feels so clear to me because I know I've lived with people who are from Russia. Just. Putin came, and he has a certain version of reality that he's promoting and the construction of the russian orthodox church. Like entering into russian life. I feel like it's relative. I'm not totally, but it's just like this very precise thing. And I watched the tv and they have the patriotic things. It's similar to what's happening in America, too. Right? You have these patriotic things and you're like, oh, this is a construction. This is like a recent pop religious conservative point of view. Or you see people taking on things like traditional paganism. It's become like this very right wing thing in Europe, and it's like they're constructing this version of paganism that's very new. I feel like that's a similar thing that happened with the russian orthodox church because it didn't seem that strong. Like, when everything was coming down, my friend, she was going to have a monastery there.
Speaker C: Do you think it's reactionary, or do you think that just they're finding that the narratives that we do generally use together and don't serve them, and they are finding different.
Speaker G: Oh, that.
Speaker C: The american view, in each of the examples, what is drawing the people in your perspective for these alternative world users?
Speaker G: They're alienated by society. Mostly men. It's mostly men without a partner. That's just what I've been noticing. It's true in Germany, too, of people for the AFC. Like the alternative of.
Speaker F: The pattern.
Speaker G: Yeah, the pattern is, like, young men, they have bad employment prospects, they don't have a partner. They're like Incel type. Yeah. And it's very extremist. They're alienated from their own country, so.
Speaker H: They go down this route.
Speaker G: The AFC people, they're very into the russian propaganda. Yeah, they're in East Germany. What's weird is, in West Germany, they won big in the election. The AfD. Yeah, it's like Westermans. So Hessen and Bayern, which I never thought. Yeah, it's, like, really weird, because I thought Germans didn't like Russia, but it's like, even East Germans didn't like them. They're like polish people, but now they're very pro. Well, I've had some of my experience. Well, about Russia. So were we talking about QAnon or russian propaganda for Russia, for America. So for Americans, I can explain. Russia, it's a government mostly cultivates two sides to appeal to the far left and far right. So on the right we have this christian dom, lost white kingdom or whatever that appeals to the far right, that it's like this christian traditionalist family values. No.
Speaker I: LGBT.
Speaker G: No. Trans decadent West Europeans. Yes. What is it? It was like the Europeans are so decadent. The American west. Yeah, the last evil west standing white, something like that. If we go very extreme, like something that appeals to a very far right audience and maybe like in the middle somewhere. So they added religion. Like, yes, russian people are not religious, honestly, many people don't go to church or even if they have some. It's almost like, okay, not to insult anyone. It's like, almost like astrology. People will like astrology. Yeah, that got very big when after everything collapsed in the Russia, they got really into astrology after. Yeah, that was really big. And like the financial pyramids. But I'm saying, like astrology right now.
Speaker D: Because of the pop culture and it's.
Speaker G: Like Putin or whoever, the government, they intentionally started blasting in the last decade. And so that is repackaged and it's presented to the right. And then I'm just saying. And to the left, it's like a communist country. It's like the best view I know of this socialist and communist Ussari sacred. I'm just saying I've seen that in a lot of leftist communities. And there is this whole hypocrisy about how you invaded Ukraine. It's okay because they're like anti imperialist USSR or something. I've spoken to people who was like, you know, this whole thing that, oh, you should listen to people from, I don't know, there to respect minorities and their experience. And they go. And they completely would derail my opinion. They would say that Russia is so multicultural. I'm like a diasporic, I'm brainwashed. Like, this is people on the left that would go completely almost right wing in their approaches. Yeah, that's so weird. If they feel there strongly and they just don't want to. There's something about polarity in America in general, like left and right because of the electoral politics. But I was just saying how Russia packages this propaganda and we would have very far right people and very far left people, but nothing in common in America. But they love Russia for completely opposing reasons. They also don't exist. And prove me an objective reality. They're manufactured. They're manufactured. And maybe they're peppered with local, domestic, foreign propaganda, but it's just like amusing to. So, yeah, it's like this weird construction and they've never been to like, it's so mean.
Speaker C: I think there's like a bunch of different things going on. If you're talking about the war in Ukraine or case I feel like truth there is not quite as clear as whether or not greenhouse gases are warming the planet. And you can get benefit from either side. Like, Putin can use some piece of misinformation to promote what he wants to do in Ukraine. But it's harder to get at the truth of that than it is to get the truth of the fossil fuel companies destroying the climate.
Speaker G: You meant to regular burst then?
Speaker C: I'm saying that sort of like a big difference between the result of a.
Speaker F: Court case or like the fight, oh, just ending.
Speaker C: That's a different. I mean, you know the history of Ukraine better than probably anyone. It's messy, it's complicated. There are russian speakers there. There's lots of things on both sides that don't lend to a very clear explanation of who's right and who's.
Speaker G: I mean, I would disagree.
Speaker I: So in Russia, you probably understand people have culturally a very strong notion of, like, you have to know your history. My grandmother, when I was growing up, it was always like a thing, like, you have to know the history, you have to understand the history. Otherwise you cannot believe in this reality correctly. And Putin is a very smart guy. He understands that and he understands history pretty well. When he launched the invasion of Ukraine, I first was a little bit confused. He gave this two hour history list. I actually watched it and I was a little bit confused, like, why is he doing that? But then later on, I realized that that was an extremely smart move because. Exactly. He understands how to manipulate history. He understands how to put in a different version of the history into people's minds. And right now there is this whole movement in Russia where they literally rewrite history books in high schools. So if you do that, if you start propagating this information, create these roots that are totally distorting reality as far as the history goes, then you cement a base that will be for generations. Yeah, it will be hard to eradicate in the US.
Speaker A: I was going to say that's not just Russia.
Speaker G: This is nothing new.
Speaker D: I think.
Speaker C: Go back.
Speaker D: This is always done in nationalism, the imperialism.
Speaker H: Go.
Speaker D: And I think I want to understand AI's role in it. I'm not technical. And so what I want to also hear about is I can think of obvious way how AI can be employed to even further this type of propaganda or manipulative. And if we get out of the philosophical part of what is the truth with a big t, but just like, if that happened or if that fake video or not. But I'm curious, is there a way that AI can also be used as a strength that we never had before in the past to actually create some form of trust. What kind of information is, like intentional misinformation propaganda? Like flat out lie that doesn't match? Like going out of the stories of whatever the subject version.
Speaker F: I mean, optimistically, I want to say yes, but if the AI is only as good as the data it's trained on. Training the AI. Are you training it on textbooks that have content taken out of it on purpose? Like, who wrote that textbook?
Speaker C: Identify face? I mean, that's a challenge. A lot of companies images, but actually catching them.
Speaker H: Just like Chung.
Speaker C: Right? So even Chung detect itself generated by itself, there's high risk, increasingly sophisticated concept. So how do you detect that? That is an issue it's always harder to detect and create. He mentioned a project that you worked on at Google. I think it answers that question pretty well. I feel like people may have missed it. Which is combining something like provenance with basic ideas.
Speaker F: When you take a photo, and Apple will definitely launch something, probably before the election, but the basic idea is that when you take a photo, there's a hardware device in the same module that does the Face ID detection and stuff, and it will compute a signature of the image, something that's like proving that that photo was taken on the device. And then if you share that on Facebook or the Internet or whatever else, Twitter. As long as Twitter cooperates with Apple, then Twitter can show a little badge that confirms that that photo was taken by an actual iPhone. It's very easy to defeat because, for example, you can take a photo of a photo, you could kind of mitigate that with depth detection because iPhones have a lidar, and then you'd have to attest to the lidar depth and stuff. But then you could defeat that in certain ways by having a big photo. So it'll never prevent state actors, but what it can do is it can increase the cost of an attack.
Speaker C: Do you mind if I ask a follow up question? You said it can't stop state actors.
Speaker F: Yeah, because you could always construct some kind of apparatus to make the fake, but have the providence system say that it's real.
Speaker C: Is there a reason that the provenance system wouldn't not just say that it's real, but also from where it was generated?
Speaker F: For example, if you identified a particular device, that would reveal the person who took the photo.
Speaker G: Right.
Speaker F: And so you wouldn't want to do that because it's like PI and stuff.
Speaker C: What's Pii like?
Speaker F: Personally identifiable information. You wouldn't want to include inside of the image, something that identifies who took the photo. You could maybe have the user opt into that. But Google is not going down that.
Speaker C: Have you heard of zero knowledge proof?
Speaker F: Yeah, the system I'm describing, that was actually the part I was working on. But you take the photo, you attest to it and then you can use zero knowledge proofs to prove various transformations and stuff, but you can do it without zero knowledge.
Speaker A: Zero knowledge proof. What's that?
Speaker C: You worked on it, I think.
Speaker F: Yeah, zero knowledge proof is basically a way of proving that, you know, some information without revealing the information that you know. So in this particular case, it's something like you have an original photo that's taken off a sensor and then you do some transformation like a crop, and then you can generate a proof that says this crop piece of image was actually taken by the original sensor, but you don't have to include all the original stuff that came off the sensor. So you can prove to somebody else with a relatively small amount of data that this really is the cropped photo of something that came off of the phone. Because without something like this, you upload your photo to Facebook, they crop it and now the attestation doesn't work anymore. So you need these kind of transformations.
Speaker C: May I give an example of where I heard it that I thought was incredible, kind of for the use of maybe fighting government narrative, know, using it to know that footage was captured on the ground by a journalist, maybe in Iran, maybe in Palestine, without having to give the information of the particular person or journalist that did it, or as close to that as possible. So one of the things you mentioned was multiple perspective taking, which coheres quite well for the US. The truth, the whole truth and nothing but the truth. Multiple perspectives. Well, this enumerates. How do you defend against it? I kind of feel like this is a really, really good way, which is you can at least get a lot closer to getting different people's perspectives. That is as close to the truth as you can get verifiable from, for example, on the ground and then track that through its spreading or propagation on social media. And that's one sort of really important lever that you need to pull, I think. But the other part of that is even if you have that technology and it works, you need a few different parties to agree to use it. So like Apple need to do it and then Twitter need to agree. That's the standard. And then you need the cultural norm, I think, to shift, actually quite hard to do. To say, I'm no longer going to believe shit unless it's got this on it as well. Right. That's trigger yeah. How do you make that marketable and clear that, well, this image is fake because. And so you need, like, a marketable term that people can share in one sentence, and then you can sort of track how people are checking images or text that doesn't maybe have that information.
Speaker D: Like a trust check kind of.
Speaker C: Yeah. But to the point where they're not going to reject it just on the basis that it's an institution again. Right. Because the thing that happens is people will go, I don't trust Twitter or Elon's been taken over by blah blah blah. And so you want it to sort of be independently verifiable and believable by lots of different parties. I think.
Speaker A: Eric, I cut you off earlier if you wanted to.
Speaker B: Oh, yeah.
Speaker E: It was completely a different discussion, but I think as well, it definitely solves media like photos and videos with having that verifiable thing. But I wonder in my mind, how that solves with chats, where most deep takes are spread, like WhatsApp groups, because they are entering encrypted. Right. Which is like, how would you access the data? How would Apple get access to data? There's this other thing outside of serial knowledge, which is really hyped right now. The next thing, which is like serial knowledge computing, where you can make something that you can verify that photo is taken by an Apple phone without no one ever knowing that compute being done. That is more theoretical and very expensive. And, yeah, this whole thing is very expensive. How do you actually scale it and how do you make it work in an Anton encrypted environment, which everything is going to even like Facebook messenger?
Speaker F: When I think about tech, though, the kind of threat model is different. With an image, you could imagine generating one image of some politician snorting coke, and that's really damaging potentially at the wrong time. Whereas with text, I can just type like, oh, this guy, I saw him snorting coke, and I don't need Chet GBT or whatever to do that. So the relative impact of AI for generating particular texts, to me, seems very low. What I would be more concerned about is seeding the Internet broadly with these texts in such a way that it influences search engines or other kinds of discovery, things that have been traditionally been trusted. Right now, there's examples where for certain kinds of queries, if you ask Google, and one in particular that was really embarrassing, was like, when was Bard canceled the bard project? And because there has been like a hacker news comment and a Reddit comment that said, as a joke, oh, Bard announced today, they announced also that they're going to cancel it next week. And so then if you search Google, when is Bard being canceled? It came up with a particular date.
Speaker C: And that wasn't AI generated.
Speaker F: But I think with AI generation, it's easier to distribute this kind of stuff in many places or trick other people into putting it on their own websites. Like stuff like that, maybe.
Speaker C: I don't know.
Speaker I: Just as a fun note to what you just said, there was a story when one of the stocks of one airline company suddenly crashed, and that it was because all the algorithms accidentally picked up on some very old news story about some crash of their plane that has nothing to do with anything that was going on in the real world right now. But the stock went down because algorithms, they just resonate. They have their own echo chambers. So what I wanted to say is that since we started talking about potential solutions, the way I see it, and unfortunately for maybe a lot of people, not me, I don't care, there is a collision between the solution space. The way I see it, there's a collision with the concepts of privacy, because there's a lot of. If we want to start creating real solutions for real objective truths, there's a lot of solutions that can come out that are very effective if we really allow for dissolution of at least some barriers of privacy. For instance, even the idea of having cameras on the streets that constantly capture everything, a lot of people feel uncomfortable about that. But I feel it's a great way to create some objective reality of what happens in the public space. So whenever any accident, any robbery, or something happens, there is always a way to go back and see the video. Now, when we have ways and tools to manufacture videos and manufacture those images with deep fakes, how do we prove that something that the candidate snorted coke or didn't snort the coke? The way I see it personally, is that if we have a very strong technology that links our identity to some tools that will allow us to create signals that we can send from particular time, location, and say this or this, I'm observing this or I'm observing that, and this signal can propagate with some prominent network that can. Later on, if the information that I have addressed as saying yes or no in a particular moment, if later on it proves to be right or wrong, then my reputation score goes up or down. So, for example, if the candidate had snorted the coke in some party, if there is nobody that sent a signal to say that that beat happened, then that's questionable. If somebody said that, then there is a signal. But the idea is that there has to be a way to know that it's not a bot that sent that signal, that it's a real human in a real time and space. So that's something that I see. There is one company, for example, that works on interesting technology right now. It's a world coin. I don't know, I'm looking at you, it's like you're supposed to know it, but maybe you do. Worldcoin, right? Yeah. So they're building an identity solution on a blockchain with actual ways to verify a human. And I'm really curious to see if there is ways to do that. And I do see that blockchain is probably part of the solution, but the identity verification and something of that. The way I describe the provenance networks with the reputation scores and signaling on a real time space locations for verifying.
Speaker A: Facts, that's super interesting. It reminds me of this quote, I think by Orwell. Whoever controls the past controls the present, and whoever controls the present controls the future. Because the ability to rewrite history and determine what did happen and rewrite events to a certain narrative or justify a certain regime or something like that, I think, yeah, this whole decentralized technology is like maybe the solution, I don't know, finally useful.
Speaker B: I'm thinking about something by being in the Bavia for a very long time. I'm not very impressed that OpenAI and everything was created here, because I feel out of all the places, you can really be yourself. It's not perfect, but I feel the efforts that in other countries, people make to kind of distort reality and just by interacting with people, just like the know the social norm, there's an empowerment in letting people be as they are, instead of way by pointing their businesses, because everybody has businesses. So basically, I feel this is a very good place because the more you let a nation or people, or group of people kind of be themselves, express themselves without trying to on how they should, and then people focus automatically on their strength. If you point their weaknesses to put them in. What happens a lot in authoritarian regions is like we put fear into you, we put negative to kind of force you to behave in a certain way. Well, for me, empowerment is like how you remove all of this. For me, all be as they are, letting the information be as it is, by seeing things as they are. So formation for me is a sign of weakness. But in a good, then it's okay, we all have. But then this is very bad, because why add to it? Nobody's perfect? The empowerment of truthfulness and these things as they are, this is important. It's not only about seeking false information, it's also about what kind of society they are. And the strongest society are the societies in really express themselves. And then they give.
Speaker H: What I wanted to touch upon with the rewriting history is that in the Netherlands we are actively rewriting history because we had a really big colonial past which we used to be really proud of. And we called it the golden era. It's really bad, but I can't say it any different. So I got taught in preschool that this was the golden era for the Netherlands. And these were our golden days, that we were the most powerful nation in the world, et cetera. But now, because the times are changing and the perspective is changing, we are actively rewriting all this history and sort of looking at all the art, everything we have, everything we sort of the gold face in a non.
Speaker C: What's the new name for it?
Speaker G: I don't think they have it yet.
Speaker H: I think they just call it by the date. So it used to be like between 1600 and 1700. And this period used to be called the golden era. And I just called 1600 700. But we have a really big museum and with all the national art, and they are rewriting every physical note that has the art on it. Like this used to be perceived as this, but now it is perceived as this. And I think it's very important to take history also through the lens of perspective. Because we used to have a really different perspective on this history. And now, because times are changing and we get more perspective from other people about this piece of history, we're sort of viewing history through the lens of perspective. And that's what you touched upon and also what you touched upon. And I think AI can maybe play a big role in this. I hope it will be, because I think more than truth, there should be perspective on it.
Speaker I: But this is more like a reputation, like a pr cleanup for the country, not a factual manufactural construction.
Speaker G: No.
Speaker H: They also are teaching your children to have a different perspective on it.
Speaker I: The perspective. But the facts are still the same.
Speaker H: Yeah, the facts are the same, but they present. I think, that a lot of fact is that we went to all these countries and we basically robbed them. But the narrative of it used to be that we brought education and this and that. And now they are opening this piece of history and letting other sources come into the history book. So this is how it was for somebody who lived there. This is what their sources are saying about this blurred period. And this is how they viewed it, but also in the ethics of today's world and in the ethics of the future, what do we think about? They sort of try to open the debate about it. How do we feel about it? Do we have to compensate? Our minister recently apologize, but is it even okay to apologize? Can you do things like that? I think as a country, it is some way very big pr, but it also goes further into it's in some way our heritage. I can't help it that it happened, but I can have an opinion about it and sort of spread more awareness about it. And I think that in that case, perspective on history is also really important because I think it took us by my opinion is that by robbing these countries, how I view it, it got us in a sort of economical place that I also benefited from as a person. So in what way can I be aware of this, listen to it, and sort of use that for the better? And I think it starts with teaching people perspective.
Speaker I: So from the perspective of asking AI to help us clean these things up, I think this is something that will not be very helpful, because, again, this is something that we can change the way we perceive our history, but the factual checks, like whether or not something happened in a certain way, that is more interesting to me. And I think it's more dangerous to twist those kinds of stories like they're taking out of the history of books in Russia right now. For example, like the word Kiev, as in Kiev, Russia, that the Russia actually started. The first capital of Russia was Kiev, right. And they are rewriting books right now to try to twist that reality. And that is a very different story when you actually change the facts. Not just let's think about it differently and let's think.
Speaker H: I don't really feel that what they are doing in the Netherlands has a lot of overlap with what's happening in Russia. Russia, I think it's a very different approach to it, but I do. In cultivating truth and in cultivating facts, I think if you take a lot of facts, like, hopefully the situation in Russia will change. And in 50 years, there are needs to be new knowledge and new history. And then it's also good to take the perspectives. Like these children were taught this in high school. These children were taught this in high school, and these children were taught this in high school. That's why their perspectives can be different. But these are facts, and you sort of create more space for truth.
Speaker I: How do we know the facts? I'm curious about people, of course, in technology. What do you think if we do have models right now that are being trained on information that is potentially non factual, like historic, that have been written, rewritten and so on. How do we create ais that will be able to discern those facts? From what my understanding OpenAI has been postponing the training of the model from later than September 21 is because they didn't want to. At least one thing that I've heard is that they didn't want to train the model on the information that will be already synthesized by the model the.
Speaker F: New one does have.
Speaker I: Yeah, I know they just moved and I'm kind of curious how they worked around this.
Speaker F: I don't know. Other people who work on LMS might have a better opinion, but I think it's mainly because most of the data set duration and activity was concentrated around a particular cut off date. So a lot of the data sets have the same date and it was convenient to continue using that. So they had to do lot of their own work, which they have not published, to push the data further in terms of cleaning stuff up, finding good sources of high quality information. I guess while I'm talking, I guess addressing what you're saying, other people also might have better opinions about this because I don't work on this stuff anymore. But I think in the short term, maybe like next five years or so, it's unlikely that we will task models with identifying what is true and what is false. Instead, if there is a usage like when I was at Instagram, running any model like this was way too expensive for your actual user data, so it just wouldn't have happened. But now they have hundreds of thousands of gpus, so they can do it, and so they probably will. But what it would likely look like is things. Identifying patterns, finding accounts that are saying similar things, looking for networks that are likely controlled by the same individuals, or information that's disseminating that is misinformation, probably annotated by a human, but then connecting that to other similar phrasings of the same thing. That kind of stuff is still very hard to do at scale because it's so expensive. You could just zero shot it with GB four, but then you'd be paying whatever, two cent per post, which is like hundreds of billions of dollars a year. So they're not going to get electricity.
Speaker C: Yeah, exactly.
Speaker F: So they have to find ways to train tiny models to do particular things by distilling or something like that. So I think that's probably what things look like.
Speaker A: I want to touch on that because we've talked about AI as now defense against external sources of misinformation. But there's something that's already occurred which is interesting, which is that we would like to sanitize what these models say and don't say, to be congruent to a certain set of current political beliefs about how the world works and what is social justice and what that looks like. And this is part of this thing of using the morals of today to judge the character of people who lived in the past, just like today, we eat well, some of us do meat from factory farms, and I bet you in 50 years everyone's going to be like those monsters or something, right? But here we just take it for granted. And so already there are uncomfortable social facts that exist in the form of statistics and trend lines and so forth, of our communities and realities that we are training these models to ignore and not speak about. And so there's misinformation from other people who have maybe disinformation, who have agendas on pushing geopolitics and propaganda and so forth. But there's also a blind spot in ourselves where we want to construct a social narrative of the world that is aligned with our moral values, but that our intentional injection of bias into what these models think is true or not, is another area where they will become detached from reality in some way. And I don't know if people think of that as a risk, but I.
Speaker G: Think of that as a risk. What about journalism?
Speaker F: You're saying they become aligned with current values in such a way where in the future they're misaligned with future values.
Speaker A: We will teach them to ignore certain things that are facts of our social reality because they are uncomfortable, and mostly because our concern is that people will use those facts to justify narratives of hate and bigotry and world being flat, whatever type thinking, right? And so it's like, I think one thing that we're all aware of is that there's many lens of analyses to interpret the same set of facts, and that depends on the stories that are present in your mind already. And so we want to be careful about what facts are presented in what context, to not give support to belief systems that we view as intolerant or exclusionary or prejudice or these kinds of things, right? Those are still social facts, but the explanation behind them may be nuanced and subtle. And so that's already like in the RLHF or reinforcement learning that's pruned. What the models can and can't say, it's already evident, right? Like we're already at a point where it's not. Yes, exactly. Where there are unspeakable truths. Right. Which I think is interesting. I don't know if people thought about.
Speaker G: That, but, like, discretion is, because when we talked about history, I thought specifically for history, there is some. I don't know if it's like organization. You can develop different systems of ranking. So if the model is trained. I don't know, you will know some parameters it's trained on. And then somebody creates this ranking, like those instructions that you write to, censoring how the chajipti should behave, like bing. Those parameters that they talk all the time to try to censor it more. Those types specifically for history, someone can develop a ranking system. So, okay, we have those facts about history, and we need to test them against some other facts. So let's say we have in this version of covenant of Russia.
Speaker I: They went.
Speaker G: In with investigators to document. Yeah. They had the investigators come in and they were like, okay, we have to. The war crimes. And they had, like, an investigative journalism, and they had this whole system. Like, don't journalists have this framework? And they're like, oh, they're documenting it. A lot of it's not released to the public yet, but they're like, they're interviewing a lot of women because they.
Speaker H: Didn'T want to go public.
Speaker G: Well, a lot of it is released. But you're saying. I don't know if it was the UN. They had a bunch of people after Luca happened, and they were gathering a lot of the evidence right after it happened. So isn't that the same thing, what you're talking about? Well, I'm saying we talked about AI helping with misinformation. So let's say you have some information AI can tell you. This was like, let's say, okay, let's say Google was fake news. Yeah. So to have this community notes, but generated by AI and say, I synthesize this types of information on this ethics criteria, and you can, like, let's say there is, like, four, I don't know, some types of different ethics parameters. And a person can just. Or let's say, okay, like OpenAI and this, western companies, they will adhere to this ethics framework. So if we have history, we will need this types of sources. So sort of like media literacy or history literacy for AI, which we use it in our technology. Well, yes, I also know that PC is annoying and let's walk stuff or excessive censoring. But then let's say we all live in western countries by reason. We adhere to this UN charter and even in theory at least. Okay, we can say we have this subjective truth, but then we all fall back on law and we want to be in a country where we have strong institutions and power of law. So how does that.
Speaker A: That's a great transition to the next topic. I want to be mindful of time. If anyone has burning comments on this topic of social construction, truth and the intersection of AI, now is a great time, but otherwise, let's take like a little five minute break. And I'm going to go to the bathroom. Yeah, just a little bath. And then we'll reconvene and we can have another discussion on social institutions. Go ahead. Sorry.
Speaker G: Oh, I thought we're taking a break now.
Speaker A: Yes, we are.
Speaker C: Okay.
Speaker G: Sorry.
Speaker A: Okay, so just reconvene in a five minute Jeff station.
Speaker G: So.
Speaker H: Yeah.
Speaker C: But in a very.
Speaker G: Information.
Speaker C: Film, like a particular.
Speaker G: And these days all restaurants, like.
Speaker F: You'Re not.
Speaker C: Treating.
Speaker F: There are people who.
Speaker C: Yeah.
Speaker F: It'S just sort of like. There is. I mean, stop.