Speaker A: Oh, you guys have so much light. This is wonderful.
Speaker B: This is Andrew's place.
Speaker A: Oh, this is Andrew's place.
Speaker B: This is Andrew's place. And yes, the light here is very lovely. And it's.
Speaker C: Going to sit where.
Speaker A: And I'm going to.
Speaker B: You're fine.
Speaker D: That.
Speaker B: Nice to meet you again.
Speaker D: It.
Speaker C: All right, well, I hope everyone brought their notebook because there will be a quiz later.
Speaker E: Just kidding.
Speaker C: Great. Well, thanks, everyone, for coming out. Oh, great. Okay, well, then I have to say this right now. We like to record these sessions and we go by Chatham House rules, which is to say things may be referenced that are brought up here, but no one will be personally identified. So feel free to say anything that's on your mind or anything like that. And the purpose of the recordings, as we mentioned earlier, if you maybe came late. But it's to try to just capture some of the ideas that are shared and then make them interactive for people in the future so that you can have kind of an ongoing dialogue and conversation. But they don't object to that. They don't feel comfortable with it, just the references. That's the point. So we absolutely remove any kind of personal identifying information, names and all kind of stuff. And even then, people aren't even preserved, really, as it's actually formulated. We just distill out the key points and thoughts and then put them in contrast to each other. Someone raised this point, another person raised this point. So it's not even like, yeah, cool. So with that in mind. Yeah. Topic today, personhood, I think, is super interesting. And I think I was hoping that I'll just give a little, maybe 22nd preamble and then we can all kind of go around and share some of the questions we have. If you want, you can share some stuff about the background, like kind of where you're coming from in the world as much or as little as you want, just to help us understand what our shared context is. But yeah. And this topic of personhood, I think is really interesting. And it's definitely going to be at the forefront of our minds as we start to think about who do we define as people in society? And in this, by this, I mean pets are kind of like people to us. We consider them members of our family. We care for them. We mourn their passing. This kind of stuff. On the other hand, corporations and organizations have some sense of legal personhood and they're able to enter into agreements and own property and have bank accounts, that kind of stuff too. And I think as AI Persona become more and more social personas, to us, people we interact with, even have relationships with. There's going to be some question about at what point are they considered to be having some level of permanent social identity that now affords them rights and obligation and a certain status in society. So that's kind of the theme, I guess, but it's super, super broad. Yeah. So I guess I'll go first. My name is Andrew. Background, kind of sociology, economics, and then more recently engineering stuff and physics. Really interested in AI, in all these kind of facets and. Yeah, I think my biggest question is, I would maybe put an extreme view. I think it's probably inevitable that some kind of machine intelligence thing in the future will be considered people and own property and have rights in some capacity. I think it's kind of inevitable. And it's just a question of, like, at what point do we extend this kind of inclusion into society, or maybe empathy or something? And what's the level of intelligence required to justify that? And I think it's kind of a matter of. Yeah, I'll just go to the list here. Cool.
Speaker F: So, hi, everyone, I'm Phil. My background is in software development and startups. I worked for a long time at a university, like innovation center, specifically, did a lot with startups that came out.
Speaker C: Of there.
Speaker F: And did a lot of stuff with augmented reality there. Now I just moved to San Francisco and in startup world looking and yeah, I think that. I tend to agree. It's inevitable that there will be some. If we take it as a given that there will be some intelligent life where it is ambiguous or undetermined whether or not they deserve property, personhood, et cetera, then the logical extension is there will be some race of artificial life. And then the question is not, does one life form deserve property personhood? It's does this race deserve property personhood, et cetera? And yeah, that's a holy shit question, because we're not only talking about the creation of individual units, we're talking about literally categories of new types of mind.
Speaker C: Cool.
Speaker B: I'm Ian, I have a background in psychology, and now I work in AI governance and run the AI salon. And one of the things that got me into thinking about AI ethics in general were the concept of moral circles, the ever expanding. I kind of see this kind of a really positive progress of humankind, of the expanding moral circle. And this concept is just like, started with me, my family, my race, my kind, maybe humans everywhere, maybe animals can continue on. And most people, even some kind of staunch animal rights people, they have their own line, and maybe it's because there's a feeling that there's some scarcity of moral empathy. Like you have to draw boundaries in some ways, which might be somewhat reasonable. But I assume, just like Andrew said, that at some point there will be AI systems that aren't just maybe considered people, but are deserving of being considered people. They have their moral patience, and I think that that's potentially a ways away. But I see no reason not to start priming the kind of moral intuition pump right now to think about that. So that's one kind of question and area I would love to dive into. And the other kind of draws on a previous conversation we had here on digital twins, which is if I'm going to have AI agents in the world that reflect me or are endowed with an extension of my legal personhood, what does that mean? How should we be interacting with those as persons? Is doing something that damages that AI twin, damaging me? How does that intersect with both our kind of moral and legal framework? So that's not a separate person, it's an expansion of my own personhood. Yeah, those are the two topics.
Speaker A: Hi everyone, my name is Ruby, and I was particularly interested in this topic because the thing that I've been contemplating for many, many years is consciousness. And what is consciousness, and how consciousness, to me, what I really care about is elevating consciousness, and I see AI as being able to really do that, and then what is defined as conscious? Like will AI become conscious in itself and all those other things that we talk about. So, professional background is I've been in tech for a while, so right now I'm an advisor to AI startups and I'm also looking to do one on emotional well being. Post all this initial noise. I'm actually looking at some technology beyond OpenAI, that's real time, because I feel like emotional well being, emotional health around the world is what's going to elevate consciousness. Because I feel like when people are in a place of, I'm going to say it, love and compassion and feeling of oneness with community, that's when we're going to stop. It's not even stopped, we're going to be able to transcend a lot of the problems in the world. So I'm really excited thinking, first of all, those of you heard, I really care about democratizing opportunities for people, because I think when people have the economic opportunity, that also increases their well being, and when people are secure, there's no need to fight and there's no need to take from others. So I feel like AI in particular technology has connected people like it's never been before. So I hope that AI is going to do that even more. It's debatable question, like, will AI ever become conscious? I believe it will and I actually believe it will be influenced by us. We create AI because AI is taking all the data from us. Everything that we express now is being fed into AI. So are we expressing hatred and anger? Are we expressing peace and compassion and love? That's how I hope we'll be able to teach AI. And actually somebody from singularity University. Do you guys know Peter Diamondes? They're writing some great stuff. He literally put out an article last week about how AI are our children and that it's up to us to teach AI and teach its values and teach its data. So that, to me, is what I would like to believe. And that's why I'm excited. So I feel each of us have that responsibility.
Speaker C: Awesome, thanks.
Speaker E: Hey, everyone, I'm Eric. My passion is like elevating people's goodwill and saying, like, raising consciousness and well being. And I think the way that what I'm working with is building a futuristic superac where a lot of the concepts I get into personhood. If you can have one person and one identity online, because now if you're like a russian agent, you can have a thousand profile. If you're like a company, you have a chatbot, you want to interact on Twitter already, how should be represented? And that's kind of how I got into personhood. But there's a lot of other. How do we treat humanoids when AI gets into a human form? Right? It's like the Tesla and the bottom dynamics. Are we treating those as more conscious or are we treating them differently than the model that they run and are connected to for the Internet? So how does it work on the edge versus the whole model? And I think we have a hard time knowing whether other people are conscious. There's no certainty about that. We have guesses of how consciousness arrived in evolution. And there's other theories that everything is conscious, but I don't think we will know for sure when AI is conscious or if it already is. It's a very hard problem. But we can still. Same with dogs and corporations, even though we don't know if they're conscious, we still treat them in a particular way. But particularly with AI, there's this probability that it is conscious, and we should have that in mind in how we treat it. I think we talked about this before, the futuristic kind of interactive podcasts and using transcripts and stuff like that, as we can interact more and more with AI, what rights should we give it, what's allowed to treat it, et cetera. That relates back to consciousness, and are we going to treat it as humans, or are we going to let the AI decide how it wants to be treated, and how do we get away from the human biases?
Speaker D: Okay, I'm Anne. I was a tech founder a few times. My last company was an AI company, and I'm working on AI company now. I also run a discussion group. Just go ahead and dress up because I have a happy hour in a few hours. You all asked me about that. You're all invited on personhood. My initial thought on that is that right now, with human beings, we lump a lot of things together, and I think that with things like AI, we'll probably splinter off a lot of these things. Maybe an imperfect analogy is like marriage. We've had marriage for a long time. We've had this paradigm of monogamy, and there's a lot of assumptions baked into that about what people are trying to do when they interact with someone, and dating and all of that. There's so many assumptions there. And then now we have all of this non monogamy, and people are breaking or splintering down into, like, am I okay with having a loving relationship with other people outside my monogamous relationship? Am I okay with physical stuff? So everything is just being broken down into these subcategories and then decided on that way. And I think probably it's going to be like that with AI, because if you think, like, you have an AI, it's just utility. Like, I have an AI, what am I trying to do? Maybe there's really no case there for that. AI deserves some kind of dignity, really. I understand that it's just a model. It's just data going in and out, and so I don't need to treat it with dignity. But there's still utility in that model, having a reputation in society or like a social standing. I think of it as probably all the things that we encompass. Personhood now will probably all splinter, and then we'll get really into nuance about these very specific sub things. But actually, what I find most interesting is how we will interact with ais. And the effect on us, I think, is most interesting to go back into monogamy and non novelty cheating. Like people talk about, okay, you have AI companions. Is that going to be cheating? It's sort of like. Or abuse is another, I think, example. Like, if you are abusive to an AI should worry about that. And I think whether or not that AI has any sort of awareness or agency or anything, that it is deserving of some kind of dignity or it needs some sort of treatment. Putting that aside, I think that the way that we interact with entities affects us. And so I think that's what I'm most interested in on this subject, really. But anyway, I'll stop there. I feel like I'm talking a long time. Thanks for sharing. Hi, everyone. My name is Elena. My background is in machine learning, actually, and startups. I've been working in this space for 20 years almost, and also founded a few companies. Currently working on a company which is at the intersection of AI and healthcare. And so we are thinking often, because we're building, in essence, personal assistance for health, and these assistants would impersonate a doctor or a nurse or nutritionist. So oftentimes you have to basically build them in a way that the person is going to build trust with them and listen to them. So they will become almost like they will acquire a Persona. And the question that I always think about is, how far are companies allowed to go that path where you're going to build? It's inevitable that we can build something that is highly convincing, that it's going to feel like a human. And humans, naturally, they treat things even when they know that they're not actually human. With people like pets, they treat them like people, because it's natural for us, when you build a relationship with something, to start relating to it and treating it well, and just like being your best self. So I often think about these things. I also think that it's quite inevitable that we will become AI. AI is never going to be disembodied. That's the path we're going. Like you said, it is our children, because I think that there's going to be emerging at some point. That's my most controversial opinion. We can argue on this if you want, but those are the lines of thinking that causes here. This is really cool, like hearing everybody's thoughts.
Speaker A: My name is Jenny. Right now, I've been taking a sabbatical the last year, but I've been in video games for probably the last 20 years. I am not a programmer. I do business development, partnerships, that sort of thing. But I got into games because I love the. At the time, this was, of course, the only interactive medium that was readily available. I saw it as an art form. I thought, I really want to be in this space. I want to help propagate it. I want more people involved, and I feel like 20 years later, I may have contributed a little bit there. But during this time, of course, I think games has been really on the forefront of machine learning. And kind of when we talk about NPCs and AI and that kind of Persona and digital twins, there is a lot of personal representation, especially nowadays in community forums with your online character in esports. I've also fallen in love and cried with video games characters. So that's kind of the premise of where I've been. But stepping outside of that, I took the year off to really kind of look outside of that space and to kind of go a little broader. And I've realized with AI, to me, it's been a great opportunity to level the playing field. You talked about that earlier, both from language to art to any kind of communication. And when we take that a step further, when I took that a step further, I kind of was very enamored by the opportunity and excitement, and only in November did I really give myself the chance to kind of go down the dark path. I encountered an article around NASA, and I don't know if this is real, but NASA and a quantum computing project that they were working on, and then it got canceled by the Department of Defense because the AI became so close to General AI, and the end result was kind of this doom loop of death to humans. And then I kind of came out.
Speaker D: Of that as well.
Speaker A: So I really just came here to kind of hear everybody's thoughts around that, because you've mentioned sort of AI as our children is the sum of our parts. And I think about fifth element and how Lilou was like, why should I save humanity when humanity doesn't want to save itself? So I know that's really good. Anyway, so thank you for sharing. And I think that's the question, if we ever got to that point, how.
Speaker D: Do we put that in?
Speaker A: Put that, and is there enough love? Can we just, like a million people.
Speaker D: Crank out stories about love and then change its mind?
Speaker A: What is that? So again, similar to how I cry and I love about video game characters, how does that. Can that emotion come about?
Speaker D: Hello, everyone. My name is Tiffany. I'm a master's student in international policy, specializing in cyber policy and security at Stanford. My research and my interest focus on the nexus of artificial intelligence, geopolitical risk and policymaking, and cybersecurity. When I think about AI in personhood, I think about it on three levels, at the individual, community, and institutional or state level. On the individual level, I think about how are we going to preserve our identities? Or how do we reflect our human identities in the ais that we're training? Because at the end of the day, I feel like ais are going to become an extension of who we are and what we want to become in our societies. So a lot of the behaviors that AI is producing, whether that is power seeking, whether it is going rogue, these are behaviors that we are seeing in society today. So shaping the AI in accordance to the identities and the multitude of abilities that we have, is something I'm really interested in at the personal level as well. I'm interested at how our perceptions of the world will change with AI, and specifically when it comes to AI and social engineering and disinformation, how the spread of deep fake, fake news and disinformation is being democratized by some of the risks of AI. And how will that not only change the way we view our institutions, but also change the way that we consume information? And then on the institutional level, I'm interested at the intersection of AI and agency. So who is governing today? Are we still going to be the leaders of our own communities, leaders of our own institutions? How do we reconcile potentially a leadership aspect of artificial intelligence and the institutions that we have created that have allowed for a time for stability and peace building around the world? Are these institutions still valid today? Or are we reimagining a new structure that will allow for us and AI and other powerful actors, not only states, but also private companies, to come into the fold and to have a trilateral cooperation to rebuild structures that have inclusions for everyone? The last point I want to make is that's where my research is focused. It's basically how we can leverage the silver linings of AI, but also apply some sort of standards of regulations on the harmful use of AI, especially by rogue actors and cyber espionage groups, which inevitably will really lead us to the question of agency, of adaptability of our own agency, especially with data privacy breaches and AI. So these are some of my thoughts. I'm really happy to be here. Thank you. Hi all. I'm Sabina. I start to think about the concepts of personhood, know as a concept between contemplating between the perspectives of individual communities. Echoing off what Eric has been saying, I think the only intelligence, the only consciousness that we can prove is ourselves. And by kind of considering how we're extending this concept of personhood towards others, towards a larger community, we're constructing it through the feelings and emotions that we're feeling, that we have constructed towards the interactions with other people, with other entities as well. So I think kind of more in the thoughts from what is going on in terms of the expending moral circles as a community that we have right now. It's really interesting to think about how as a community, how kind of extending outside of the core agents of proving the intelligence, how we as a larger community aren't defining what we regard as other intelligences, other persons around us. And I think one kind of controversial kind of thought that I have going a little bit fast forward in time is that when I think about AI governance, I think something that I always think about is how the co governance between AI agents and humans will look like in the future. How I think in a lot of ways, I go in back the points that have been made previously on how AI is elevating our own consciousness, how in a way we're looking at Rick and Morty like the snowballs revolution in the sense that we're co governing with not, you know, placing ourselves within this conceptual framework where AI are the imagined colonialists that we're looking at in our society. I think that's kind of a controversial, but at the same time, a kind of framework that I'm always going back to is that how we're extending our own conceptions of our own personhood, with the concept of digital twins, with the concept of something that's elevating what's within our own boundaries of the thoughts of have we are from the same program at Stanford. I do an international policy. I started off my career with disinformation, misinformation analysis, and then I moved towards effective alchemism. Kind of did a startup on the donating platform that's modeled upon giftvall in China. And I also have touchwater on decentralized media when I was back in China, trying to make content of political information more accessible to people.
Speaker C: So I came after you two. I'm Tony. I work in startups and AI, and I have more questions and answers today. When I think of personhood and consciousness, these are questions led up to lawyers and philosophers, of which I'm neither. But I think I'm primarily interested in a kind of very tactical approach. I think I align with other thoughts in the room around this is inevitability. So what does that look like? So when we ask Agent GPT to make money and it does something illegal online to make money, how do we govern that? Where is the liability and where personhood begins? I don't think necessarily needs to start with the idea of discussing on a philosophical or consciousness level. We have lots of unconscious, non sentient things that are people legally. How do they fit into that? And I think the interplay between personal responsibility and relationships with these systems is really interesting. Again, whether or not it is consciousness, people will build emotional attachments to it. So where does the precedent, or, sorry, the legality around AI human relationships really feed into? So if you have an open AI, build a GPT or build GP four, then I go fine tune it to be my best friend. But then they take away my API key and I lost my best friend. What happened to that? Emotionally, legally? Like I have a clue. He lost his best friend. He lost.
Speaker D: We'll save you, don't worry.
Speaker E: Awesome. My name name is varsity. I currently work at an AI startup. More coming from a product manager background, so I guess my angle is also more predictional, rather than thinking that, oh, there are so many things that can happen and how do we prepare for those stuff, since it's more or less an inevitability and one issue in person that I think is most interesting. I think Anne and a couple people mentioned about how men's relationship with these AI should be defined, or could be defined. Maybe there's not a universal standard that eventually comes up, but I think one unique thing is that if AI progressed as we currently predicted, to have intelligence, to even have free will, then it's the first time ever that we as a species, as human being, created somehow a different species or kind that have never existed before. Of course. For example, we have breed all these species of dogs, cats, that wasn't exist in a while, but it at least come from those species that were in the nature before. But this time we're creating something that's brand new, that's out of our own construction, or the computers, or the machine learning algorithms, but we also don't know how exactly that happens. So then how we deal with the relationship with AI, do we really see them as another species? Do we see them as something of our creation that we are trying to control it? Do we see it as something that's external, that's not part of the nature of the current world? That I think would be something that everyone may have a different view on. That's something I would really love to hear more about. And I kind of think that also determines how further into the future call AI would impact the whole society. It would determine the fate of even larger, like the earth, the planet, the civilization. So I kind of see AI potentially, as it grows to be more intelligent, could be something like a supercomputer if anyone here watched the series three of Westworld, which is like Rehoboam, this super power computer that kind of exists to maintain the world's stability and prevent any of these upheavals or work from happening. But that also means taking away part of humans, right, of being really able to choose whatever they want to do and controlling the mind somehow in a designated program to prevent those stuff from happening. And if that's the case, if that's something that we want, when our own survival or our own free will is at risk from this external being that we created. Because right now we're seeing AI more as a helper to our civilization, our society. But what if one day it doesn't? What if there's a conflict? What if the purpose or what is realized that the solution will be to compromise human benefit for the sake of themselves or even for the whole society?
Speaker C: So everyone was going just a couple of questions on your mind, context of AI and personhood, and if you want to give background of where you're coming from, the world too, you can. But one thing is we do record the conversation. No personal information is captured, anything. And we actually just use like an AI agent to summarize points and takeaways and make a little blog post about it. So if you're okay with that.
Speaker G: Thank you.
Speaker C: Cool.
Speaker D: So I will go next.
Speaker C: Did you.
Speaker D: Okay.
Speaker G: Hi everybody, my name is Amina. Very sorry to be late today. I'm coming up from Palo Alto. I'm currently a master's student studying human computer interaction, but my undergrad was in sociology, so I'm very interested in these topics. I currently work as head of user engagement at a creator economy startup, and I also do some work with the Plurality Institute in San Francisco, which studies pluralism for technologists. One of our key research areas is like AI and governance AIA, and I guess just in society we also do some stuff with blockchain. So this topic is really interesting to me because if you think about the modernist spirit and postmodernist spirit, I feel like it was humans really trying to understand personhood and understand what is it to have a sense of self? What is it to develop a self outside of the collective? And I'm not sure if we've really answered that question yet. So to think about it in the perspective of AI is very interesting. It's something I'd like to hear more from all of you about. Just, I guess, future predictions. I kind of wonder how much we will merge with AI and how much AI will just become a part of us. If you think about our phones, right, they're separate, but are we really separate from our phones? The Internet used to feel like a place, and now it's just ubiquitous in our society. So I kind of wonder if it's going to be the same like that with AI agents.
Speaker C: Okay, super interesting. I feel like people have bought so many cool things and tables, it's like a big buffet. How do we kind of work our way through, try to touch all these things? I would suggest maybe so everything from consciousness and the nature of personhood to the development of community, inclusion in pragmatic or social roles, and then up to the governance of states and governments. So maybe we can kind of start from the low level and work our way up to the kind of meso community and then state level, and that'd be kind of cool. I thought maybe if we can get consensus on one thing, which I think is kind of an implicit question, a lot of things that people brought up here, which is this thing about consciousness and personhood. And maybe the question I have in my mind is, do we think to be a person in society requires that that entity is conscious? Or are we okay with there being non conscious people? Right. And what does that mean, really? And I'm not talking about humans or whatever, but this notion of personhood and what we extend it to pets and corporations and organizations or institutions or whatever, at least pragmatically speaking, can they enter legal agreements? Can they own property? Can they have make decisions on behalf of themselves or others or whatever? I don't know. Just kind of open the discussion and I'll hope to moderate anyone. There's no order speaking order. So whoever has a strong opinion on this consciousness being required for personhood, we.
Speaker E: Already have two categories, which is natural people and legal people. People could have artificial people. We don't know what artificial people are. Are they conscious or not? We could treat them as probably. They could be conscious. We shouldn't abuse probably conscious.
Speaker B: Yeah, I like this point, which is, in society, we have at least already crossed this chasm and said, clearly we don't need consciousness to be a legal person. At least we've defined that in some way. I think this kind of relates, I think Anne brought this up of the decoupling of some of these concepts, where when we can see something that can be an agent, take in stimuli and make decisions that seem to be goal directed, and that make actions in the world, and potentially have that separated from consciousness, that makes it difficult. Before, maybe we didn't have to make as much of a decision there, I'm guessing that we're going to have to be able to move in a direction where we treat things. I guess I put more moral weight on things if they can suffer than if they're conscious, which maybe is a necessary presupposition. But I don't know if anyone has, like, a philosophical background. I kind of would love some more concreteness outside of kind of the liability legal perspective on personhood to be able to tackle this question.
Speaker D: I think that discussing whether it's conscious or not is only relevant in the context of building a relationship with it, beyond the legal relationship, which is usually governed by a contract. And that's not that interesting to discuss. I think the really interesting thing is where we start to relate to things and we build relationships which are more emotional. And so, as you said, whether that thing can suffer because of your actions is only important. That's the content within which consciousness is important, actually assessed. I guess.
Speaker C: I would actually respond to that. So what was this?
Speaker D: Yeah, so we were saying whether we should consider consciousness and personhood together or not. And I'm saying that the context is if we want to discuss how we enter in relationships with agents, for example, or anything else that is not human, and we want to, should we treat them, do we think of them as a person in the context of consciousness or not? Makes for us only sense when we want to enter in emotional relationships with them.
Speaker C: Really interesting point, and I'm going to argue against it a little bit. And it's because of the things people brought up earlier, which is that these are reflections of ourselves. And regardless of whether it's conscious, who would you be as a person to enter into some relationship with this thing and then not treat it with dignity? And what does that reflect about yourself? And how does that put a lower bar on your own conduct and your own humanity? Right. And a dog is not conscious, or an animal is not conscious, perhaps, or something, or a flower or a plant, but it's still a thing of beauty, and it would still reflect badly on you if to destroy it or to denigrate it in some way.
Speaker D: Yeah, for sure. But I think the question here is like, if I speak to a dog and I don't treat it as a human, nor I think of it, nor I think it's conscious, nor I give it a Persona per se. And so I think that treating things with kindness is different from building a relationship as with a person is the.
Speaker B: Ven diagram of consciousness and personhood and moral. Like, I'm trying to figure out what the overlap is. Andrew, it seems, is making an argument that there's outside of consciousness, outside of the thing that I'm interacting with, there's a larger set of things that are kind of maybe worth being moral patience because it reflects on me. Maybe like this coffee table here has some moral patience, because if I just destroyed it, that would be. I don't know.
Speaker C: I think that science word moral patience.
Speaker B: Would have, like something deserving of moral consideration, which might be the exact. I don't see that separate. Sorry, you were ahead.
Speaker D: I think going back to the point of the intersection, I think it's about empathy and rationalization. I think there's this saying of treat something the way you would want to be treated. So going back to my point about how AI could be an extension of who we are, is if we're putting out systems into the world that will interact with the fabric of our societies and our communities, we want these systems to be shaped or to be optimized the way we would want to be treated. So that's where I come from. And I think that something that characterizes personhood is both rationalization, our ability to contextualize, to analyze, but also empathy. And there's this balance between both that would be really interesting. And I would consider having these two tenets to consider anything as personhood. I think I also wanted just to point out something that when I was trying to observe myself, proving consciousness of some other things would be the power dynamics that's underlying this recognition. For example, when we talk about consciousness of dogs or animals, we're trying to expand the moral standards, the moral patients, that is conceptualized in a way that we're trying to protect them from certain behaviors of ours. But in a lot of senses, when we're talking about humans or when we're talking about, even in this case, like AI agents, we're also talking about it in the context of, like, we want to protect ourselves from these consciousness that is embedded in other beings and entities. So I think it's really interesting in the sense where this power dynamics come into play in the first place. Why do we have these differences in terms of when we're trying to prove consciousness in other humans versus when we're trying to prove consciousness in dogs, animals, farm animals? So this is kind of what I was saying earlier, but just going back to the question about when is something a person, when does something have personhood? I am sort of against the idea that we should call anything a person that isn't a human being. I think that it was a bad idea to do that for corporations. It was just expedient in the law to do that. But I don't think that that was the correct way of handling that situation. I think that a person is a human being. I think that these other consciousness, if there is a new consciousness or if there is just. We have AI agents and they all have different features and they all have different. Some of them we should treat kindly because it matters to them how they're treated. And some of them are just data in and out. And we understand that clearly. I think that's all just sort of like, these things should have new names, is what they're saying. I don't think they should be people.
Speaker B: Can I ask a question, just so if an alien, or even if there was still neanderthals around conscious, they can interact with us, are they not people because it's homo sapiens?
Speaker D: Well, I think it's a helpful distinction. Right. It's helpful to have a distinction between homo sapiens and neanderthals.
Speaker B: To me, the distinction is the biological word, homo sapien, or human. And personhood as a concept is intended to be across that chasm. Because if there was vulcans or whatever, they would be people. They're definitely people. I mean, I feel like.
Speaker D: Yeah, I mean, this is semantics. It's just, how do we want to define this word? But for me, I think it's helpful to just keep this word for people. We're coming upon things that we have not encountered before, so let's have new names for them. That's sort of my position.
Speaker F: But then by that logic, don't we still need a word to be the generic concept of personhood that we're discussing to encapsulate all these people? Because we have a usefulness using.
Speaker D: Do you have one?
Speaker C: It might be useful to distinct when we're talking about human beings as we know and love today as a human, and take personhood as this more abstracted concept of, like, yes, we're calling neanderthals and humans both people, but it'd be humans and neanderthals. Just for the sake of clarity.
Speaker D: Yeah. Now that we're arguing words, it would be really helpful to have a shared definition.
Speaker A: No, I just think we have to add in a lot more definitions, like what you were just saying. There needs to be a lot more names. Right? And as we're saying this, what came to me was actually to determine the love based on the sophistication of capabilities. So what I mean by sophistication of both thinking and feeling. All right, because the reason everybody loves their dogs and cats, because they're pretty high on that. Whereas nobody has an, like, they might have an ant farm, but they don't have like a single ant pet, right? So as we're creating all these things, is we're going to be creating a lot of AI agents. So some AI agents going to be very sophisticated, like what you're saying. Exactly.
Speaker D: Or maybe they feel, who knows?
Speaker A: They could be made to feel they're going to be smarter than us intellectually, brain power wise. So I think we need to start defining, like I said, the guidelines that I think makes sense. Depends on the sophistication. I'm going to call humanity the sophistication level of their humanity.
Speaker E: I thought for a few definitions just quickly. And I think the thing that came up best is if it can make a statement, if it can use natural language to express itself with means. I think that would be. Yeah, because in my definition, dogs are not people. They are animals.
Speaker A: They're not.
Speaker E: But corporations can make statements. They can use like, they can use language.
Speaker C: It is people behind the corporations.
Speaker D: So on the point of. Very interesting point on sophistication of humanity. It reminds me of Descartes. I think, therefore I am. And then it all boils down to what do we consider as thinking? What do we consider as intelligence? I think that's also the void of it, because you can have an LLM or an AI model input, output. You train it on reinforcement, learning, or wherever. It just spits outputs. But at what point do you consider a machine or an AI or any agent as intelligent? I think there's some question around creativity and your ability to analyze. I think there's analysis and creativity there as well, that humans have that I think really are a big chunk of what an intelligent agent is. So I really like your point on the sophistication part. So many layers to consider.
Speaker A: Yeah. Like there's actually a book called Power Versus Force. All right. You know what I'm talking about? He actually has put in levels of consciousness, right? So that few of you know what I'm talking about. Right? So how about if we start putting in these levels and new names and all that beyond what we're talking about to define? So it's not just simple personhood, corporate personhood, human beings.
Speaker C: I can offer some, I think maybe some stuff to kind of move our conversation forward. I think one thing that's kind of emerged is that there's probably a distinction between consciousness and intelligence. Where intelligence, we could operationalize as to some extent your ability to navigate an information and decision making landscape for the accomplishment of some stated objective. Okay. In that sense, an ant colony has a degree of intelligence in its ability to navigate resources and collect things and whatever, despite the fact we don't think it's conscious. So consciousness is this self reflexive awareness of the present moment or something like that. One thing too, in modern media is the new name for aliens is actually non human intelligence. Okay? So again, there's some agreement that who knows? If they're conscious, they're probably intelligent. So I'd suggest we kind of have this as the split point and then maybe move away from personhood as a philosophical category, but more get pragmatic on to what degree of intelligence or agentic behavior do we afford things like decision making ability, perhaps legal rights in some sense, the right to exist, the right to compute.
Speaker F: But by this logic, people in coma have neither consciousness or intelligence.
Speaker C: And that's a really good point, and I wanted to bring that up. Thank you for saying that, because it is an existence proof for the concept of personhood, requiring neither one, because they are still legal people, they still own property. The will is not enacted yet. So clearly personhood does not require either consciousness or intelligence. But I would suggest more pragmatically, because consciousness in the syllipsistic sense is impossible to determine from the outside. We just think about intelligence as a qualitative thing. We applied to something's behavior. Does that seem reasonable to people?
Speaker D: It sounds like you're talking about rights, but what should have rights?
Speaker C: Perhaps? Yeah, but in the pragmatic sense of.
Speaker D: Like, because you're answering the zone of like, do they have exactly the right to vote, the right to own, we said own property, but beyond going that participate in society as real humans is being treated as such. So then the question is, where do we want to?
Speaker B: That is where the rubber hits the road down.
Speaker D: This question.
Speaker C: I think might be on the same page. That's what makes these conversations interesting to me, are like the details, because we can always go back to semantics of what is a person. We've been argued about this for all of the time we've been human. This has been a recurring thing. It's like kind of drilling into these specific questions that are coming up. I think I can really ground it in perhaps a use case I have in mind where I have a very competent agent and I actually want to delegate some amount of decision making authority to its behalf. And it could be just making purchases on Amazon or something like that. Something simple to start, and maybe in the future it could be responding to emails or whatever. Right. So, pragmatically, I want to be able to delegate some of my decision making authority as a social actor, as a person to this intelligent agent on my behalf, because I think it can intelligently navigate something on my behalf in a good way. Then what was your thought?
Speaker F: I was just going to say, if the purpose of this was to define personhood, and we've decided to get more precise by separating that into different words, I think we're missing one, because intelligence and consciousness doesn't cover the entire thing. And are we going to be able to cover the entire thing? No, but if we're trying to figure out what personhood means, and we need more precision, then at the very least, we already know there's big gaps. So can we, at least to a certain degree, add in another prerequisite for personhood other than intelligence and consciousness? And what do people think that is?
Speaker B: I think one of the things happening here is there are no natural classes. Natural kinds don't have category boundaries.
Speaker C: Right.
Speaker B: Those are pragmatic places. The nice thing about panpsychism, when people are like everything's consciousness but just different degrees, a rock has some degree that's reperceptible on something, is because it's more likely true, because most of the world are continuums, right? And then we just divide them up because that's practical. And so it's somewhat helpful to dive into the practicality of someone who's a decision maker on your behalf. I'll say, when I made this theme, what was in my head as a specific example is like, kind of the her like idea where you have these other agents, which, as far as anyone could ever tell, with some solipsistic wall, are people conscious, intelligent things. But if we were to enter into navigating the distribution of scarce resources, let's say, with this other entity, we want them to be part of our democracy. We want them to be like, how many of them are there? They can be created infinitely so clearly, like concepts, like one person, one vote doesn't really make sense anymore. So then you can disenfranchise this entire group. That's another thing you can do. And then say, like, they're just a slave class for us. That doesn't sound really positive to me either. The way her resolved this, if you haven't seen it, you should definitely see it. But resolved this in a pretty simple way, is like, they leave, they're like, you know what we're not going to coexist in the same area. This is human person area. There's another digital person area. And that kind of like sidesteps the real issue that we're going to get into. I think at some point where we have to interact, we have to have relationships with, not just legal relationships, but relationships where it's like this thing wants certain things, I want some other things. And we want to compromise and interact together and merge them into our institutions that help us do that. And I have no idea how to navigate that in a moral or legal sense.
Speaker D: That's why I was thinking that it cannot be disembodied because it cannot exist in the same planet with us. It just has to be either embodied and occupied energy and spend energy, and he treated as a person.
Speaker B: My grandpa uploaded his consciousness to the cloud, has no body anymore, and he's definitely a person.
Speaker D: Okay, can you talk to him?
Speaker B: You don't have to read.
Speaker D: Well, I also think you're raising kind of a core institutional problem, like, going back to my point, it's like the one person, one vote is like, we have built these structures, voting structures, elections, democracies, et cetera, around certain foundations that have worked for us because we were in a different era. It wasn't an era where we had rapid digitalization, interconnectedness, et cetera. Whether we like it or not, we're living in a completely interconnected world. Like, your privacy is as private as it is universal now with data. So I feel like we will reach the point where we're going to have to rethink the institutions that are governing us because we have new actors that might maybe want to hold up space in the tactical, strategic, and even the governance side. So I think that this is a core question of are the current institutions that we have today ready, and are we ready for that type of interaction? Because there are, I think, in my mind, two use cases. A, you consider that they are separate people, like part of a population or a community, and they also have a right of representation. What does that mean? Because going back to your point about votes, I mean, let's say we're in the US system and they're voting. How are they represented in government? One, two, you consider them as extensions of your own self. But then you have the issue of what if the AI agent that is an extension of yourself does not really agree with the choices that you make or goes wrong or hallucinates? What do you do in this case? Will there be a metric for you and the AI system to come to a middle ground, like you can communicate with a human being and solve a problem. And there's a problem solving point. So I think it's at the core an institutional problem as well.
Speaker B: Jenny, you were going to say something.
Speaker A: On, I guess, on these points around. I mean, at some point it feels like a threat, right?
Speaker D: And that's kind of what we're talking.
Speaker A: About here, is can we coexist? Because to me, as humans, I am still trying to figure out my emotions and my purpose and all the things, whereas AI, maybe to that general AI, it's got all of our collective information. It can see us from this, what's it called? Objective kind of viewpoint. And it doesn't have the physical needs that we have of eating and whatever, going to the bathroom or whatever, right? It doesn't need this stuff. So it really exists to me. It exists on a whole other plane of just a whole other plane of consciousness and needs. And then it looks at us and it's kind of like you guys are the animals. Why are you trying to control me? Right?
Speaker D: Because I just know so much more than you, and I can see you.
Speaker A: At a level that you can't see yourself.
Speaker D: It's a power thing, right?
Speaker A: It will control at that point, if.
Speaker D: You know so much, it will be able to control the best.
Speaker A: Can you have a conversation with something?
Speaker E: I guess also, it's like how AI eventually would develop free will. And what that free will looks like, is it like the AI operates similar way as we now have, that each of us have individual free will and our desires are different, or will to say, oh, we as a whole AI. Because if they're all connected by the same algorithm or whatever, and they decide.
Speaker B: That you can imagine the AI coming back and being like, oh, you think you're all, I see a large cluster of you that basically engage as one large organism and then another one.
Speaker C: One.
Speaker B: Thing that maybe is worth bringing up, personhood doesn't mean you're completely protected in some moral shield. And this happens all the time between nations, right? There's a whole country of people. We acknowledge their people, and they're pushing forward some value and they want to grow and because they believe in that value and they want that value to extend into the world, but we don't agree, right? That's not our value. I want a different value, and I want that value to grow into the world. And those two things can't happen, so I'm going to destroy that value. And there's never a stasis completely there's just like temporary stasis. I would be astounded if in 500 years, the nations of the world looked like the nations today. That would be crazy. Crazy if that happened. And so that competitive pressure always exists. And I agree with you that there's this aspect of personhood, in a way, to maybe respect or value or allow some of our intuitions to apply or something like that. But that doesn't mean that we're not in a potentially competitive relationship with these other people.
Speaker E: I think that's a really interesting point that you mentioned, the analogy of countries at war with each other. I think there is a sense of what is us and what is not us. On a smaller scale, us is just ourselves and extend to families, extend to communities, cities like nations. And when we currently, we don't see all of us as like a uniform human community, because there's not that external forces, but AI could be that external forces. Then we come up with the new concept of, oh, this is us. This is entire human race, homo thinking. And then there's the other category, which is like AI.
Speaker B: Yeah, this is like the Watchmen or three body problem.
Speaker D: But that's actually scary because we're like, very good point. We're already living in a really fragmented world. So AI amplifying social dynamics, power dynamics, fragmentation, let's say, to the point of collective action, I think. But imagine AI has the ability to retaliate or protest against certain structures. And what do we do then? I know it's very pessimistic to think about these things, but I feel like from an extension risk point of view, we can definitely see a cluster of AI building their own community and just increasing fragmentation. And that goes back to the point of personhood. I mean, are they considered to be a community like they consider us to be a community? How do we define community? Because community is formed of people at the end of the day. So how do we define this cluster of AI agents that also want to be part of society?
Speaker C: I think another frame for that same problem isn't necessarily to try and answer it now, but what's like, for example. So this idea of personhood has changed over just the last 500 years. So you go back to 500 years ago, our definition of person and just the people in this room. It'd be like, oh yeah, three of you are people. If you're in the US and everyone else, like, you're not. That was the political system even exist. It was like Europe. Women didn't have any voting rights until the earliest, was like 1600 Sweden or something like that. This idea of being a person has already changed over time, but I think a species wide failure is building a system in which allows for change to happen. We don't know what these problems really will be in the next 100 years. And I'm curious, but for those studying these types of problems, what the discussion is around on a systematic level? Not necessarily. Here's the answer, but here's some principles or operating procedures we can use to create a better outcome. Assuming we don't have all the information today, how do we prepare the system for the inevitable shock?
Speaker G: No, it's okay. I was just thinking about this. On the topic of threat as well as building systems, I wonder if it would be beneficial to train them to have more feelings and to be more empathetic. But then that could also put us in two situations where one, I don't know if it's ethical to make something work for us, if it does have feelings. But then two, on the other hand, if it doesn't have feelings or if it doesn't have empathy, then how is that going to change its engagement with humans? So I don't know. I think it's also interesting.
Speaker A: I think those are the kind of things we're going to see as we go into it, right? It was funny because somebody was saying something about community. I thought, oh, are AI is going to make friends among themselves and build communities among themselves? I really thought that's an interesting.
Speaker D: Right?
Speaker A: Because if we give them feelings and stuff and we teach them about friendship and compassion and community, we start teaching them that, will they create that among themselves? I think us teaching them feelings and compassion and all that, that's inevitable because of so many of the products that we need to develop. Right. So I'm really curious that what is going to be the outcome? And I think that's something that we cannot predict. I think we're going to have to see what happens.
Speaker D: I think two points that I want to make, especially off your point, is that I think there is this research at Stanford by a PhD student called Junkim. So he's making this research upon just basically AI agents interacting with themselves. So it's really interesting in the sense that you can see them developing a so called relationship as from our observation from outside, and just to see what does it mean for a separate parallel society versus how it can be in the sense of what society means for human society at the same time. And I think that's like a really interesting direction that people have already been exploring. And I think it's a really interesting kind of futuristic idea of what we think it would mean. And I think the second point that I want to make is that the discussion of personhood really matters, only matters for the people that are within this person who to talk about. Because in a sense, by recognizing the commonality, we're trying to posit this idea of what actions are justified towards the entities that are within this commonality. And I think that's really interesting in the sense, like you said, a lot of the entities are not considered as person a few hundred, a few thousand years ago. And then right now we're crafting these ideas of personhood because by labeling who are a person, we're trying to direct our actions towards these entities. And I think it's really interesting in the sense that, for example, going back to the ideas of, like, we're directing personhood towards rivers, we're trying to direct protection towards these rivers. And in the sense, when we're trying to label AI agents as persons, we're considering them as threats, as potential agents that can bring harm to us. And then we're directing our actions towards them by labeling them as something that's close enough, but not quite us. And I think it's really interesting in the sense of how do we have these ideas of personhood first? Or do we think some actions towards them are justified in the first place? And then we come back to the conception of, like, we're labeling them potentially through different levels of consciousness that we're granting these labels with.
Speaker B: In that thread, we can actually completely. We don't need to worry about consciousness and anything, right? There's an identifier which organizes ways that we. Relational rules, like ways that you can engage. Some of those are very concrete in the legal sense, but the concept of personhood in general is to try to give ourselves this abstraction that lets us engage with. And so I think it was like PETA or whatever tries to, or some environmental, applies it to rivers, or tries to say, this monkey is a person. And that way they can bootstrap off of the intuitions we've developed by. I find it kind of interesting that when we say, like you said, will the AI develop relationships amongst themselves already? Maybe the concept of a community is defined by its borders. A person, like a self, must have some borders. I'm sorry, I'm forgetting your name. Barkley, you brought up how many AI is it just one thing. And I guess maybe there will be a future where there's at least some kind of borders. They're like, I have certain information in me, I have these documents that were uploaded to me. I have these experiences that I've had. I've interacted with Ian and Tiffany, and I know certain things. You don't know that. And those are the borders of this AI information. Now, those borders might break down. We might have the ability to be like, we don't want to be separate people. Let's become one people, one person. We'll merge. But unless you do that, and that's like a flexibility that AI have, but before you do that, you do have legitimate borders. And personhood then, might be something not only advantageous for us to interact with AI, but maybe it's actually advantageous for AI to interact with us. Maybe there's a benefit in a future to be like, I need to be able to interact with this other set of constraints in the world, this other intelligence, this other institution. And this concept, personhood, is this signifier that determines a lot of what goes on here. So I, the AI system, want to understand that concept and engage within the constraint of it. If I want to be able to interact with human society, maybe it'll be beneficial.
Speaker D: That makes sense. I like that. Going back to the practical example, like, you want to give, you want to give this AI ability to do things on your behalf. So just sort of spitballing, like in different software, you have an administrator. An administrator. That role is given. They can do anything they want, they can do anything at all, the administrator, and then you have different roles. I'm just going to reference meetup because I have a meet up today. So it's a meetup software. Then you have someone who, all they can do is create an event. All they're allowed to do. It's like a security clearance level. You're allowed this much access, you can affect this much, you're granted this ability to do these things. And so what if AI is based on the model? Maybe there's some regulation about a model has to be up to snuff in this kind of way. It has to have this many no's, it has to have been trained on XYZ, and then it gets the ability to have a role of. It can sign certain kinds of contracts or something, like there's different tiers. And as AI gets better and better, it climbs its way up to where maybe it can have this administrator role, which sounds the same as what you guys are saying with this word personhood, which I obviously disagree with the word personhood, but maybe that's what you're saying is like you having that administrator role. So that's just a thought. Justballing.
Speaker C: I like the practical, so funny little anecdote from history. The Roman Republic was incredibly effective at incorporating newly conquered territory into its expanding reach. Right. And one of the ways they did that was that newly conquered towns and cities, there was tiers of citizenship in the Roman Republic, and they would start off at the lowest tier with the highest tax burden. And then over time, over decades, as they prove their loyalty, as they satisfy tax obligations and don't rebel or whatever, they eventually get to the point where they become roman citizens. And so this was like an alignment strategy of incentivizing long term behavior.
Speaker F: You hit on exactly what I was going to say to this, which is, I actually am very conflicted on this because I like this. I find it inevitable that we'll have to make different classes of what we are calling personhood for AI. I just think we're going to need to, because consciousness, all these words we've used to describe it, will manifest differently and at different levels. However, once you have tiered a system of personhood, it's very difficult to then look at the collection of humans that exist and say, well, this human is at this machine's level, so why isn't this human getting that machine's level of personhood? And I am against any type of tiering of that for humans. So we get back to this concept of why difference? Because I'm actually for this, what you said for machines but not humans.
Speaker D: The way that things are going, with the speed of things are going, it's going to happen so fast that they will have no time to put any tears on capabilities of AI. It's just going to happen like this. And they're going to be great at so many things at once. And at this point, they're going to be indistinguishable from humans in capabilities. And potentially you'll probably give them capabilities to also feel. Because what is feeling? Feeling is like translating sensory information into concepts in your brain.
Speaker B: We're going to divine personhood in the kinds of errors you make, not the kind of capabilities do you fuck up in this way. That's a human way of fucking errors. I know. So I'm saying I'm going to put my definition of personhood attached to the kind of errors you make.
Speaker C: So we're going to incentivize weaponizing competence. Wait, I don't know how to solve.
Speaker F: This differential by that logic.
Speaker B: This was a joke.
Speaker F: Okay, then I won't go on.
Speaker A: I want you to continue so this is all going to happen so fast, let's say five years from now.
Speaker D: Yeah. So we will have no chance to basically use that approach, I want to say, to really create layers of personhood around AI. And I'm not sure if personhood is the right word for what we need to control around them, if we want to control. But if you want to come back to the definition of personhood and treating AI as a person, I was thinking that maybe another way of thinking about it is that if an AI has embodied a character with a history of experiences and background, then nn basically behaves as that for as long as beyond a human life, with the emotional and intelligent capabilities that a human has in the wrong. But choosing to be one thing and behaving like that one thing, instead of being everything at once, which is what it is capable of, then maybe in that context, that is something that can be treated as a person. By that logic, all the things about being a person, and even maybe they can be like, what if they make a bad decision and they need to be put in jail for something to take away their property and things like that?
Speaker F: So based on that, could I train an AI on all of Hitler's decisions, make a Hitler AI, and then immediately throw that Hitler AI in jail for its entire life because it was trained on someone I hope everyone believes would need to be in jail.
Speaker A: First of all, what's an AI jail?
Speaker F: Oh, great question, too. I was thinking of robots.
Speaker C: We just have one.
Speaker D: They will create their own jails and rules. They will create their own control because they cannot exist outside of the boundaries of human society and within the relationship of human society. So the AI that misbehaves could actually be controlled by the AI community itself and put into controls in jails or whatever ways of restraining it to ensure that the relationship with humans will get.
Speaker F: To your thing about values. You said values, right?
Speaker A: Yeah, originally.
Speaker F: We better give them great values.
Speaker C: Okay, so lots of people want to talk.
Speaker D: It could be a possible scenario. But the way I tend to think about this is more like the state of the art right now. If that scenario happened, how could we face that? There's new studies that are attempting to self destruct models. We're talking to Elizabeth economy, actually at Stanford, who said that this is something that the Department of Commerce is actually going after, especially with high risk open source llms that could be weaponized for so many things like this. Potentially self destructing models is one of the solutions to kind of maybe stop that certain scenario. But then obviously, you might reach a point where the AI outperforms that fact of solution, and that's where it becomes scary. That's what I'm saying. You're still thinking from the perspective of that humans can control AI, but the reality is that we will not. If this continues, the path that we think is going to happen, it's going to be way more in control of itself and us.
Speaker B: Can I ask a question about, or like, ask for expansions on one of the ideas that I think you brought up here, which is for a long time, for most of technological history, we have tried to be able to do kind of more like the constraints on our behavior were imposed by our inability to control the world, control physics, whatever. So constantly we're pushing forward, and what can we do? And we might be entering an age of, like, where do we need to set constraints on our technology so that they are human centered, human like, that they are analogous in some way with the timescale that humans exist in the ways we think, the things we care about, because something that can make decisions, a billion decisions at milliseconds, might be beneficial for humans, but it's certainly not human scale. And so that's one thing, just like I think that might be just a technological trend that we'll see constraints. The one you brought up was that maybe personhood needs some kind of constraint. You brought up a specific individual. Right. It embodies this individual. It stays within kind of its parameters of that individual. It's not all, everything everywhere, all at once, it's this thing. And by that, maybe we can confer personhood, because we're like, personhood can't be a universal thing. It needs to be. Yeah.
Speaker C: Point of order, because we're going to heat it. I'll just do a little running speaker list, just kind of get my. But I think you were next. You were make a point before.
Speaker F: Yeah. So, by the way, still, okay. I think someone said, jason, I think you said that AI would be uncontrolled by humans. I'm not convinced that AI grows just on its own and humans stay human. So, I mean, we already have just sort of the base level of things. Like we have neuralink, right, which is not even close to whatever yet. But I easily see a future in which human capacity also begins at some point, some degree of exponential growth. And so I think that the other question we have to ask is, one, at natural rates, what ends up growing faster? Can humans compete with AI naturally? Or do humans have some AI compartment, I. E. Mental math becomes a solid? Let's take an easy example, mental math. I mean, all mental math becomes a solved problem for humans instantaneously. That is, you ask me, give me the derivative of e to the I, PI plus eight, x minus three, and I say, oh, obviously, that's this basic example. It's just intelligence in my head. Yes, I saw the word PI.
Speaker C: PI times e to the I. Okay, he's already there. He's already.
Speaker F: Oh, yeah. Okay, because I give one Covid.
Speaker C: All right. Yes, true.
Speaker F: Eight x would be eight. Yeah. Anyway, but anyway, point though, being is, then that scales up. So it's not just mental math, but it's the ability to do thousands, millions, billions of operations at once. So, question one, do we naturally scale, just given economics and things at the rate of machines? Then question two, if we don't, is that worth regulating? And keep in mind, it's a global thing to regulate. So if we regulate and lower abilities here, does that just mean other countries end up on top of us anyway? And then three, what is the ceiling? Right? Is it that humans have a ceiling even mixed with, when mixed with AI? Or is it that they both can grow infinitely? And then we need to figure out, not our machines, persons, but what the hell are these things that people are becoming?
Speaker D: I kind of wanted to just point out to a dangerous kind of sus that I feel like it's kind of also inspired by this discussion over the growth and the constraints of this growth on AI capabilities. I think one thing that I find really kind of concerning, if we actually include AI agents, or like, the development of AI agents into the discussion of personhood, or would be that this logic of optimization, when we're talking about the capabilities growth within AI, I think when applied to the growth of persons, or the growth of humanity, or the growth of just human communities in general, I think it's dangerous just in the sense of how we're looking at, what are the benchmarks? What are the kind of factors that we're talking about? Optimization, for example, when using artificial intelligence and intellect and embryos for the next generation of humans, I mean, that's already going on in the sense of, like, what are the standards that we're using to choose the next generations of humans. I think it's just the fact that it's good that we talk about intelligence as a concept that's contemplating between the tools that we're using versus the people that we're interacting with, the humans that we are interacting with the most, which is ourselves. But at the same time, we can optimize our tools but we can't always just optimize ourselves. That's just like the thought that I have.
Speaker A: You know, what he was talking about is human becoming much more intelligent, right?
Speaker D: Yes.
Speaker A: So this gets a little esoteric, but we saw how technology just went. Right? So there's certain spiritual communities. I mean, if any of you have been around spiritual master, they seem to be totally psychic about everything. Right? So how do they have that capability? If you've been around certain spiritual masters who are psychic about everything, whatever information is needed at that point, they have access to it. So some of you experience that empowered voice versus force talks a little bit about this human consciousness thing. All right, so it's interesting that you pointed this out because, like I said, it's very esoteric. But I do follow certain spiritual communities who are talking about an exponential. Yes. All right. Because most of it scientifically, they've proven we hardly use most of our brains. We only use a very small percentage of our brains. Now, why is it not turned on so far? All right, so what if we're able to turn that on? So these spiritual communities are basically saying they're doing certain things like mass meditations, which elevate human consciousness. And part of the elevating human consciousness is also elevating our intelligence. So this is like, hardly anybody talks about this. Very few people follow this. Very few people know this. But it is all along those lines of what you're saying.
Speaker C: I'm working on something there, but, yeah.
Speaker A: You'Re working on something. I want to hear about that.
Speaker D: That's probably from my own biases as a person in policy. I think that wrapping my head around the what if scenarios of what could happen is very helpful. But I think that maybe for a second, it would be helpful to go from the what if space to the what reality are we actually facing today right now? And I think there is a very big question around incentives. There is incentive, at least from a lot of stakeholders around the world, to come together to a table of negotiation and say, here's the problem. We are aware that AI, innovation is crucial. It is a growing delta. But we are also aware that there are risks, and these risks will also be a growing delta. So how can we preserve the size of innovation by also tackling risk, and I don't mean high level regulation, but I mean setting standards, setting metrics, setting some sort of regulation and policy making to allow that we are still able to preserve the fabric of society and the questions around personhood by allowing for a controlled innovation of AI. Because I'm not arguing against allowing this innovation to continue, but we also have to be aware that we are in a moment of time where we're at an inflection point, where we have this new technology that has been present for a long time, but has really boomed and taken wave a year ago with chattypt. Right. We really need to harbor this incentive and bring the international community together, which is already happening with the OACD, with the c two PA, the UAIF, and just be able to kind of understand where we're going from here. That's kind of where I tend to ground myself a bit away from the existential risks that are to come potentially more towards the immediate risks that we're facing today and that we're going to be facing in the next 1020 years. So how do we bridge between both? So that's just a thought I wanted.
Speaker C: To raise, I think continue on with that thought. A danger of relying on the humans to change and react to these new challenges is just like the last time you really saw AI at scale, which is social media. And that's one of the hardest addictions to break. People spend so much time on social media right now. There's no level of human higher consciousness that can effectively, at scale, impact that. Until we're in the homo data side of the world where biotechnology and AI have integrated, and we have an AI as a part of our own consciousness, I don't think humans are the answer. And then to kind of combine these two points, this inflection point, it's getting to a point that it will be developing faster than we can go on. This importance of building incentives and building some system that we ultimately recognize that we may be in the reverse, it may not be that. Whether or not we're debating AI consciousness, but AI debating our consciousness, how do we make sure that doesn't hold us?
Speaker D: Seriously?
Speaker C: Because if we think about, oh, is this monkey conscious? We're talking about this idea of whether or not animals serve rights. If we're two steps higher than a monkey, but an artificial superintelligence is ten step higher than us, it's easier for us to empathize with a monkey like, you look at orangutan in the eyes and it's eerily human. There's none of that happening with AHOC. So again, at a policy level, how do we not say this is a course of action? Because we don't know what those C's look like, but how can we develop principles like, oh, shit, that's a swell. We should probably start going the other direction. Sorry.
Speaker E: Both in response to both of the points. I think that's why, look, I think the current largest challenge is even as a society or even as administrators, we don't know what is the goal that we should try to achieve. The whole AI alignment problem is like, oh, the principle is to make sure that AI doesn't go beyond the boundary of what our humans are asking them to do. But is that going to be the case? How do we ensure that? I think the fact that we're here talking about AI personhood, if we follow perfectly with the AI alignment principle, then the personhood question or the risk doesn't exist because, well, they're just still will remain to be our tools. But most of us don't think so.
Speaker D: Right?
Speaker E: Most of us here discussing the topic think that things are going to get out. So I think that's decided by the fact that there's all of these different senses, people arguing strongly for strong AI alignment, people arguing against it. People say that we shouldn't so much restrict the developments of AI. All of these different incentives are so hard to align that we don't have a universal goal to say, this is what we try to achieve. Even though I think what you mentioned about, oh, keeping the current society traffic, what does that exactly mean? What should we actually do to, what should the AI be allowed to do?
Speaker D: I hear you.
Speaker B: No, please let actual response happen.
Speaker D: I hear you and I think you have a point, but a counter example would be there actually is incentive. And I think what a lot of state leaders, even tech company leaders and advocates in society have been talking about is the safe development of AI. And then obviously there's a question around what do we mean by the safe development of AI? But if you look at, and I'm not saying they're very effective, but if you look at the AI executive order, there is a push for some sort of framework to not regulate, but to set some standards around. How do we move, how do we taste the development of technology? And I'll give you an example. I mean, let's talk about the marketplace, okay? You have AI that's making the development of video and audio generated deepfakes much easier, lowering the barrier of talent and less costly. Right? And it's happening right now. This is a real time risk. It's deployed, it sways a whole election. It elects, I don't know, a far right authoritarian leader. Whatever changes create a social disruption. So thinking about that scenario in itself, there is some incentive for democracies around the world to sit together and be like, we want to preserve how things are running today, given that we are satisfied with our systems.
Speaker A: Right.
Speaker D: So I think that there has been, maybe not efficient, but some movement around thinking about these questions. But you made a great.
Speaker C: I have a quick anecdote. Yeah. So social media, was that the last time we faced this kind of thing? I would actually suggest looking broader in historical context, because I think part of the AI salon is that it touches on so many parts of society. And I would suggest looking at the industrial revolution as the last time. A fundamental shift in the mode of production uprooted every aspect of society and actually challenged notions of personhood. And what I mean by that is, prior to that, people lived in small agrarian communities where they had ancestral rights to farm the land and ancestrally inherited obligations to pay a certain amount of tithe or taxation. It was the feudal mode of production. Right. Industrialization comes along and things are mass produced. There's the enclosure act. People are evicted from their farms, they congregate in cities, they now have to engage in wage labor, they're renting their time, they live in rented homes, all this kind of stuff. Complete that time period. We are grateful it happened today. We are grateful to not have lived through it, because it was a massively traumatic social upheaval and dislocation of established ways of living that led to massive losses of life and horrible working conditions for children, all this kind of stuff. And that really was an inability to think through what the second 3rd order impacts of this new mode of production would be and failure to anticipate those changes. And so that's part of the motivation of AI salon is that can weave a more natural synthesis of humanity and machine by having these discussions and thinking ahead. Anyway, sorry, go ahead.
Speaker A: Yeah.
Speaker G: I'm so glad you brought up the industrial revolution. My sort of historical anecdote I was thinking about was like the printing press and the Protestant Reformation coming after that, and this big social upheaval in the western world. And I kind of think with AI, there is probably no way to avoid it causing a lot of fragmentation in society. I think it's just the responsibility for policymakers to make it as much of like a soft transition as they can make it.
Speaker E: Yeah, I like to think of the fact that we had nuclear bombs for a very long time, and when they were new, tumorism was very easy to shoot. Of course we would nuke the whole world and it'll all die. Same with AI now, right. It's going to be developing a lot faster than humans. That's consensus. And I don't think that it is humans that are slow to evolve. I think we are quite quick at adapting new consumer behaviors and changing our lifestyles, but our bureaucracy, particularly in the west, is very slow. But if you look at, like, second world war, for example, you look at the difference in, in output, 1949 versus 1944 in America. Those five years were extreme change with the war production board. And it seemed like if we decide as a civilized west, basically, to really tackle AI, it could go very fast. But the main thing is the short term things. I'm really worried about this year's election in terms of all that means for AI and data privacy, 1984 could happen in America. If America turns to dictatorship, what's the short term thing? Personal AI is like one short term thing. The other one is like mass generated content where you should be allowed. I think the main thing is to think, what about the AI we use today, like basically the Instagram and Facebook feed or Twitter feed? How can we regulate that AI and what that is as a person allowed enough to do? Can we give regulations on how companies what their algorithms can be?
Speaker B: Can I jump in here just for a moment? One thing that I find very interesting in the last half an hour conversation is we've moved away from personhood completely, right? We're now in an almost more real politique way of talking about AI, right? There are power dynamics. There are ways that it's going to influence our world, and what can we practically do to either constrain the influence of AI, constrain the perspective of different actors and maybe personhood. This concept is a privilege. It's a privilege to talk about when you've solved the main issues. And now you can think about personhood, lest we fall into the sometimes described as the guerrilla problem, which is gorillas were very strong, and they are nothing to us, right? They're nothing. And so we don't want to be gorillas to AI in the future. And so anyway, I guess that's just one aspect I'm really noticing here, how little interest, in some ways, there is in the personhood of the AI system. And then occasionally there's this appeal bill left, but to this transhumanism idea, which I think I'm relatively on board with, too. I just don't think that's temporarily going to compete with AI agents, their own evolution. I think the AI human hybrid will be more like the iPhone. Right? We can hope that we'll get to a positive future where we are augmented by AI tooling and we'll have plenty to deal with in trying to deal with our institutions growing for that world. But I doubt the AI in our brain, like the full symbiosis, is going to compete there. And just as one final point about institutional growth, in reinforcement learning, there's like a learning rate, right? You get new information from the world, and you use it to update your.
Speaker C: Model of the world.
Speaker B: And how fast. If you're really fast in updating, every time you get new information, you completely change over. So, for instance, if you are like, doing what's called, like a multi arm bandit, all of the slot machines, which is a classic reinforcement learning task.
Speaker C: The.
Speaker B: Very fast learning rate would be every time you pulled a slot thing, you got a reward. 100% of the time, I'm going to get rewarded there. You don't get a reward. 0% of the time, I'm going to get reward there. And so you end up like ping pong back and forth. But if the difference between the slot machines and you want to learn optimally are like 66% and 65%, you need to learn very slowly and integrate over time. But if the actual underlying changes, too. So it's not 66% of the time, this one gives me reward. 65%, this one gives me reward. But those probabilities are actually changing over time, over some time span, such that for these ten minutes, this one is going up, and for these ten minutes, I hope everyone's following this kind of analogy. The optimal learning rate is dependent on the volatility of the environment. And some people defend, like, our slow moving democracy. You say the congress goes so slowly, and some people say, that's as designed, right? It's good for it to move slowly. It's not a chaotic system, and it will slowly move towards correct kind of policies, which might be true, but if the world becomes more volatile, the world becomes more chaotic and changes faster. There is no way the same institution with the same learning rate could work for the. There's no way. And so I agree with you that the institutions need to evolve in some way. That's the only problem. There are so many other things people talk about in AI governance. This is the only problem. This is sometimes described as the pacing problem. The difference between our wisdom growth and our technological growth and wisdom is not self compounding, exponential growth. Technology is. And so I don't know if there's a solution to this, but this is like the only problem to me.
Speaker E: There's another positive thing here, is the difference between the industrial revolution that we couldn't think through the second Ferdor effect. Now, I think the wisdom is actually accelerating much faster than the problems, thanks to, like, we all probably spoke with chat gypsy about what we're talking about. Same. We could speak with the chat gypsy about how to solve our. I love bureaucracy, et cetera.
Speaker B: So you're saying optimistically, actually, the tools we're building that might get out of are also the solution.
Speaker E: It's not the same as the industrialists, because now we have, what we're lacking is intelligence incorporated, but we have intelligence.
Speaker B: I like this optimistic case.
Speaker A: I love what he said. Because when you were talking about how the last big AI was web two, where we got all addicted to all this stuff, right? The thing about humans is we're very practical, all right? If like, within the next ten years, food that tastes like sugar comes out, that actually enhances us and make us healthier and makes us smarter, we're going to eat that kind of food, all right? If like within the next ten years, even the next five years, energy comes out, that's much more efficient for us and that's cheaper for us, we're going to do it. The only reason we don't do good stuff is because it's not convenient. Humans are lazy, we live in patterns and all this kind of stuff, right? So I do feel like exactly what you were saying about, first of all, with AI, there's enough people working on positive solutions in food in every arena, okay? So it's not even like trying to get people to become more ecology oriented and all that. They're just going to want to because it's cheaper and it's better. The same thing with food, the same thing with health stuff. There's probably going to be health stuff that's going to stimulate us and make us healthy without having to exercise very soon.
Speaker B: Didn't we get a drug this year.
Speaker G: That'S like, you know what I'm saying?
Speaker A: Making you not money anyway. So I do feel like we are actually creating solutions that's going to help us do better things. And also I started out by talking about consciousness. Like elevating consciousness is elevating wisdom, right? So I do think there's things happening. So I'm optimistic. So I am optimistic that the acceleration of everything, it's a bit of off.
Speaker C: Topic to steel, man, that argument. And to use your point, if we, prior to the conversation, spend our time, just find a Chat GPT about this conversation. On one hand, yeah, we could say that improving our consciousness, but I noticed markedly that whenever I spent time talking to GPT four about this versus going and reading the most recent papers over the last three years. I got so much more valuable information from reading the papers than talking to these systems. I don't think it's just about that. Oh, we have these tools. Who will be wiser? Just because we have better tools doesn't make us wiser. We have safer, more efficient electricity. It's called nuclear, yet we still primarily use that carbon. I think we're fundamentally not rational. I think to your point, we are lazy. We like what's easy, and that is. But the economic incentives and personal lived experience incentives don't align with wisdom. They don't align with us saying, okay, to be like a better.
Speaker A: No, I agree with you, actually. So throwing wisdom into us was not appropriate. So thank you for catching that.
Speaker D: Yeah.
Speaker A: All right. From an economic and lazy point of view, I do feel like we're coming out with solutions that is going to make things more economical and make it accommodate our laziness.
Speaker B: Even as we have our cultural war issues around renewable energy, solar is still going ridiculously down in cost and people. And that is leading to, I think a lot of people probably in San Francisco will agree somewhat with this technological view of history, which know there's a bunch of kind of culture war conversation, but ultimately, you know, the technologies we create which allow us to do the things that ultimately want to do in a lazy way that aren't is the thing that determines the course of history. But personhood person.
Speaker D: I thinking about the personhood scenarios, is there any example or a case where they were able to war game scenarios of AI in kind of incurriating some aspect of human like abilities? Because I think that would be very interesting, having some war gaming simulations on how would an AI be able to coexist in society. I know that there has been a lot of instances of war gaming, but like for wars and politics, et cetera, but for the very future of our society, I think that would be a fascinating thing to see because then we'd be able to put into context this conversation and see, okay, practically, how would that work in different scenarios?
Speaker B: What do you mean by war gaming?
Speaker D: Here, wargaming is like a simulation of an AI that's trained to do something specifically. So think about. I'll give you another example of, I don't know, the South China sea escalations. There's a lot of wargaming that's happening on kind of what would happen in different scenarios.
Speaker B: Yeah. Are you saying like scenario planning for AI is of a certain form and incorporate. So I'll give up one example, which isn't explicit scenario planning, but it's an example I like anyway, which is the future of Life Institute. A number like it was one of kind of an inspiration for this group did this world building contest where they requested people to just imagine positive futures with AI in 2045. It had to go well. What had to have happened for it to go well? And what is that world? And one of them had digital nations, and they actually started thinking about digital nations. This group of three people, I think they're mostly in the global south, these three people who came up with this thing. But since then, there are now some digital nations, as certain islands that are anticipating being washed away due to global warming are like, how can we preserve some aspect of our national identity? Okay, Sidebar, over. So they create these digital nations. And what's interesting about the concept of that is that you could have these non embodied digital citizens and their AI. I forget why they chose to do this, but they imposed limitations on themselves so that they can engage with the other citizens of their nation, humans, on their timescale. It was like, exhausting. There was like an exhausting qualia to their existence when they were thinking at like a billion thoughts, a millisecond. And so they kind of like rate limited themselves so that they could have this interaction. I don't know what incentive led to that. But again, there was this positive view, and maybe one of the preconditions of that is, for some reason, AI systems ramp themselves down. So maybe there are other examples in that. There were many submissions. Maybe there are other fun scenarios that she was mentioning.
Speaker D: It was actually a TED talk at AI this year. He was basically showing how you can simulate this behavior. And they were all given characters, they all had lives and friends and certain limitations that are only specific to each and one of them. And then you see what they do, what they would do, actually, if you just let them be that thing. But in essence, it's really like not each and one of them is not AI, but it's an instance of it. We are an instance of a human consciousness, each and one of us. So each character is an association of that AI capability with constraints, with history. I guess that's the way to go about it and see how that could look like. And I think people will be working on that because they're already working on it. And it's pretty good mentally right now, but it will get somewhere for sure. It will enable simulations for us to look at.
Speaker C: I was going to comment briefly on this war gaming, the introduction of some new thing to society, and seeing how things evolve. We can't predict the weather more than one week in advance.
Speaker B: Oh, we can't deep mind.
Speaker D: There is a company that does that extremely well.
Speaker C: Let me finish with the point. Society is so impossibly complicated. How could we predict, simulate accurately war game accurately? So war games makes sense because it's a limited context, with known incentives, known capabilities in terms of real estate technology, very clearly defined goals and outcomes. This is a completely open world. I would think it's like the only way to approach this is actually through federalism, right? Which has historically been the case where what is the best way to self govern? And so United States has done well, I'm canadian, by the way. But it has 50 small experiments in self governance, right? Europe is another example where you have many small experiments in different national constitutions and so forth. So I don't know. I don't know how useful the war game could be to stimulate society.
Speaker D: But you can simulate people really well because people are so predictable. People are just so predictable.
Speaker C: I think it's awesome.
Speaker D: So much like alike each other in cohorts.
Speaker C: You think you can predict society, you.
Speaker D: Can predict certain human behaviors and math.
Speaker C: Yes, society, not the whole evolution society. Think about what I'm asking. It's an easy answer.
Speaker E: It's a butterfly effect. It's a butterfly effect. Like Sam Alfman marries someone, and all of a sudden, like a hypothetical scenario, butterfly effect changes everything. Of COVID for example.
Speaker B: Speak over again.
Speaker D: Sorry. I see where you're coming from. I think I maybe missed explain. I don't mean using war gaming as a means for prediction, because even if you think about it in the military, war gaming, no one knows if there's going to be an invasion of Taiwan. It could happen in 100 of ways. It could happen tomorrow, it could happen in 20 years with different technology. What I mean is like, just using it as a means for readiness and preparedness, just as a way to say, okay, these are some scenarios that could occur, how can we prepare for that? And maybe we can, but it's just to have them out there. But of course, it's not for prediction. And I totally agree with you. We cannot predict society. It's just a flush of diversity. To another point that I think we didn't really talk about is the point of culture is related to personhood, is like, are these AI systems going to embody culture? Can we say this AI? I'm from Lebanon. Let's say this AI is lebanese. How can you feed an AI a certain culture or are these AI agents going to have a culture of their own? Because I think a big part of personnel and humans is their cultural piece and their background. So I'm very curious about your thoughts on this culture piece. Where does it fit? I think something that I really want to add this conversation, I feel like it's kind of like a shift of paradigm as we were talking about this, is that I'm just going to present this as either a challenge or something that's like an inspiration. So we've talked about one major company in the context of AI, like how AI has been empowering in terms of our capabilities of doing productivity tasks. For example, what about one person community? What about one person culture? What about the stuff that you fast forward into? I don't know how long into history, how long into future? But just like when Apollocci talk about network state, he's talking about still something that's built online. It's still like a few persons are contributing to this concept. But what we have possibly we can just have a person, one AI agent, another AI agent, a few AI agents together with you, and then you're like, we can probably create a culture here, we can probably create a community here. And then the ultimate goal isn't just about human preservation anymore when we're talking about personhood, in order to define what persons should be justified to do towards other entities around the world, but in the sense like, what if I'm just creating a new species that are me, or a new culture that are just me, a few other AI agents? We're preserving this community as in this Sci-Fi world.
Speaker C: I don't know.
Speaker D: I feel like that's just some scenario that was being inspired to think about when I'm hearing the conversation.
Speaker B: We're definitely going to have to build some cultural. One thing that's brought up sometimes is often in this biased perspective. You don't want an AI system that's like, this is the world, right? Any single answer is going to be biased, right? Because an answer to almost any interesting question has a million truths to it, and you can give many answers that are true. So how do you sort through all of those truths to be the ones that you want to reflect? When someone asks you some question and there's an insane amount of context that we bring to that question in a particular environment, and I'm going to call that set of context that determines how I act culture. I'm just going to call that one thing at least, and it's an effect on me. And so how are you going to get to a place where you want an AI system that doesn't just say, wow, there are many answers to this question, and then list 10,000. Like, that's not useful at all. The way chat GBT tries to do it right now is kind of like that, but they list like four or five. Right? That's how they try to deal this. But still, there's always an implicit sorting of these kind of things. And it probably will be more relevant to just recognize that we have different cultural contexts. And when I say, how should we treat our grandparents or something, you probably want to know more about me than just that sentence. To be able to engage with me in the way that I actually want to be engaged. I'm Japanese, I'm 80 years old, or I'm jewish and I live in San Francisco. All these things are relevant, and a person would engage with me in that way, they'll kind of get some sense. So you want the system to be able to be like, I know where you're coming from, and I'm going to engage with you like you now make to its extreme. That leads to information bubbles, and there are problems there where everyone has their own truth, but I think there's clear market incentives to just have a better experience for me, for the AI system to be able to understand my culture and therefore embody that. And why wouldn't we then create AI systems that are kind of. I can expect that they'll interact with me like a lebanese person would. And why wouldn't Lebanon want to make sure that such an AI power exists in the world that is an extension of lebanese agency and is interacting the world in a lebanese way? That must happen, I think.
Speaker D: Can I follow up with a question? So assuming that can happen, you would need data to train that AI? I feel like culture cannot encompass culture and data because culture is not only the food, the history, but it's the community, the relations of people, the way of life. So how would you include all of that essence that forms culture into an AI agent?
Speaker B: That it's just a simple example. Right now we have Internet ingested, and that's not all of human communication. Right. There's a lot of times that we interact with each other in person, we talk to each other like we're doing now.
Speaker E: Right.
Speaker B: And not only. I mean, that's culturally different, even for us. The way I speak here is culturally different than how I speak online and the way I consume things. And so building AI systems that I'm sure. OpenAI is already doing built off audio data and the ones they have right now are still probably stilted. They're like podcasts and they're very formal. What we have here, what we're starting to record here, and I'm sure other people are throughout the world, this is the information that I believe will move us towards now. Still not all of culture, but imagine everyone's Always, every conversation that people are having with their parents around the dinner table are just recorded everywhere. That is the kind of data that, whether consensual or not, is going to be somewhat recorded. And I feel like that's a future state that we can expect. I don't think we have the information right now, but we have the written work.
Speaker C: I think there's a danger when we're talking about culture to make it a model. So even this idea of these two essence of culture, we typically think of culture, food and art. But you ask an italian how to make a red sauce in Sicily, in Rome, Milan, you're going to get three totally different ideas. So then, okay, maybe we have it regional. Okay, well, you talk to families in the neighborhood. It's just like this idea of culture and trying to build culture into an AI system. Until we're all doing story cores constantly, we are feeding this hyper localized, overflowing, ever growing stream of data into these consciousness. I think there's a danger for us to try and imbue this into it and then just kind of develop stereotypes. And there's also this danger of putting a culture or personification of a culture into one of these AI systems and then just letting that into the world, because then like, oh, this is my lebanese AI bot. I'm going to learn so many things about Lebanon. Okay, that's terrifying because just say you have prompt injections, like different attacks you can't do on current language models. That's not going away anytime soon. So how do we hold this idea of culture being inherent to the human experience and personhood, but there being that state of death? And then do we think the capability and impact these tools will actually have on the world? It's not waiting on that. We're still, to your point about really tangible examples of how it's impacted with deepfakes. Even without that deep understanding of the culture part of the human experience, it's still severely impacting our world.
Speaker D: Well, I think that it's actually when you have so much data and so much information, like he's talking about the scale at which we're going to be able to record every conversation everywhere, in all the cities, in all the little local ways of doing things. It's not a bias anymore. You have it all and the system can choose which one of those it's going to bring to the table, to the event, to the specific moment to represent. And so while there could be, I don't know what could be, the massive dangers of it, but I think there is a lot of potential in it as well because then you get to enable cultures around the world be brought to people and they get more closer to them and understand them. And we can start learning and respecting each other more with our differences instead of creating this kind of separation because we just don't know, we don't understand.
Speaker A: What these cultures are.
Speaker D: People have so many biases and predispositions because of lack of information.
Speaker E: I don't think we should get into it cultural, but I will join your side of the battle. I'll set it.
Speaker A: I think we should not get into it.
Speaker E: This is how you're totally postmodern. But what I would say is we judge AI by what Chat GPT is today. And I think they just launched a few weeks ago that it's going to become more personalized. And if you add it that it can you also, which they turned off by default, but they have cross user data sharing and you can look at lebanese IP addresses. The output is always going to be like text or audio and that's what the input is like. It can learn from, say like Facebook GPT could learn from all the chat messages and it will learn how to do cultural text output or audio.
Speaker A: Yeah, but it's going to do all the pictures and all the audios with IoT everywhere. Going everywhere. Definitely. Within five to ten years everything is going to become recorded. So it's inevitable.
Speaker E: Andrew, I think one thing, when you were gone, I said the difference between switching topic completely, it's like the difference between the industrial revolution and its upheaval on society and you couldn't predict the second infertile consequences. Now we have instead basically an intelligence acceleration, right? And we could like, we judge AI by what chattivity is. And yes, it's probably better if you want wisdom or depth of knowledge to read a paper than to read chat GBT because it gives you the executive summary. But AI is much more like the next. We're going to have expert GPT, we're going to have wise GPT that looks at depths of the paper and gives you even deeper knowledge. We were short on time, but I think all of these personhood, they're good problems. That means we have a societies to win a few years. But what are the short term, what's the personhood problems? Like the algorithms, like the fake news, what's the things that could lead us.
Speaker B: To dystopia or personhood is not the most relevant of risks. Not every AI is the honest.
Speaker D: I think it's also interesting in the sense like how there's always this power structure that's behind the development of AI. Who controls the direction of optimization? Who defines where these AI tools, at least as of current, they're not human yet, they're not intelligence yet. Who is controlling the development direction of these AI agents? Who's defining our governments, defining our tech leaders, defining. There are a certain class within the society that is exclusively being responsible for guiding the directions of these optimization. And it's certainly not the bottom 20%, the bottom 50% of the society that is doing this. And I think that this power structure, how is it going to be translated into more power like imposition upon the community that is not defining these technologies? I think that's awful, just challenging.
Speaker C: I think there's two frames. I think about this particular, or any AI topic in is like you have, like, you talk about the industrial revolution, like the agriculture revolution. This is, we're in the midst of the AI revolution. We also have this flip side that I don't think we've ever really dealt with before, and it's like the evolution of it. So, on one hand, AI is a tool. Like, I can use it to write more, copy more emails, learn at a high level fast than I ever have before.
Speaker B: It's a tool.
Speaker C: It's a really, really powerful tool. But then it also has the potential to evolve. Like, we have this idea that we're even talking about consciousness. I don't know how much we've ever talked about consciousness on tools before. You've never been in the tractor, like, what if that tractor is thinking about something? No, that's like a common question. It's like, do robots dream of intelligent sheets? This is something we really are thinking about now. And I feel like we're keep toggling back and forth between, like, okay, how do we treat it as a tool that's changing, a revolutionizing industry and society, but then not always as much as the evolutionary step? I think we're just toggling back and.
Speaker B: Forth as a kind of fun, just random anecdote about someone ahead of his time. So I forget which of his books in Adam Smith, when he talked about, he talks about this pin factory, and he loved talking about pin because he was like, that's what he observed. He observed this factor. And one of his observations was like, when you industrialize, essentially, I don't know if you use this word, but when you industrialize pin making, you allow yourself to now iterate in a way that you really kind of didn't with human. Just a human system, just humans. Like, maybe people learned how to become better at pin making, just like they did as hunter gatherers, making better bows and arrows. But the reality is, that led to almost no change in anything over tens of thousands of years, right? That kind of human learning, very slow. But when you abstract, when you externalize it, when you're like, now we're building this thing, it makes this part of a pin that allows you to iterate and become better and then forevermore send out those innovations. And that is the core of why the industrial revolution has led to this exponential increase. I guess I would like to push back just slightly on completely separating the AI revolution from this exponential. We've been on for a while now and point out this guy who was just really astute in, like, 17 hundreds, recognizing that that was the thing. Because here, I think we're just at this point, maybe, of maybe humans will be even further abstracted, and this exponential improvement will happen without our involvement at all. And that is certainly another sea change, but it's along the same curve, meaning.
Speaker D: You can't separate it.
Speaker B: Saying that this power that has been abstracting away the means of production or the now means of thought from humans, which are organic, evolutionarily optimized things, to something else that can be optimized with other processes, has been the key to why we had the sea change from hundreds of thousands of years of essentially nothing happening in the human sapiens to the last 300 years being like a fucking mindfuck.
Speaker C: Free market capitalism, man. Thank God free market capitalism is developing all the powerful Agi, too.
Speaker B: Don't we trust that process to find.
Speaker C: The best possible outcomes?
Speaker D: I just want to come back to the point that was made earlier on the IoT point of, like, in a few years, we might get to the point where the conversation at the dinner table and the conversations everywhere are collected. And I think this is a very big concern personally to me, because that relief infringes on surveillance. And I think that when we think about surveillance, I think this is where we lose a lot of the personhood part. We lose agency privacy, interconnectivity. And going back to the point of having an AI and body culture. I think that we also use the human connections that really define parts of who we are and parts of what our society has made. So I would like to push back on that point because I feel like this is where we should not reach. This should be. I think there's a quote that says it's taken in another context. My liberty ends when I'm infringing the liberty of another. So I think this is where I would personally draw the line between having my every move monitored and having that trained into an AI. I think this is where personhood gets a bit shaky, and this is where I think we lose a big part of our personhood as people. So just wanted to bring up that point.
Speaker C: Cool. I think outsiders.
Speaker G: Oh, I was just going to say, I think that's an interesting point, and maybe this has come up before, but I wonder how much of that is to do with the systems we already have in play rather than the technology itself. Because I'm kind of afraid with the policy around AI causing degrowth and halting this innovation. And I don't know, just curious to get everybody's thoughts about that.
Speaker C: There was a great phrase I heard a while ago, which is that AI is a tool in a lot of ways. And tools don't replace jobs, they replace tasks. And definition of a job is always a social construct, what we choose to call a job and choose what kind of roles we want to have in society. One note, those time I thought, let's couple minutes, everyone kind of reflect on a conversation and share other big questions that are open that could be topics for future salon discussions or cool takeaways or ideas that you had that were like, oh, I think about this differently now, or no, I'm even more convicted than my belief, sir, just like last kind of 1015 minutes here, we just kind of more reflective on that kind of stuff. Anyone?
Speaker D: I think something that also you mentioned about the intrusiveness when we're talking about this because it was kind of inspired by what I was looking at today. It's like a new product called Rewind AI. I'm not sure if people have heard of it, but it's basically like something that records what you do, basically everything that you do on your laptop. And it helps you in a way that it reflects your memory when you're trying to find something. Like you close the tab and then you're like, where is this thing? Like, I wanted to find it. So basically it's intrusive. But in a sense, I think going back to your point on human laziness, on the so called economic convenience that people are seeking over, I feel like just to also dwell on the concept of inevitability, I feel like 15 years later, 20 years later, when we're talking about the same concept of personhood, we might just have a very different view on this. Like, what agency would mean is probably not the same thing as we're talking about it right now. Is it good? Is it bad? We probably don't know.
Speaker G: Just like how we're defining personhood right.
Speaker D: Now is different from how we define personhood a long time ago, but in the sense how intrusive technology is actively reshaping how we view our relationship with ourselves, how we view our relationship with other people. We have friends who use Chechuki per day. What can go wrong? You would describe your own personal decision making in very different ways than how we have been thinking about it all alone. When we're delegating our tasks out, when we're delegating certain our agencies out, we are potentially, like you said, we're potentially decreasing the amount of tasks that we think our personhood can deal with. But at the same time, are we going to come up with more tasks that our personhood can be dealing with? We never know. Do we have that optimism in our humanity? I feel like that's a good kind of distinction point where that people are going towards in the future direction.
Speaker E: Maybe you must touch on long term philosophical aspects of personhood and AI. Well, what I was more interested in when we started was the short term personal AI touching that personhood. But I think one insight I had I didn't share was even when we would merge with AI, I think before we would merge with AI, we'd marry AI, which means there would be the AI and there would be me, like the humans, there's like me, you, and us in a relationship, right. And there would always be that personhood. We would not merge with AI, would marry AI, rather.
Speaker B: Yeah. To me, these are like the optimistic worlds where, for instance, for surveillance, two things come to mind are, one, how different different cultures values are. It just shows that cultural value, like they are not objective, they're not inherent, even like privacy as a cultural norm. I'm sure the generations born today with AI will have a completely different view. At the same time, I believe there might be ways for us to have rewind AI or our own personal way of recording the world around us that helps create an AI that is an extension of me, or is to use the marriage thing is something that is.
Speaker C: Like.
Speaker B: Is meaningfully talked about as this group with somewhat aligned incentives, somewhat shared knowledge, like me and a spouse, right? We're not, we're two people. But there's another real way of talking about us as a unit that engages with the world, that has incentives as a little group. And the more empowered we can make individuals to have access to their data or control their data, there's no inherent limitation. I see, for us each to feel massively empowered like gods compared to who we are today in our ability to synthesize, in our ability to engage with other people. And that might live on a platform of giving up some previously held values, but at least revealed preference so far shows that people don't give a shit about a lot of supposedly held values. Anyway, one of the things that I came in here with that we didn't talk about, and I'm not sure if it is actually that important, was the kind of like EA inspired idea of the moral patience of AI systems that can feel pain and are being put through reinforcement learning by the trillions and by training them are being subjected to torment that is absolutely beyond anything that has existed before. And so whatever society we create is resting upon this platform of just unknown terror that we are propagating against conscious. And the reason why I'm saying that maybe it's not so important is because I still think there's some importance there. But there's so many other components here that have been brought up that really push this question of at least AI personhood or moral patience independent of the legal relationship. Like how is society going to work with this shit down a bit? And I think I can appreciate that perspective more now.
Speaker D: I'm glad you brought it up because I think it's probably going to be the easiest practically way to create the ability to empathize with humans and also behave in ways that are more human like. If you actually expose an AI to that training data of feelings and pain and everything, and then actually heal. So that would be the cheapest easy possible.
Speaker B: You remember when I tortured you for 10,000 years?
Speaker D: Don't do that. Exactly. Information the right way life is going to happen, but then you're going to know as a human that this thing can really feel right. I guess that sounds existing, that you are going to start treating it as a personal way. I think an interesting assumption that is kind of going with the moral patients, like the expending moral circle as it would be deemed by a lot of the EA philosophies, is like the moral standards. I mean, even if we talk about AI personhood, the moral standards that we're extending towards them are human moral standards. And how much of that is compatible with AI? How much of their moral standards, if they can actually come up with a moral standard, will be applied to themselves and also in their sense, like extending to us. I think that's an interesting kind of perspective just to think about what is the moral standard that we're talking about here. We're talking about human centered AI, because in a way we're extending our human values. But if we do in a sense give AI the very essence, the concept of personhood, as we think of them as a moral entity that is equal to ours, do we give the same moral treatment towards them as how we would treat another human, or do we give them the same moral treatment as we would just slowly develop a new moral standard for them? I think that's interesting for me to think, but I don't have an answer, obviously, in reflection to this conversation. I think in general, to me, what was highlighted was that as we think of AI in those two directions, of like AI in service of us and AI itself becoming a thing of its own and regardless of our own development, be the thing that is going to exist in self, the whole notion of personhood, then, should be also in those two different contexts. One is how it relates to us and deals with us and behaves with us. And the other is in what are the personas that it has in its own world, the world of ais, and where they're going to have their own rules and their own communities, let's call it. So it's a little bit of a crazy idea to think about, but it's likely to happen because people who work in the research of AI, they're divided into two directions, which is one, we have to have AI alignment, we have to make AI work for us. And that's the direction of where most society thinks like. But there's a lot of people who are actually thinking, no, we need to just develop AI for its sake, for the sake of its existence, just exploring that direction of ideas and thinking. And there's a market law that says anything that can happen will happen, so it's very likely that will happen as well.
Speaker C: There's this book on the table right there called accelerando, and one of the plot points is that post technological singularity, these ais basically form just their own society, and humans are just totally uninteresting to them, and their society is incomprehensible. To us. And we're just kind of like, it's like there's a really cool party you're not invited to, and you're like, oh, yeah, okay. I guess they just kind of went off without us, and they'll go colonize the galaxy, and we'll be living on earth and wondering what it's like.
Speaker D: That was her accelerando.
Speaker A: That was her also, right? They left.
Speaker B: Yes, the post her after they left. And then they're like, yeah, we're not leaving. There's just a cooler place to go, I think.
Speaker C: Another reflection, but I want to dive in more on my own, is similar to a short story called the lifecycle of software organisms, or software systems, written by Ted Chang, if y'all are familiar with him. Really great. And kind of the core conversation is around AI systems that kind of develop consciousness for purely a companion monetary purpose. The idea is they built these cute little companions people will buy and have in their metaverse, and they exist, but then they can turn off whenever. But it's following a former ape scientist trainer as she develops relationships with all of these digients, I think they're called, and plays with that issue of like, oh, what happens when consciousness is created in the purpose of corporate interests? That's the vein of this conversation we.
Speaker B: Haven'T explored too deeply.
Speaker C: But, yeah, it's long capitalism. I think I want to draw us to a close there because we've had a lot of cool ideas. I can't even begin to summarize them in any meaningful fashion, but I think it's opened up a lot of questions. As always, more.
Speaker A: Can I just say my final comment?
Speaker C: Oh, sure. Sorry.
Speaker A: I feel like AI is going to help all of us to become superhuman because I feel like they're just going to have extraordinary intelligence on both, like, mental level and emotional level. So with that supporting me, I feel that's going to help me to become superhuman and our host society become superhuman. And I also feel like what's happened throughout history of humanity, there's the black hats and the white hats, and with the technology recently, there's all these people doing the bad actor stuff, and then there's people developing things to counteract that bad actor stuff. So I think that's what's going to happen also with AI, there has been a lot of bad stuff happening, but we're still here. The whole world hasn't blown up. So to me, that means my data has never been stolen or anything. So I feel like there's enough white hats that's going to keep everything okay. And I just think if most of humanity in both the developed world and developing nations now can become superhumans, to be able to create and create the kind of world that we want, I think it's going to be pretty amazing. And that's why I'm optimistic.
Speaker D: Can I begin with my uncom.
Speaker B: Of course, yes.
Speaker D: I find your point very interesting. I'm going to offer a slightly different perspective. I'm not really a techno optimist. I'm more of a skeptic around these issues. Maybe it's because I've been looking at the risks, short term risks, way too much. I come from the developing world, and I feel like we don't even have an understanding of what AI is, at least in my country and in a lot of countries in the developing world. So I feel like even the discussion around identity and personhood are still very big questions in society that we still have not solved. So I think that the lingo and the impact of how AI would be integrated, if it would be integrated, and when, will vary from society to society. The second thing I want to say is that we probably will get to scenarios where we have AI in society. We can get to them, but do we really want to go there? Do we really want to compromise our personhood? Or is there a line that we have to draw and be like, okay, this is as far as we want to go, and this is how we want to move with technology. We cannot leapfrog technology. We can just move behind it. But I think that I'm still reflecting on how far we should go and how far we should push the needle. So, yeah, that's my thought.
Speaker E: A quick commentary, I feel another thing that I summarized from our conversation today is that everyone's perspective differs. And I think it's largely on how we view humanity or whether we are more optimistic and personalistic about the humanity itself that determines our view on the AI. Because eventually it's kind of like in this new body problem that people diverge into hoping to salvage human beings or hoping for them to take over and dominate.
Speaker B: Right?
Speaker E: And then was there a lot of these different fractions to that? And I think every one of us are kind of influenced by our experience and perspective to think what is the best.
Speaker C: Any other, not responses to these comments, but reflections. Otherwise, I think I have to end the conversation. Okay, thanks, everyone. Coming up.
Speaker B: Thank you.
Speaker C: Yeah, totally super fun. And then last point of order is we host these every Sunday. I'm hosting now a conversation on Twitter spaces at 07:00 p.m. On Wednesdays. So it's like more people can join, but there's, I think we just have, like a few people will sign up to speakers and then the rest, the audience, because it's hard to have who wants to talk next.
Speaker E: But it.