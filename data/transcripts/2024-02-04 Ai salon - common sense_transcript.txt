Speaker A: Is that I record these conversations. And I record them because I think the information in these conversations is really cool. And I don't know all the things that I might want to do with these conversations. But currently what we're doing is we have a substac and we synthesize these and we try to summarize them and put them there. That's hopefully helpful for you all, just as you can look back and like, oh, that's what we talked about. And also helpful for other people. The way that the processing steps is we transcribe them, we kind of defluff them, kind of remove a lot of the fillers and do our best to anonymize them. Those transcripts currently are not being posted anywhere, but in an anonymized form. They might be at some point. And the general goal for us here, forgetting the recording for a second, is this idea of Chatham House rules, which is basically like, I encourage everyone to talk about the ideas we talk about here, but don't attribute them to individuals. Though I do hope that you follow up with each other and find new connections.
Speaker B: Okay, so spicy takes are all spicy takes.
Speaker A: There should definitely be spicy takes here. I think that that's really the point. By having the kind of conversation we're about to have for two and a half hours. For those of you who basically will go till four, it'll take like an hour, and we'll start to get the lay of the land, and then we can have spicy takes.
Speaker C: Right?
Speaker A: Like, you will start to get places. So anyway, with that said, my name is Ian. I'm one of the founders of the Axlan. Andrew Cote is the other who isn't here today and today hosting in partnership with Liz, whose homework we're in on this theme of common sense. And we're going to spend a little bit of time going around. And let me start just introducing the as lawn itself. The goal is to bring people together to talk about AI from kind of historical, philosophical, sociological, like many different perspectives. Whatever perspectives you bring in are valid perspectives here. And to talk about this kind of transformative and impactful technology. And each time we're talking from the perspective of a particular theme. And so we're going to try to not talk about all aspects of AI, but stay within the context of the theme of the day. And the theme is common sense here. So that can be taken many ways. And I'd like us to go around and just introduce yourself and give your background if you want, if it's relevant here. And some of the ideas that have been floating in your head related to this theme. So I'll start. My background that's relevant here is I was a psychologist for a long time, and now part of my day job, I guess, is like some form of prompt engineer. And I'm blown away sometimes by the kind of inferences that AI systems can make when I give it relatively impoverished information. And yet it goes way beyond what I say to kind of what I mean, the context beyond it. That's kind of what I think common sense is like, that background of correct priors that inform the inferences that we can make. Now, while I'm impressed with that, I have no trust at this point about the full space of things that I believe should be known without saying and what the systems actually care about. And I think that's an opportunity for huge kind of risks. Those are kind of the blind spots that I'm worried about. And so what I would love to spend some time talking about here is like, can we crystallize this kind of vague term of common sense, get my own kind of idea of it into a way that seems like we can make progress in measuring it, in making a little more concrete, and then we can start to talk about how AI can kind of relate to that idea. But, yeah, let's go this way. Sure.
Speaker D: Hi, everyone, I'm Spencer, I'm an anthropologist. I'm doing my dissertation on the anthropology of AI. That specifically is mostly about how AI raises questions about what it means to be human and how mostly AI researchers are trying to answer those questions now through the tools and concepts of machine learning. So they're almost becoming kind of anthropologists themselves. So, yeah, this topic of common sense is very interesting to me. I wonder if machine learning models do have a form of knowledge that we humans would call common sense and how we would actually figure that out or what we would call it, and then what I would in turn say about our own common sense. I guess as an anthropologist, I kind of have like a constructivist view of common sense. So something is common sense because we have a consensus that it is like we say it is, and it only becomes problematized when suddenly someone lacks it or another entity starts to have it or it doesn't go far enough. So I'm curious what that means for AI and these other intelligent agents.
Speaker E: I'm Laura, and don't have a ton of experience with AI, but looking to gain more and learn. Working on a personal project, helping a friend whose daughter has leukemia, and she has a hospital social worker who's supposed to refer her to different programs that she's eligible for, but she's just not doing her job. And so I'm trying to help her and I don't know anything about what programs are available. And so I thought AI could help me get up to speed really quickly and maybe tell her, here are the programs that are right for you and haven't gotten it to do that yet, but I'm sure it's possible. I just have to figure out how to do it. And yeah, so that got me kind of thinking about all the different possible applications of AI, including in spaces that maybe they're not thinking about, like they're not early adopters, like nonprofits and social work, and doesn't seem like anyone's building tools for those segments yet. And yeah, I guess as far as common sense goes, I've definitely gotten some answers that seem like kind of obviously not correct. And so I guess as people start adopting AI, how can we train kids and students and people who are, I guess everyone's going to be using, like how can we train them to look out for those pitfalls and mitigate them and be aware of them, at least if we can't fix them immediately.
Speaker A: Cool.
Speaker E: Hi everybody. I think you all know my name is Liz O'Sullivan. I am CEO of AI guardrails company called Vera, and I also do some policy advising through the National AI Advisory Committee in DC. So I am out west for about another three weeks. I'm really excited. I usually live in New York City, but I came out here to see what all the buz is about with the San Francisco version of the AI scene. And so part of this is my anthropology of how do the different coasts think about and work with AI? And so far I'm taking notes and have really been loving the experience out here. I might actually move here. We'll have to see. I was thrilled when Ian indulged me and allowed us to have the subject matter for this topic of common sense for multiple reasons, I think most importantly for me, and as we get into the conversation, you'll learn more about my background, I'm sure all of our backgrounds in that. In a previous lifetime, I ran the data operations pipeline for a computer vision company, which meant working with dozens of people all over the world. And we were labeling images and trying to train models. And this was my first experience in deep learning. And it was actually remarkable to me the weird things the models would do if not explicitly told not to. Right. And so in particular, for instance, there was a situation once where we were trying to train a model to detect dog breeds, and we figured out that when the dog was in the upper left hand corner of the image, it just didn't get it. And part of the reason was because all the other images, they were centered or they were in the lower right hand corner. And so that common sense factor of, like, it's still a dog, it's just in the different place in the image is, I think, actually one of the hardest challenges in AI in general is conveying these things that we take for granted about gravity existing, right? Or like, people have to stand with 2ft on the ground, or things like that. Humans have five fingers, those things that it's obvious to us, but it is also a huge source of blind spots for us as humans trying to train models to do things, then that's when things go wrong. It's when these blind spots or edge cases or outliers aren't considered in the development process. So that's thing one is what actually is even common sense, and how much work needs to be done to get models to that point where we don't have to think about every possibility. But also, a lot of my work is. That's topic one for me. Topic two is that a lot of my work really, is in helping people get beyond the prototype phase. That's why I was when you were talking about it, because in my experience these days, so fast and easy to get a demo or proof of concept that looks and feels like magic, and that's 85% of the way there. That last 15% of making it actually work is going to take you 20 years, $15 million and six phds, and you don't even know if it's going to happen at the end of it. And so we spend most of our time at Vera on that 15%. So I'm much more interested in how things actually are and what is the real application, where is it truly going to be used, where is it good, where is it bad than I am in this utopian or doomerist narrative that seems to be dominating the conversation. So I wanted to bring together a group of people, super thrilled to be working with Ian to do it today, to really get to the core of it. What does an AI powered future actually look like, and how do we determine what's real and what's not? And I think common sense plays a big role in that. So I'm really looking forward to this.
Speaker B: Yeah, thanks for that. So, my name is Mikhail, my background is in physics, so I did a PhD in physics and then did a technology startup after that. That didn't end up working out quite the way I wanted to, but it was a really good experience, and I work as a machine learning engineer now. So everything from setting up prototype models to training them to putting them into production, and that's given me a certain kind of perspective on how this works. And I think a lot about where llms are going to be a part of our lives and how they're going to permeate a lot of the different tools and systems that we interact with every day. On the topic of common sense, I kind of share Spencer's constructivist view around, like, common sense is what we agree is common sense. And if an AI model begins to behave in ways that don't agree with that intuition, then that would really bother us. I also think that common sense is probably adjacent to reasoning, and reasoning is one of these things that llms, as soon as they get to be sufficiently large, begin to display. And the argument that these language models are just sarcastic parrots, they're just repeating things in some kind of statistical way without understanding underlying relationships, that argument is getting weaker every day. And so what does it mean for very large generative models to be able to reason about the world? And I think it has a lot to do with how we set up the problem when training these. So, anyway, I have some more thoughts on that, but I'll save that for the conversation.
Speaker F: Thank you. All right, hi, everyone. I'm Luigi. I'm an AI entrepreneur, so I have a company. And as part of what we do is basically simulate human interactions, particularly transactional human interactions. And so as part of this work, I really need to start going deep into understanding, okay, what are the underlying dynamics and what is the underlying logic between human interactions? And that kind of got me more into this topic of common sense, because the light language models that we know right now, touching on what you were saying, really infer logic and to an extent, common sense from language, right. But I'm not entirely sure that that's the only way we humans do it. I mean, I think on one side, obviously, we infer logic and common sense from language and from how we consume language in the world. But on the other side, there is also a matter of symbolic structures.
Speaker B: Right, like mathematical logic and embodied learning.
Speaker A: Exactly.
Speaker F: Embodied learning. So I'm now starting to think much more about, okay, in my case, for example, in order to simulate human interactions, how much do you need to go beyond the kind of language, inferred logic and common sense and start going into the symbolic. So this is more like the interest that I've been getting it from. And obviously I'm generally very interested in also the philosophical side of it.
Speaker A: So looking forward to this conversation.
Speaker C: Hey, my name is Esmail. I've been working on kind of like my startup for the past three months. That is a new video podcast platform, but it's more like using AI as a means to analyze the video content and try to see what will be the best possible suggestion to the people who listen to the podcast or just want to follow the content that they like. And I think about multimodality of the AI a lot because one of the most important things that I think currently is missing in the AI context is just human cues like facial expressions or understanding what is actually beyond just the words that we're using. Like you mentioned something like gravity, stuff that are just really simple, but they're not written down. So there's no description on how AI can actually be beneficial to humans for providing more context. So I'm trying to develop the system, basically thinking about it every day that what will happen if we all start just curating the information that we're feeding to our LLM systems, like the AI that is behind the tools that we use. So that's pretty much the common sense part that I want to talk about and think.
Speaker G: Sorry, what was your name again?
Speaker C: Ismail.
Speaker G: Ismail.
Speaker A: Yes.
Speaker G: Hi, everyone. I'm Jeremy. I run a small generative AI agency and consultancy called Handshake. Been around for a few years now. We kind of do three things. One is teach classes about generative AI to kind of like director, creative director, project manager level folks. We build custom software solutions for clients. So very much on the implementation side. An example would be like we're working with gift to build a chat bot on their knowledge base of podcasts and videos and reports and things like that. So lots of retrieval augmented chat bots. Right now, sometimes we do more fun stuff like helping authors bring their books to life. And then in the middle of that is our consulting practice. So we work a lot with senior leaders, usually at the C suite or board level of organizations. So work with valvoline or Walmart or Deloitte to kind of think about what's the impact of generative AI on their organization and their clients, both from in the near term perspective and kind of a future perspective. On the national security side, I work quite a lot with kind of like navy groups and FBI and different kind of security groups, not usually as a prime contractor, usually as a subcontractor but really appreciate those contexts to really reframe the kind of potential use cases that exist. And yeah, the building the stuff is the most fun part, the common sense part of this. I think a lot about it in a few different ways. One is like, I work with a group called AI and faith as an advisor. It's a bunch of religious leaders and then kind of semi retired cs people arguing about this stuff. And I like religion as a framing as opposed to spirituality, because it really sharpens the subjectivity. 84% and rising of the earth identifies as religious according to some religion. And so what common sense means, the reason why I like that is it sharpens how common sense stops being consensus very quickly, even in the realm of something scientific, like leukemia. I'm sure that there's certain things you wanted to get right, like that humans have hands that are attached to arms, and that cancer has to do with cells and things like that. But even within purely scientific materialism, what is correct becomes very divergent. And even within a religion, what is correct becomes very divergent. We built a chat bot on top of the Talmud, for instance, which has a lot of divergence.
Speaker E: You did?
Speaker G: Yeah.
Speaker E: That's crazy.
Speaker G: It was very easy to do, technically. Where can I play with this madrash AI?
Speaker E: Cool. Yes. I'm so excited about this.
Speaker G: We interviewed a bunch of rabbis about it, and most of them were like, well, why does it only give one answer? The Talmud was in v two. We'll redo it and make it look more like a Talmud page that has divergent views and things like that. So there's the old ML definition of common sense that Allen Institute of AI used to be really into. And it's surprising how much of that has been resolved by making the models bigger. And I have no belief or conviction around. Can you just make them bigger and it'll all solve it? Or do we need symbolic approaches or to bring back graphs in at some point? But I'm less interested in this. Like, why do the models still sometimes not get transitive things, right? Like, if he's her son, then it doesn't know that she's his mother. And more like, how do we deal with subjectivity? And how do we get out of this framing of, like, tis, Tis, Tis. The AI companies aren't making unbiased models, because how do we instead make biases really explicit and controllable, and then grapple with as a society about what we might want them to do and what.
Speaker D: Interest, objective kind of purpose might help them have?
Speaker A: Hello, everyone.
Speaker H: I'm apur. I'm an operator based startup based in Estonia called rightgrav. So we use microgravity. It's like a biodiscovery platform using microgravity. We also do instrumentation for scientific discovery. So that's why AI is very interesting to me, because the scientific discovery, mostly what's happening right now, could not utilize what's happening with llms and AI, because the instruments and the infrastructure which is used for discovery is so far behind that they can't even catch up to that. So infrastructure like emerald cloud labs and people who do works on those who are building AWS version of those kind of infrastructure for scientific discovery, I'm very much interested in that, because right now, hardcore research, or whatever people call automated discovery is way far behind in terms of common sense, I would say. I think every civilization is a form of metaphysics, material and social. And I think alignment, when we use the word AI, alignment, I think it's more of the alignment on those regards, as in aligning the AI. I think it's like when the society is kind of scattered in those areas, I think that's where the misalignment with kind of this AGI happens. And it's not the AGI, it's us. I think we have been misaligned.
Speaker G: Sorry. What's microgravity?
Speaker H: So microgravity is like simulating gravity on Earth.
Speaker B: You're not going to space with this?
Speaker H: Not right now. Okay, but that's the kind of plan over the years.
Speaker B: Okay, yeah, but how do you simulate microgravity on Earth? Dropping stuff?
Speaker A: No.
Speaker H: So we have a full stack solution, like hardware and software. We work with companies like Novartis and other pharma companies, where basically we give them the cutting edge extreme scenarios of microgravity to find the cutting edge materials, therapeutics and whatnot. But there is like other stuff happening.
Speaker A: Which I can't talk about.
Speaker G: Simulated.
Speaker A: We could also squeeze. Okay, let's all feel like we're like leaning over.
Speaker E: I would love to pick your brain about AI and science.
Speaker A: Okay.
Speaker H: I'm not a scientist.
Speaker A: To make sure. Cool. So we just went around and introduced ourselves. But please, what we're basically doing is, what's your name? And if you want to talk about your professional background, you can. But what background is relevant for the conversation today? And what kind of questions do you want to pursue on the topic of AI and common sense?
Speaker D: Yeah, can I have a few minutes before.
Speaker A: Absolutely. Cool. So anyway, thank you all for introducing. I think where I actually want to start is let's dive into this idea that has been kind of talked about by different people, which is like, how common is common sense, right? Is there a set of human. Is there a common sense that we all share? And how constructed is it? And if it's constructed in the way that Spencer kind of brought up, how divergent are our different constructions of it? And I'll put out there that I think we probably miss the 99.99% of common sense that we all share and then focus on the parts where we diverge. And an AI system, which is an alien, may not overlap with that 99.99% that we share. Like, when I look at an object and I just see that it's an object, that's an inferential bias that I have, that we all have. And there are like, people who study that, right? There's a shape bias. People use shape as a categorization thing more than they use color. Why do we do that? Maybe there are reasons in the world. Maybe shape is more functionally associated. I don't know. There's a huge number of things. But then there are also very important ways that it seems that common sense isn't so common. Right. When you interact with other culture. So maybe that's someplace we can talk about first, just like how common is common sense. And does anyone believe, as I do, that there's actually, like, a fairly large chunk of common sense that we could recognize as common? That's where I would like to start. And then just as a telegraph of where I hope we kind of end up going is if there's some aspect of common sense that we would like to make, either because it's joint and we'd like it to be in our systems, or it's divergent, but called out, as you mentioned, to reflect, like, particular biases, what kind of information would AI systems need to have that they don't currently have that would support the learning of that kind of perspective on the world? Do they need to be embodied? I mean, right now they just have access to text. That doesn't seem like the most. So let's start there. Can you start by introducing us a little bit more to the idea of a constructivist view on common sense and.
Speaker E: We can take it from there?
Speaker A: Sure.
Speaker D: Basically, this constructivist view of common sense hinges on the things. There's no fundamental definition of common sense. We call something common sense, basically, when it's problematized. So if someone has a certain behavior or doesn't understand that there's a sort of social norm that's going on, we would say oh, they just lack that.
Speaker A: Sort of common sense.
Speaker D: I sense that we have a different understanding what common sense is.
Speaker A: Because I.
Speaker D: Think there is certain body of knowledge that all humans must have in one way or another. Maybe that fingers have a hand of five fingers. I would personally not call that common sense. I think it's a different kind of knowledge. So common sense is more of a social sort of set of knowledge. It's a knowledge of how to navigate a certain society or community.
Speaker A: Yeah, maybe I'm blowing the scope too large by saying we all share an understanding of a folk physics where we understand the balls drop and fall. And maybe you would like to constrain common sense to an understanding of the social and cultural context that you're in. Right? I think so.
Speaker D: And I would also say that I think everyone has a common sense. Just everyone's common sense might be sadly different.
Speaker A: Like, everyone has a culture where everyone.
Speaker D: Is culturally raised, but culture is diverse. It's kind of defined as that which is diverse. So common sense, I think, shares that same kind of property.
Speaker B: It might be helpful to have, like, an example of, and I'm trying to think of one myself, maybe you have one more readily at hand, but of a social interaction where an individual, lacking what we might call common sense, acts in a way that we problematize. Do you have an example of that?
Speaker D: Yeah, I would say that just like.
Speaker A: Going to the grocery store and doing.
Speaker D: The grocery store things is common sense. So knowing what you're supposed to do with the items on the shelf and knowing that you're supposed to go and cash out at the register at the very end, I think if someone does that, doesn't do that because they don't have common sense, or it's common sense to just do that thing.
Speaker A: Do you think there's one that's brought up in misalignment literature is like, if you're like, clean my house and it killed the cat, you don't think you should have to specify, clean my house and don't kill the cat. There are certain expectations that are left unsaid, and there are a huge number of them that I've noticed this. I had a cat sitter recently who is not a trained cat sitter, and she's a very conscientious person, by all accounts, and did an incredibly terrible job. And my cat got harmed, and I was like, okay, what are all the things that I know about taking care of cats that she clearly did not, that I need to make apparent for her so that she can act in a conscientious way that she probably wanted to, but completely failed in doing.
Speaker B: Okay, so this is like an illuminating example, I think, because. Sorry, go ahead.
Speaker G: Well, I would just say sorry. No, you talk first. It's illuminating for me, but I'm not sure in the same way.
Speaker B: Yeah, I think the example is illuminating because you've been socialized is not the right word, but like, you've been socialized to cats, and as a cat owner, you spend time with cats, you know what their needs are, and you attend to those needs, and you've done that enough that it becomes routine and you don't have to actively think about it. The non cat owner has not been so socialized and is making all kinds of mistakes because they don't know how to attend to a cat's needs. And so you have intuitions about that and common practices that seem rational and normal, but that's because they've sort of become like background processes in your brain.
Speaker A: It's like a good teacher, good teacher. Not all experts are good teachers. Right. You have to be able to figure out those.
Speaker G: I think there's more useful definitions for that kind of knowledge than common sense.
Speaker A: Okay.
Speaker G: I work a lot with contractors, and it's really distinct, the difference between what you think you're telling someone to do and what you are actually telling someone to do. And if someone hasn't done the job before, you quickly realize that enumerating in an explicit way all of the possible things that they must do in order to complete the task is impossible. You must simply wait for them to go, make the mistakes elsewhere and come back to you. I would not call that common sense. I would call that learned knowledge or learned behavior. Of course you could call it common sense, but I think that's exactly what.
Speaker A: In my head is what common sense is within the context. Right. It's within the context of that job. So I guess it's not common, but it's very analogous.
Speaker G: You could call that common sense, but I think because we have all these other definitions for that, like skills or knowledge or understanding, that a more useful definition of common sense is one that is actually, and that is more relevant in the context of AI models, is like, how deep do we have to go before? Because, for instance, it is not common sense in many places that cats are not food or that under no conditions would it be okay for.
Speaker A: Let me re restate it. To make that kind of knowledge that she didn't express in the cat domain is the kind of knowledge that is analogous to common sense. And I kind of am now getting a little bit closer to Spencer, which is like that kind of knowledge applied to the social cultural area might be what I think we can also squeeze one more person on this couch which might be more comfortable. So would you like to join us on this couch?
Speaker C: Thank you.
Speaker A: I really think we have plenty of space, but whatever you're more most comfortable with.
Speaker G: So just to clarify, you're proposing that we situate definition of common sense in kind of like a culturally specific, socially specific set of shared understanding, as opposed to the understanding that when I set beverages on a table, the beverage does.
Speaker A: Not fall through the table, which this was with Spencer. And right now I don't want to like. Clearly this always happens. I think it's good for us to find some scoped definition of what we're talking about. And we could talk about folk physics, like an understanding of physical relationships. We could talk about these kind of skills like understanding background knowledge that is applied so that you can have more effective conversations without explicating everything perfectly. And the current thing under I think that is the most positive statement of what common sense might be is some aspect of the set of beliefs and perspectives that allow you to have easy relationships within the context of social, culturally tasks. It has to be held in common.
Speaker D: For it to be common sense.
Speaker A: Yes.
Speaker D: I guess this is a good segment for my intro, if you don't mind.
Speaker A: Yeah, that sounds great. And I'm also going to just let you introduce yourself. We went through introductions a moment ago. Let me give a brief, just intro to what we're doing today. Just so you two feel kind of included. It's the AI salon. You're going to be having a conversation for the next 2 hours. We're recording. The recording will be anonymized for all uses, but just know about that. And we follow this thing called Chatham House rules, which is talk about the ideas here outside, but don't attribute them to individuals. We introduced our names before. Feel free to ask people their names, but we won't go around explicitly again. And we're talking about common sense today. And I won't say exactly what that is because right now we're in the midst of trying to kind of define that and the relationship between common sense and AI. So both of you came in afterwards. So I would love to hear after your name and your background relevance to this conversation. It doesn't have to be a professional one. And what you were interested in talking about today, that's a lot on you. I know.
Speaker D: Okay, so I'm Mohammed I did math in college, and I think the interesting thing about common sense for me was that I'm working on a robotics company, and robotics is just filled with these instances where you're like, wow, we just take so much things for granted. And the tasks that are very easy for humans are extremely hard for robots and vice versa. So I think recently, when I keep thinking about why is it that certain tasks are so hard for robots and easy for us, I get to answers like, our brain has just evolved to bias towards visual information so much. And like, a huge part of our brain has evolved to process that. And because of evolutionary reasons, it was so important. And computer vision is just not there yet at all. And there are huge problems that are just completely unresolved.
Speaker A: Does anyone know the phrase the paradox? That's like, things that are easy for.
Speaker G: Humans, it starts with a w. It's like Wasserstein or some dude.
Speaker A: Yeah. This has been repeated many times over where we expect things that were hard for us, like chess. It's like, ah, that's the peak of intelligence. But actually looking at things and recognizing them is a very hard intellectual thing. And none of us care about that because we all do it perfectly because we've been optimized to do it over eons. And chess is really hard for us. But we can learn in one lifetime, which means it's not that hard a problem. Anyway.
Speaker G: I always forget the name of the for understanding depth.
Speaker D: I keep getting shocked at how hard it is in the computer realm.
Speaker E: Interesting depth.
Speaker D: Like depth in pictures or like doing depth estimation.
Speaker A: Sophisticated. Yeah. So people with one eye have no common sense.
Speaker G: We should piece that out later.
Speaker A: Yeah, it was a joke, but. Sorry, could you.
Speaker I: Yeah, sure.
Speaker E: I'm Ali.
Speaker I: I'm an engineer, but I used to work in data science and I've actually not kept up with the AI hype over the last couple of years. So I've been looking for some kind of way to get into the conversation. So I'm very new, I guess. And I think the talk here today was more interesting to me because I feel like AI is. A lot of the discourse around AI is very philosophical. And so I kind of wanted to hear what people had to say that was independent of all the developments and events and technological innovation. What are people thinking about it? Mankind, I guess.
Speaker A: Cool. Welcome.
Speaker E: Nobody's fundraising today, so.
Speaker I: Okay.
Speaker E: Talk about the utopia.
Speaker B: Okay, so the paradox that we were referring to just now is called Moravx paradox.
Speaker A: Cool.
Speaker B: In case anyone.
Speaker A: Thanks. I've looked this up so many times, I will never remember it, but I.
Speaker D: Appreciate I should make a more helpful name.
Speaker A: Pour more of it. Come on. Does anyone else want who we haven't heard of from yet, want to offer a positive or negative kind of perspective on what we're starting to circle in common sense definitions?
Speaker E: One thing that I spend a lot of my time thinking about is, and I don't know the answer to it yet, but is there a set of information or facts, whatever you want to call it, knowledge, common sense that everybody can agree on, or at least 99.9 repeating? Because if so, then it would be great to have that be the starting point for all the branching models that I'm sure we're going to have over the next hundred years. But it's really hard to whack them all. Every time you come up with one fact, you're like, oh, the laws of physics. I believe that. But there are people who don't actually believe that the laws of physics are caused by forces or behave in a predictable way and that they could stop working in this way any time in the future. But I'm really interested in what is the set of information that has near complete overlap across cultures and maybe that is the common sense. I'm not sure.
Speaker A: I feel like maybe if we end up wanting to scope common sense into not physics, I keep pointing, I. I can't, can't represent physics. Not physics, but these social and constructed kind of cultural aspects, I'm guessing the answer is going to be no, there is no overlap. There's no complete overlap there, which would bring us to kind of Jeremy, Jeremy's point, which is like, maybe instead of trying to find this overlap or unbiased perspective, we want a controllable or transparent representation of like, what common sense are you operating under right now? Because it's not common to everyone, but it might be common to this 100 million people. So maybe work with that.
Speaker F: Like the cat owners, for example.
Speaker A: All cat owners, we all know.
Speaker F: Exactly.
Speaker G: I think maybe if we're going to situate it in the kind of social layer, it'd be interesting to kind of think about. One place that takes me is that when I'm interacting with a large model, it knows a lot of things that I don't and I know a lot of things that it doesn't know. And there's that kind of phenomenon of like once Internet search came out, there's some, I'm butchering this and couldn't reference the actual study, but apparently some study somewhere said that people started believing that they knew more than they knew in their brains, but what they actually knew was where they could find it, which I don't even have an opinion on whether those are different. But there now this common sense that includes not only other people, but then the models, not in a sentient sense, or in proposing some kind of subjectivity for them, but in a sociotechnical sense. Does it shift in response to that?
Speaker A: That's a fascinating idea. Does anyone want to build on this?
Speaker E: Well, another question I have is whether is common sense something that can be generated outside of experience, or is it something like, can you be told something that is common sense? Or is that otherwise a constraint on the term? Is it an important constraint? I'm not sure.
Speaker A: I think it's a really good question.
Speaker D: Because I was just thinking about why is it called common sense and not common knowledge? What work is the sense doing in this term?
Speaker A: I feel like maybe what the sense is some times, like some relationship to what actions can you take. So common sense would be like, with very little delay, you will act appropriately. Common knowledge. You're like, I'm supposed to do, let me go through the flowchart and end up doing the right thing. But that's not practical for many circumstances. Maybe with Jeremy's point of with a connection, a faster connection, a neural link to other knowledge bases, a broader set of things could become common sense. Or we would want to call them common sense because they are practically causally relevant to how you act quickly.
Speaker G: There's also the word sense, if we're going to situate it culturally. What's interesting is that sensoria are actually culturally specific, like the kind of folk model that we use of like there's five senses, and we largely ignore things like proprioception. And then if there's like in a. I've worked a lot in east Ghana with a group called the Eve, and they put a lot of emphasis on a sense called cecelo lame, which is like the felt experience of an emotion in the body, which is know, when you say I feel anxious and that sharp feeling you get in your chest, they wouldn't call that touch. They would call, or like whatever the word for feeling that your insides is, but they'd break that out. They connected a lot to intuition and as a way of reasoning. And so how do we experience senses is also part of this. And then that kind of gets us to an interesting point about the robotics in terms of like, okay, well, we think we can all agree that if objects have one side, then there's another side to them that we can't see. But that might end pretty quickly, too. In terms of commonality, could you train.
Speaker B: An AI to learn common sense by just like, watching tv? It sees human attractions and how people converse and how they behave in different settings. And then it would, with enough training, data and a sufficiently large architecture, enough weights to compress all that interaction, learn a little bit about how humans interact, and then be able to imitate that. Because we've done that with language, we can ask chat GBT to talk to us in the style of Shakespeare or Hemingway, and it's been conditioned to have very human like conversational ticks. So can we condition an AI on common sense human experiences by just observing them enough times?
Speaker E: Observing parents and little kids? Because kids actually sometimes have to be taught common sense, or they learn it the hard way.
Speaker I: Right?
Speaker E: Like they touch, like a hot stove, and it's like, okay, that's not a good idea, that kind of stuff. Or parents explicitly telling their kids things that seem obvious to anyone over a certain age.
Speaker B: Yeah, or just that it's inappropriate in certain situations to run or to make a mess or to engage in certain kinds of behaviors that the owls in the room disapprove of.
Speaker E: Can we take a quick poll? I'm curious how many people think that something like that might work.
Speaker C: Obviously working on it, but something that I really think is like the mixture out of everything that we're saying, the mixture of what is actually the sets of rules that we're going to give to the system and how much of it is going to be just. We allow the system to learn the basic rules on its own. And I think all of it, in the end, will go to how much we try to give the system the freedom of what we want them to explore. Because a lot of times there's this conversation around AI that unless we see them making a breakthrough in physics or something that we haven't achieved as humans, we can't really call them agi or think that they would just be able to do something that we are not capable of doing right now. So I think it's just the balance between how much we need to give to that system and how often we need to give that information. Little information. Don't touch the stove if you have guests around, don't run around, but stuff like this to the system that they would just know that, okay, for the next hundred million times that I'm going to be in that situation, I'm just not going to go with that flow.
Speaker A: Can we stay on the poll actually for a bit so the poll is that through observation in a way similar to observing linguistics and like, I guess this also relates to how much you think AI understand language, but to the extent that only through observation, they have gotten whatever mastery of language that they seem to show right now, that a similar database of observing actions and human social interactions and whatever would lead to common sense understanding in a way that people will be like, I can't believe the sense of this system. Who thinks that that purely through observation.
Speaker D: Will happen, is the assumption with the current compute and computer?
Speaker A: No, it's not the current, but it's the trajectory that there's a crank that's being turned and that crank will have different kinds of data that will come into it in the future and more compute, but it's not a fundamentally different crank. Okay, so very few of us, actually only four of us believe this. I think probably most people could understand the arguments the four of us would make. I'm curious, what are the perspectives of the people who don't think what would be needed instead?
Speaker D: And I guess another question after that would be then do you think common sense is way different than a specific set of knowledge, fundamentally?
Speaker C: Yeah, I think one of the things that I'm just thinking about constantly is we're dealing with carbon based living, which is us, and then silicon based. And do we really perceive things in the same way? Even if we give all of the power that we have to the silicon and have the most amount of compute power, is it going to be the same amount of the same way of perceiving common sense? The same way of perceiving the world?
Speaker A: Yeah, just to bring some conversations that have happened at the salon in the past. Sometimes people make aspects that end up being some form which might be true, that the embodied nature or something is critical kind of knowledge, or like the capacity that you're going to die, like certain things that are kind of fundamental impact how the system can learn what it could know and maybe. But then an often kind of point that's brought up is, let's imagine that your entire world was text, and you can imagine humans who are closer to that. Maybe they're quadriplegic and they're still embodied, of course, but they have like a lived experience that is closer in some way to the set of experiences and the kind of rich life that current llms have. Would you deny them that group that they're developing common sense through that impoverished data stream? Or is that just a scoping of the kinds of information that they can work over?
Speaker C: Yeah, I think in that situation, the way that we communicate with them matters the most. So we need to find the optimal way of communicating with that environment. And that optimal way is just going to be through trial and error for us. But that environment, if we look at it as like a lab environment, as everything is kind of like at a plane level, and we don't really have the advantage of millions of years of evolution in us, that those systems don't really have that. And we're just basically giving a head start to them to enter our world and then they will start just developing their own knowledge and understanding. I think we all are curious about that here, but that would be like the most interesting way of looking at it.
Speaker E: You go ahead.
Speaker F: I'm curious about you. Did you say that you believe that that could be the case? Yeah, because I think that kind of connects a bit to what you were saying about if we take the social dimension of common sense at that point, maybe the carbon nature, or like the embodiment nature of it is not so relevant. Because at the end of the day, social dynamics, I mean, they are for sure like partly dependent on whatever carbon or embodiment. But at the same time, I do feel, and I think our experiences on social networks to an extent demonstrates that they are much more kind of sociological, more like not really embodied as experiences, the interactions that we have. And so that's why, for example, I do believe that by observation you would get them, because I believe that's basically how we get them, like the social side of it.
Speaker I: Yeah, I agree with you that there's.
Speaker E: An experiential component to it for sure. I think where I'm coming from is having spent so long just trying to program some degree of knowledge into computer vision models. The understanding stops at the four corners of the frame. And that's just my experience of playing with this stuff. But it's also, for better or worse, AI is about consensus. It finds patterns where there's common maladies. And usually it's like the big fat part of the bell curve that is the predictive element. And it neglects both of the blonde tail. Add to that that humans don't really agree on language very much. We mean different things when we say the same word quite a lot. And then, Spencer, we should have bet on how long it was going to take to break out the epistemology thought experiments. But in particular, I forget which philosopher. The chinese room thought experiment. So for those of you not familiar with it, you have a sensory box and there's a person in the box and on one side is a person speaking English, and on the other side is a person speaking Chinese. And then the translator sits in the middle and somebody is feeding that person in the center who doesn't speak Chinese, the phonetic pronunciations of chinese words, so they can speak to the person who actually speaks Chinese. But do they actually speak Chinese? No. As soon as you take them out of that box, they can't continue to use the language in that way. So I think that's what's going on right now. But if I were to design a system that I wanted to generate some form of common sense, I think it would be reinforcement learning driven, which basically is endless experiment, infinities of experiment, and comes to the correct or the consensus answer at the end of the train, but nobody's telling it. Nobody told Alphago how to win or what moves to take to get it, but it got there because it just tried everything and got to the lens that worked.
Speaker A: Maybe one thing, maybe one thing that will harm this conversation a little bit is like, do you believe that you could theoretically get someplace through observation alone? So one thing that kids do, or we do is we are constantly testing little causal hypotheses, and in order to do that, it's much easier if you can intervene, right? You're like, I think if I push this over, it will fall, and then you do it and it falls. That doesn't mean you proved your hypothesis correct, if we know our philosophy of science, but it does give you a much faster way to develop knowledge than inferring the causal graph by observation alone. You need much more data to do that. Well, and you still might have confounding variables that you're not appropriately able to intervene. But if you could sit and be like, I think if you push that over, it will fall, and I'm going to wait until someone does and then do it, you essentially get the same test. It was so much slower. My God, you weren't able to really do that. And through that kind of causal intervention, you actually aren't going to discover the right world. You're going to discover the one that has been proven correct for you, which informs your set of actions that you can take in the world. So it's a pretty practical one. And so my point about bringing all this up is to say, maybe there could be enough data, like we saw, where we could do this absurd thing of having a system learn these relationships observationally. Maybe it's possible, like we did with the Internet. That's a ridiculous way to learn. Kids don't do it, but we do it because we have this other source. It's not the most impactful one, but maybe both can do it. Just one is, like, way less data efficient, but easier algorithmically. Yeah.
Speaker B: I want to put a little asterisk on my response. So, learning through observation. The p in GPT is pretraining, and when a model like GPT four is pretrained, it is just seeing data that has been scraped from the Internet, and it's learning about language, and it is just trying to predict the next token in the sequence. And you can do all of this pretraining in a kind of semi supervised way. And what is being internalized in the weights of the network is things like grammar and syntax and language structure and context over time. And all of those very hierarchical ideas are being now baked into the network. That doesn't mean that the network, in the end, is going to respond exactly the way that we want it. That is the next phase of training, where you have a bunch of question answering tasks, or maybe you are trying to do alignment using reinforcement learning through human feedback, and you are then further tuning the network to tailor its responses to what we want. And it is enormously expensive. It takes terabytes of data. It's done over thousands of gpus with training cycles that take months. And you burn millions of dollars to train one of these new models. That's not how any human learns. We have a billion neuron, or 100 billion neurons, 100 trillion synapses, and it runs as much power as an incandescent light bulb. That's our brain. And yet we can learn all of these sophisticated concepts and social cues in a matter of years, just through growing up. And it might not be the most efficient way of learning, but I think we could have an AI that was a kind of like, foundation model that had elements of vision, elements of language, and if it was in a robot body, then it might have some proprioceptive learning, and through enough experience, or through enough sort of showing it example data, you could kind of pretrain it to have a set of intuitions about the world and how things work, if it observes enough human interactions that it might, doesn't it?
Speaker G: Already, though, we've all used chat GBT. If I knock something down, will it fall over? It'll be like, yeah, I think we've sufficiently moved the goalposts to prove that with purely auto regressive means, even with pure text, you can get something that portrays something that is, in almost all cases, unless you're trying to specifically construct a contorted scenario in which it will fail, does all these things.
Speaker A: If people had this conversation, to just put a point on it, if someone had this common sense conversation a decade ago or three years ago, all of the things I'm sure they would come up with, or many of them would have been proven true today. Like all of the things, if they were like, we would like a system that does this and this and this and this and this is what common sense means. The system we have now would meet those, I'm guessing, for that conversation three years ago.
Speaker G: So I guess my point is that if we're talking about could a system develop common sense purely through observation?
Speaker A: It's been shown.
Speaker G: I don't know. Does anyone really think that the chat GBT doesn't have common sense? That would be an interesting way to focus this conversation. Does anyone think that what chat GBT is right now doesn't have common sense?
Speaker D: Or it won't with the current.
Speaker G: Does the one currently not have common sense?
Speaker A: Yes. Raise your hand. I like this poll.
Speaker E: Okay.
Speaker D: I would push on this poll a little bit because I don't think it's either or. I think it has a certain amount of common sense.
Speaker A: Like for example, if you ask Chat GPT a question, it's going to give you an answer.
Speaker D: And that's common sense. It's not going to give you another question.
Speaker A: Were you bringing up the translator idea because you don't want to ascribe? You don't think it makes sense to ascribe common sense. It somehow knows how to pass through the information.
Speaker G: It doesn't have common sense.
Speaker I: I think it shows up in image.
Speaker E: Generation tab where you get inability to hold a consistent set of behaviors when you try to.
Speaker G: Okay, let's just use the lol, not the.
Speaker A: Okay, we can use whatever example is.
Speaker D: There are classic examples online with just pull up where it really fails, especially.
Speaker H: When you ask a spatial question.
Speaker D: Just like imagine I put something in between this and then this to it.
Speaker A: What's going to happen?
Speaker G: Do you think that sometimes. First of all, I'm not actually trying to argue this. I'm trying to define common sense inferentially to this question. This question is a censor. But don't you think that a lot of those word problems are things that if someone was shown that on social media, often I look at those, I'm like, if I didn't think hard about that, I would get that wrong. Not because I don't actually think how things are work, but if someone posed it to me in text form, I might get it confused. Does that mean I don't have common sense?
Speaker F: I think even easier example is that charge GPT, for example, struggles to count letters in a word, right? I mean, that's, I think, pretty basic, right? So for example, one could argue that then I actually agree with you that on one side there is common sense, on another, spatial sense, maybe there's not that much common sense to connect to what you guys are saying. I think we were forgetting about simulations, right? Like simulations of environments can lead kind of experiential learning without embodiment, right? So there's also that option there that is now being used, for example, now I am trying to use it for human experiences. Like human interactions. You can, yes, show them human interactions, but you can also start having some form of self play of human interactions.
Speaker A: Right. I'm going to transition us to a slightly different part. Not that this, I'm just stay in this area for a bit, which is, I'm going to say that there's some amount of common sense. We're going to move with that a little bit. And an example of what I mean there, which I hope reflects some of our definitions. If you say to Chat GPT, like, man, my partner and me are having this argument in this way, kind of difficult. Can you give me some advice? It gives good advice. It's not amazing advice, but it's like, oh, there are so many considerations here. Let me ask you, what are you going on here? And then you answer it and it's like, okay, well, blah, blah, blah, and it says something, and it's not completely off the mark. It's actually surprisingly good. Right. We've all, I hope, experienced something kind.
Speaker G: Of like that for you.
Speaker A: Yes.
Speaker G: In an arranged marriage.
Speaker A: This is actually where I would like to go. I'm just trying to set that as like, I hope we all have had some experience that is somewhat analogous there. So the next step is like, maybe Chat GPT holds multitudes within it. It's deciding what's the most probable thing out. And maybe the main one it gives out is like a western, individualistic kind of cultural perspective. That's what's been fine tuned into it. But it actually does know the arranged marriage kind of thing, that knowledge exists within it.
Speaker G: Do you have an explicit desire to. Not, when I asked, does it have common sense? Most of the things that people brought up were not social level things. They were like, it doesn't know how to count, it doesn't know how to see properly. You seem to be motivated to drive us into a definition of common sense that does not reflect.
Speaker A: I'm actually not. I'm just using the social as a continued example. The thing I'm trying to bring us to is now the idea of a system that can potentially reflect many common senses, many perspectives, many value. There are many perspectives on the world and the way we interact with it and the primary way it engages might be fine tuned to be biased to a particular perspective reflecting one kind of view of the world.
Speaker G: I wonder if we can give it. I think where we're in the conversation right now is what is common sense, I wonder. It's funny that at least the large multivotal, whatever, GPT, we're calling GBT four, they're very good at the thing you're describing. Like talking about explaining Goodell's incompleteness theorem to me in polish, if I just said polish, or giving advice on someone's arranged marriage. But they're very bad at spatial things and counting. And I wonder if that's because what common sense is, is things that seem so fundamental that they don't need to be said. And because it seems fundamental that words have letters, and letters can be counted and have a number of them. Maybe it doesn't come up in the data set, or because we're like, well, of course, if this is on the right, then the thing to the left of it would be on the left. We don't say it. And I wonder if it then doesn't pick it up through just purely autoregressive means. So maybe what common sense is like, anything that is sub explicit.
Speaker A: Sure. Yeah.
Speaker B: Because it goes without saying, it's not on the training.
Speaker H: That's a great point, because I was trying to think of that myself, like why this common sense is so common. It's just like an invisible layer which just exists. It's not explicit. So we don't know why we have this common sense.
Speaker A: What was its purpose?
Speaker H: And if you go to any other society, it actually has a different kind of common sense. It has its own purpose, and it's not like explicit. Even if they give you, like a text. Let's say you read a text and you watch the behavior. Sometimes text and behavior doesn't match.
Speaker A: So what do people think of are the gaps in that common sense right now, the things that are not said, and therefore outside of the current data sets? And what kind of information would you need? Like, what do we have access to? Clearly, we learn common sense in some way that's different here than somewhere else. What do we need?
Speaker I: Yeah, to go back to your example about children, I think a lot of common sense gets taught at a very young age, and I think because of that and children, not all children, most children don't read that well. There's not a lot of literature, literature other than just like, it's almost oral history being passed down, parents, teachers here explaining how the world works. And because that's not captured, it's not very explicit.
Speaker E: I've got a three and a six year old, and you have to teach them. She goes on the left foot, the right. She goes on the right foot, like over and over and over again.
Speaker A: So you think if we record parents for the next two, three years teaching their kids, and then we're like. And then we took all of these. People laugh, but that's what I hear, that this is a source of important information that is given to children. That's an important part of how we learn our common sense. It's not currently captured. Models don't have access to it. So if we captured it and gave it to models, would that solve a big part of the common sense gap? Maybe, yeah.
Speaker E: I think there's also an experiential component to it as well, because you do have to teach kids not to touch the stove. It's hot. But once they do, there's a memory that carries weight that they will feel a little bit of fear or intrepidation from trying it again. So I just wonder if it is maybe some things that maybe aren't even describable within language. How does an emotion feel? Like, what is pain, what is good even? Because to a machine, like, good and bad are just tokens.
Speaker D: And we program our own notions of good and bad into them, because for us, they're tokens for the most part. That's true too, like in a liberal sort of society.
Speaker A: So do you think those parts might get. Maybe they're ain't universal, but closer to universal in that I develop a common sense of what is it to be human. I know what it's like to be nauseous, and it's not nice to be nauseous. And you're a human, you probably are like me, and you don't want to be nauseous either. And there's a whole bunch of things that come from me just saying you without other information, I'm going to assume are kind of like me.
Speaker E: Very good point.
Speaker A: And I don't like to be hurt, and I like to be massaged and whatever. I was saying physical things right now, but you can go.
Speaker E: Sugar tastes good.
Speaker A: Yeah, sugar tastes good. There's so many of these that I know because I'm a human and I just apply to you because I just assume that.
Speaker G: Speaking of which, I am experiencing hunger.
Speaker B: And I'm going to order burritos.
Speaker G: Does anyone else want one?
Speaker A: I appreciate you walking to the room with me. Okay.
Speaker E: We had some burritos before you got here.
Speaker A: Okay, cool.
Speaker G: All right, I'll catch up.
Speaker E: We just left you out.
Speaker G: Oh, my God.
Speaker D: I haven't lived with my parents since I was 16, and I feel like I've experienced some of that. And you'd be surprised at how many things that other people would take for granted. Even after 16, I definitely.
Speaker A: Get wrong.
Speaker D: And then have to learn it. But at the same time, there are things about culture I grew up in that I'm like, that's so obvious.
Speaker A: Why does it.
Speaker D: Why do more people not understand this?
Speaker B: But we're running. I think culture in our brains is a kind of, like, running simulation of reality. And so we're predicting how other people are going to respond when I do this or that. And so we notice this when we travel to a very different culture and we commit a faux pa, and all of a sudden, everyone reacts negatively. And this is like a reinforcement signal being like, well, I definitely shouldn't do that again because people responded negatively.
Speaker A: There was a paper called Wacky Worlds out of Stanford, and this was from this group that does very much in the symbolic reasoning kind of thing. They do, like, bayesian inference models of language. And the wacky worlds was like, what does it take to when you say things that you don't just think, oh, I didn't understand that word. Right? But that there's this hierarchical thing. There's, like, the question under discussion or, like, the background context and how far off do you need to be saying things and not getting responses back that you expected? Where you start to question that higher level thing, you're like, talking to someone, you're like, oh, at some point, you're like, oh, they are very culturally different, or they're neurodivergent, or they're whatever. You start to not just say, we're not on the same page about what we're talking about, but there's something else going on here, and you're able to do that. And if you do it, if you're like, oh, let's say you're from a different culture, you might change your learning rate. You're suddenly more humble, right? You're like, oh, I should tread lighter and down rate my priors because I'm in a different context right now. We do all of that very effectively. Now, that's a kind of comments. How confident can I be right now engaging right now?
Speaker C: We do that with activism too. It's like we gather a group of people that we think that they think like us, that they don't like the current structure and they want to change it. And then we share our thoughts, and then we form an opinion that everyone assumes that is the common sense among that group, and then take it further to the rest of the people that they know to be like, okay, this is what we believe, and this is what we want to change. This is how we want to see the old system change to the new system.
Speaker A: Basically, these are different scales of like, I am a human, I have human things, I can assume human things. That's one scale we're part of group. Everyone here can say chat, gt, or whatever with more expectation that everyone, we all have used it because we're a select group. And so all of those things inform how we can interact with other people. So which ones can't be taught to AI? I think the children one is maybe the gap, that is maybe the easiest one, the explicit, where we can imagine how that could be solved. We record everything, like I'm recording right now, and we train llms in the future or whatever with that kind of data. But they're not human, right?
Speaker E: I was just thinking, because if it's parents who are human, teaching kids that are humans, that set of knowledge is exclusive. It's limited only to the set of facts that are relevant for humans in a social setting. In a behavior like a computer doesn't need to know what shoe to wear, but its human. Programmers might want it to know, right?
Speaker D: Because it might actually be put into a situation in which shoe knowledge is wrong for sure.
Speaker E: Yeah. If we're talking about on the search for AGI, then yeah, shoes would matter. But I wonder how different that amount of information is. Because with humans like shoes and our daily experience, and how to get dressed and eating food cures hunger, those things are critical, or we would die. But to machines, the things that are more critical are like, answer. When somebody asks you a question, you answer it, right? That's really critical. Common sense. But I wonder if there is a third set of information that is exclusive to machines that would be common, that they need to know. Like turning off means I go away or something like that. Might be common sense that a machine doesn't have that's unique to them. It's just interesting, different buckets.
Speaker F: Very interesting. Maybe like a question that comes to my mind is whether we should strive for machines to have human common sense rather than machine common sense to an extent.
Speaker D: One question you could ask is if there's a sort of knowledge that computers have of themselves, it's common sense. Then could we say that our heartbeating is common sense?
Speaker A: Because that's like a.
Speaker D: Maybe because the way that computers work, because it's all sort of matters of.
Speaker E: Information, it just suddenly is having heart palpitations, being scary. When you feel it, it feels scary. I have a regular heart rate. It's harmless. But the first time I had one, it was like, what is going on? That was to me, common sense. I had a knowledge of biology, but I didn't necessarily need to have that to know that was bad. Something wrong.
Speaker A: If common sense is like this, background knowledge that informs allows you to act correctly within the context. I don't know. That's pretty general way of saying things. That's kind of the most general, right? Priors unspecified allow you to act appropriately. You might think that machines, to interact with human society in many ways, should understand human common sense, but they're also like the machine culture. What do we need to do? We are not humans. We're not seen as human. Even if I'm anthropomorphizing a lot right now, but we are not seen as humans. That's not our role within society. That's not a set of actions we have to take. That's not we're called upon to do. It's not how we play. And I also can see the entire earth at once. Like, I'm a very different agent than a human. I contain multitudes. Truly, what would be the common sense of that?
Speaker C: I have no fucking idea.
Speaker G: I feel like implicit in that kind of description of common sense is you started to bring up agency, which actually hasn't entered the conversation before now, which I think there's maybe some different ways that we consider knowledge or understanding or how we might define any of these things in the context of agency. And also is where I think.
Speaker A: With.
Speaker G: Kids, there's a lot of kind of interesting things. Even if we weren't talking about AI at all, we're just talking about kids. Like trying to piece out what is inherent to a kid and what is learned and why are kids different because of how they are or because they had a different context. And I think that there's an assumption when you're teaching a human, whether that is like an adult human or a child human, that at some point they're going to be doing it for themselves where that they'll be using that knowledge. Whereas I think utility is like the primary, which, I mean, different cultures view kids differently, but with a technical system, it's constructed to largely express human desires. And I think that it performs tasks and answer questions and things like this, which are not the way that I interact with the human. Even if you're in, everyone's worked with mad bosses who expect you to perform tasks and answer questions, and it sucks. And so how this kind of, like, do we expect that? I guess maybe I wonder, do we think that in the next year or so or the next, yeah, in the next year to two years, do we think that we'll start seeing more agendic systems that have a little bit more free rein on the Internet type of thing? So how does that affect the common, I think that maybe the common sense thing becomes more pointed then where it's like, okay, if I'm just like calling and responding to order me a pizza.
Speaker E: But don't hack domino's.
Speaker G: But pay them money.
Speaker D: Or if Domino's is closed, let me know. And don't just wait till it opens.
Speaker G: Right.
Speaker A: So maybe common sense becomes incredibly important the larger our actions available to us, because it makes it even more important that you, the agent, constrain your set of possible actions that you're going to take in response to any query because they can't possibly constrain it for you through a set of statements. You need to do it and navigate. They're going to tell you a direction. There are a billion actions. There are infinite ways to go in that direction. They cut out in infinite other ways, but there's still an infinite way to do that, and you need to navigate that. And so that makes common sense incredibly important. As these things become more agentic, able to do more things in some scope.
Speaker D: This makes me think of the human emotion of frustration, which maybe that particular emotion that we use to measure once the agent we're interacting with lacks common sense. So when an AI would do something that is particularly frustrating to us, that's a common sense problem versus it makes us react in another way that might be a different kind of problem.
Speaker G: I think that's because we have trouble explaining it, right? And this happens, kids get frustrated because they don't have the words to say things. And I think that happens to us where we're like, I don't even know. To explain to you that pizzas are in stores.
Speaker H: Also, like adults get frustrated while explaining to children that frustration is also real when child is just asking a genuine question and they keep asking and asking and you get frustrated is the kind of the same phenomena when you're trying to explain the GPT.
Speaker A: That's a good intuition, the frustration. And maybe one thing, if we're very effective, we recognize our frustration.
Speaker B: Can we use the frustration? I keep thinking about how we would train this. Can we use frustration as part of the loss function to train the model to have more common sales? Right, if it orders 100 pizzas or it doesn't order pizzas because Domino's is closed, and then at 10:00 a.m. Tomorrow morning pizzas show up and you don't want them. We should be able to give negative reinforcement to the types of behaviors that we don't like and it will eventually learn to constrain its action space to a more human minded, a really interesting group.
Speaker G: Maybe you're working on this too, but Hume AI has built like this really interesting vector space around emotional responses from video. And they have an open API and they share a lot of their stuff. They're really cool, but they're talking about basically doing some replacement for RLHF that's directly from emotional responses. I just suggest people listen to the podcast. The guy read the stuff. It's really interesting how they look at it across different cultures and they're like, it's interesting how happiness is very similar across cultures, but awe is very divergent and things like that. So yes, I think at least someone else is working on that.
Speaker B: Yeah, I mean, the challenge would be because this would take so many real world human experiments that the AI might make so many mistakes and require so much input of human frustration that the mistakes it makes might actually have really negative real world consequences. So we should try to somehow simulate these things in some kind of.
Speaker A: Here'S an example of morbid's paradox that I have thought about more recently. So sometimes when common sense or another aspect like human values comes up, people are like, there's no way AI could understand human values, the complexity of it. And sometimes I think are human values really more complicated than like what's a cat and what's a dog? Maybe this is not an example of more of x paradox, where it's actually very difficult to understand what a cat or a dog is and separate, they're overlapping in a bunch of, well, it.
Speaker B: Was insanely hard to do that using classical computer vision. But then CNNs came along.
Speaker A: And what I'm trying to say is the CNNs were, I mean, one their inferential bias through their structure was commiserate with the data itself. Visual world is helpful by a convolution like that was helpful. And also, my point is just we solved that to some degree. We've solved a lot of these very challenging things that were difficult in Plato's time, like what is a man, a flightless chicken with two legs and trying to specify all these rules. And that's not how it works. It's a continuum of information that nonetheless has structure in a multidimensional space that we made progress on. And I would say that values and common sense and these other aspects are probably like that. It's hard for us to know, but it's probably going to be better than the pornography. I can't define it, but I know when I see it there's truth in that. There's not a set of rules that make that thing. So I bring this up to just say it's possible that the amount of information needed to make purchase on this space, that seems kind of hard for us to grasp. It's actually not that complicated a space compared to other ones that we've captured. And you don't need that much information. I think the RLHF people have been surprised at how little information you need to make incredible progress. And this idea of hume giving more human like ways to communicate valence rather than like, I like this, I don't like this. But rather a continuum of measurement is the kind of advance it sounds like that might unlock the scale of feedback, which might be much lower than we think. That will allow us to move much faster and be like, actually it knows all of the different ways we mean justice. It's crazy, but it does. That's possible.
Speaker G: Sorry, I feel like you got a troubleshoot earlier. Were what you trying to add to that earlier?
Speaker C: I was just saying that with the current llMs, my experience has been like, they don't really catch on your cues. As a human, you can be frustrated as much as you want with text, but if they're designed to give you a certain answer, it will not affect their way of thinking. Which is like, that's exactly the point that I'm trying to think about all the time, constantly, that how can we have a system that when you try to express even with text, even worse that this code that you gave, like how many times we use GP, I don't know any of your code. That it gives you partial code and you say the system give me the full code and it still gives you the partial code. Because it's designed to give you a specific outcome, even though as a human you're giving that prompt, it's not really listening to you.
Speaker G: Have you worked with base models at all?
Speaker C: Yes, the base model for the GP.
Speaker G: 3.5, the original da Vinci.
Speaker A: Any of the untuned open source ones?
Speaker C: Yeah. So the minstrel one is actually like a really good one. The ten terabytes, is that the one?
Speaker G: What's your experience working with those? Are they also like as resistant to. Because the instruction tune ones are designed to be resistant.
Speaker C: Yeah, they follow the attention is all you need model, which is all of them is just like that black box that the people who made that also don't know what's going on inside of. That's just the roadblock that you hit with all of these current based models that we all have, just that station for you.
Speaker E: I love this conversation about human values as well, because what bothers me about the ability to have models that work for us is where it's not as simple as there are different definitions of justice, but where people explicitly disagree about it. And so how do I know which type of human I'm speaking with is the kind of person who thinks that commuting all marijuana drug charges is justice? Or am I the person who thinks that every marijuana user should be in jail? That's justice, right? They can't coexist.
Speaker A: So how do you train San Francisco or not? I mean, I think these are the things. I'm kidding a little bit, but I think those are the things that we use all the time. Who is this person? And I make a whole bunch of inferences from the beginning, and then when we talk to them and they're like, oh yeah, I went hunting last weekend. You're like, oh, I'm going to update a little bit in some direction because there's just like all this information that informs how I interact with you, and hopefully that allows me to interact with you better, right?
Speaker E: Yeah, I feel that when you're training, so a lot of the arguments boil down to more data will solve the problem, but when more data just disagrees with itself, the model gets ruined. It's like completely unpredicted, unpredictive. So I don't know, maybe we need new architectures. I said I was going to be mad if people brought up causal learning.
Speaker G: Actually, I'm not sure if I agree with the statement that conflicting data ruins the models in the context of generative AI, because we're not actually trying to predict like we are using prediction, but when we're trained on the Internet, we're not trying to describe the Internet, nor are we trying to predict what else might be on the Internet. We deliberately ruin the distribution through fine tuning in order to get a behavior that doesn't reflect the data. So generative models, we're not trying to do that. But within that subset, I think you're bringing up an important point, which is that when you have conflicting data, you will get different answers at different times, and then you have to make a post facto decision about, okay, well, this base model is equally likely to say that is equally likely to produce like a rant from an Internet comment and the legal text that I want about hunting. So I have to make a decision as the designer to make it do one thing and not another, which one diffuses. I think a lot of the discussions around machine learning models where it's like, well, the data, the data said that, where it's like, now we know that's not true, but with the things we're talking about, like marijuana, these are situated in human systems that are designed to adjudicate subjectivity. So we have legal systems that are then wrapped in democratic systems, at least in the US, that are designed for what happens when we disagree. And so I'm curious about this right now. We have this very kind of authoritarian, monarchical model of the people making the models, then decide as the designers, and they might solicit feedback from the crowd. They might be democratic in the sense that the democracy that the Communist Party says that they espouse, that they listen to people and integrate it, but ultimately it's their decision. I wonder if there are human processes around AI models that could then collectively fine tune or collectively adjudicate the outputs, as opposed to this kind of very centralized model that we have now.
Speaker B: How would you propose doing that?
Speaker A: I want to bring in one other aspect as we move into this direction, I'd like to bring in. So, Laura, at the very beginning, you brought up another aspect of common sense which we haven't touched on, which I think we'll connect here, which is, how can we help people have common sense about these AI systems? Because one of the things I think you're asking for here is like, what is a better form or a more democratized form, some form of humans giving real direction to these models that aren't just adjudicated by some hegemon. And that probably also relates to humans understanding these systems in this way, that when you're frustrated with someone over time, I'm sure you get frustrated with your kids, but you also learn how to develop intuition for your children, and so you can help move yourself beyond frustration rather than just yelling at. So, like, maybe we can talk a little bit about that aspect. Human common sense for AI systems, which maybe between both of those we can get to where Liz introduced us at the beginning, which was the idea of how can we have a pragmatic AI system that works, is working within the context of our system, which probably requires the AI to have common sense in us to have some understanding of the systems world.
Speaker D: This question makes me think of the emergence of media literacy as a sort of field of knowledge, actually, and not just common sense. And I think that emerged when we realized that you couldn't rely on common sense alone to deal with these systems because it would go wrong. Give us an example, like being able to spot disinformation, for example, or taking things for granted.
Speaker B: Disinformation works because it fools a certain.
Speaker D: Percentage of the population, right? So maybe it's not actually common sense that people need to have, it's something more. We can't just rely on common sense alone.
Speaker H: Also, in societies where people, most of the kind of demographic, is not educated enough to distinct from information and knowledge, propaganda is like, on our level.
Speaker A: Speaking.
Speaker H: Of India right now, because everyone has easy access to phones and Internet, you will see propaganda like you can't even imagine. And with AI, just tenfolds, because when with local elites and kind of mostly like kind of premodern demographic, you can put like, inject all kind of a propaganda. It's a completely different outcome.
Speaker A: Does anyone think that we can develop as a society, that there's like, what I loved about seeing Chat GPT, and what allows my parents and whatever can direct with it is that they don't have to develop a special kind of common sense, really for it. They interact with it kind of like a human, and it kind of acts like a human. And so the common sense is mostly on the AI side, what we were talking about before. But I do hear people talk about, like, we need a better literacy. We need an AI literacy. We need something. And so what do people expect from that? What do people want?
Speaker C: I think the first thing we need to develop for that is for the AI models to be able to receive the feedback from humans, because I feel like we're still not allowing every single interaction, that every single person who's using those systems have to be recorded. And the system learned from that because they have the capacity, they have the GPU power that is limited, that they need to limit the amount of feedback and intake of information that they can have. So that's the first step. And I think the second step would be how does many people who use these systems to treat these systems like they would talk to a human being rather than they talking to a robot. So if they would give their frustration, if they give the details of their thoughts in a way that they would talk to a human being, those systems would also start developing a sense of like, these are what commonly known as common sense from.
Speaker A: So you're still talking about things that humans can do that could help the AI become more commonsensical.
Speaker C: I think for a long time, we are going to be the one who feeds those llms, we are going to be the one who guides them to the point that they can be helpful for us. And at some point they might start having their own autonomy and have their own way of creating things. But I don't see it right now being the case because they're still like the babies, they're still learning from us, we're still being the one who are feeding all of our books to them and all of the text that we have, the conversation with them will give them the context at the stage that we are right now. I think that's probably the case.
Speaker A: Or were you consistent?
Speaker H: No, I think the implicit world has to become explicit now because I think we have to become adults and we have to kind of translate our value systems to AI. Why we do this, because it's just assumed. Most of what we think is right is just assumed because everyone agrees, which is fine, but when you're telling AI, you can dictate it, but it would not understand in the depth of why we are doing the right thing.
Speaker D: Why should you?
Speaker H: I don't know where I first came to United States. I don't know, like 15 years ago or something.
Speaker A: First, I don't know.
Speaker H: You're supposed to put the card in the shopping, like wherever you're going to put it back. That was the assumption. It's a good assumption to have, but I did not have that assumption.
Speaker A: I remember listening to a radio lab, like I think five years ago about Facebook's content moderation team and the need for it really gave me an appreciation for the impossibility of this, like the need to systematize the onset, especially when you don't have a structure that is like I'm proposing, might be able to have an AI system that can understand the multiplicity of it, but rather has to do the flightless, hairless bird, whatever with that perspective on morality is just insane, but is needed when you make a platform where you need to productize morality in some way.
Speaker H: Also, most of the word is implicit. It's not explicit. You can't read all the books about a culture and go there and fit in because you won't understand.
Speaker A: Yeah. To watch the tv shows too.
Speaker H: You can watch it, but you can understand. But to survive in that culture, in any culture outside of most of the western world, it's very implicit.
Speaker G: It's not like textual you brought up earlier. Sorry, go ahead.
Speaker E: Oh, I have a proto thought. Bear with me. There's an aspect of common sense that I'm interested in. That is where we are shifting between worldviews or even styles of morality. So we all know, like, deontology versus what's it. What's the other one? Utilitarianism. Yeah.
Speaker A: Can we define these two for people?
Speaker E: Deontology says basically a thing is right or wrong just because it is, and that's its intrinsic property. It just is right or it is wrong. Like, killing just by is just naturally bad. And consequentialism says killing can actually be okay if you're killing Hitler, because fewer people would die if you killed Hitler as compared to. So it's the classic trolley problem. Which choice would you make? But we as humans navigate between those two, and I think there's a common sense decision when each one is most appropriate. So if we actually were in a situation where a trolley is bearing down on one person versus 20 people, and we had to make that choice, we would probably go for the one person as opposed to the 20 person. But if we were presented with the choice of. I'm trying to think of a good example. And this is where my thought kind of gives up.
Speaker A: Consequentialism is right all the time.
Speaker E: Consequential, no. But presented with, should I lie?
Speaker A: Honor. Should I lie?
Speaker E: Honor is what tripped me up and convinced me not to be a utilitarian. Because maybe it is true that if I lied to you, then it would be better, you would be happier, or maybe even save your life, or whatever the positive consequence is. But also, I would be breaking a promise. And maybe there's a system in my head of, like, maybe I've made my New Year's resolution never to do that again, or maybe I'm just a chronic liar and it's an addiction for me, and so I really want to, but in this setting, it's not quite right.
Speaker G: It's probably about burrito.
Speaker E: Oh, okay. I was like.
Speaker A: I'm sorry, you can't come into yet. I just want to deliver burrito. Okay, you can come.
Speaker E: But I wonder, because you can't just have one moral philosophy that encompasses every choice that every human is going to ever have to make. And that's why people argue about this so much. For every one example where it's extremely clear, there are a dozen other examples where it's very unclear and people debate about it. That's why we still talk about the trolley problem. So we are constantly holding these two different and opposing moral philosophies in our head and applying the right one when the vibes are right. So I'm just like, how do we.
Speaker G: Even translate that into, see, a multiplicity.
Speaker B: Of AI models in the future, some of which are like, oh, this is the confucian model, and that is the halal model. They have different moral reasoning systems, and they behave differently as a result.
Speaker E: I think that's where we're headed for sure. But if it were like, an ensemble of all of those different philosophies, the context switcher, which may not even be AI, maybe it is a different model that's deciding when to apply each one. But how do you address that? How do you create that?
Speaker A: I'm going to continue to use this. Computer vision is the same as values analogy for a second, which is to say, like, before we had CNNs, and I'm acting like CNNs are the end of computer vision. We had a number of different models, right? This one is an edge detector. This one does these colors. And I'm sure the best models on computer vision tests before were ensembles that somehow aggregated these and did their kind of thing. And we have an ensemble of different moral systems. Some people pursue one or the other. Some people like a parliamentary approach. I've heard this of a worldview diversification. In the EA space. There are many different intuitions. They're tools in our moral belt. We can use them. But these are maybe similar to being an ensemble of these shitty computer vision models, which is, they are not grasping. They're not able to represent the underlying space well enough to actually do the thing. And that's just a limitation of us and the ways that we've projected our systems into some rules that we can understand and use and talk about and whatever. But the space of all of those intuitions don't have to be diametrically opposed. They're in a space that can be understood through revealed preference or whatever, like behaviors we take in many different situations. I don't know. That's a hope. That I have, that you might not need this context switching so much because they're just all reflections of some underlying thing that we're projecting out.
Speaker D: I personally don't.
Speaker A: Sorry. Allie, please. Here.
Speaker I: Are you finished?
Speaker A: Yes, I'm done.
Speaker I: I'd like to respond to what you just asked. I read somewhere that when kids are little and they're multilingual, they do something that researchers observe as code switching. So if they have, like a parent, two parents that speak different languages, they actually address very differently, because, like, what you were saying about Falta and different cultures, it's not just the language that's different. The behavior and also intonation and cultural context is also different. I think you're kind of going in that direction a little bit. Part of mention the right words where you need the single United nations to kind of decide when you're doing what type of code switching, because we do it all the time. I'm behaving differently here than I would in different contexts. Right. So I don't know where that mechanism come in, but a child can do it.
Speaker E: Awesome. It's very interesting. Basically, up until the generative AI moment, the AI that was in production systems, especially in high risk categories, this is kind of illegal a little bit in some weird shades of gray, but they were actually training different models. You know this, Ian, like the tabular data days, they were training different credit decisioning models, for instance, for people of color versus white people. And it wasn't segregation, genuinely. The white people models did not predict well on the people of color, and they found that these really narrow models were a lot more appropriate. So, I mean, I almost prefer a world like that, because if you go for the mean, the bulk of the bell curve, then all minorities, all marginalized groups are just going to see poor performance forever. And I think we're still experiencing some of that with generative AI even now. But feel this sort of shifted mentality of like one model to rule them all. It's going to fix everything.
Speaker I: Were you the one that asked why it's not common knowledge?
Speaker A: Common sense. Not common knowledge.
Speaker I: The chinese word for common sense is actually majority knowledge.
Speaker G: Interesting.
Speaker A: So it's just a cultural bias that we call it. Absolutely.
Speaker D: I think that's an important piece, too. And then what does majority mean? Common and majority are different words, too.
Speaker C: That kind of comes back to what.
Speaker H: You'Re saying about it becoming explicit when it becomes problematized. Think about this when you're talking about.
Speaker G: The shopping cart, like when I'm in airports and it has the sign above the toilet. That's like poop in the bottom part, not the top part. We see that you're like, who are these people that think you should poop in the top part? But clearly that's not as obvious as you think if people are not accustomed to seeing that kind of toilet. So the challenge of making it explicit instead of implicit in all these cases is a big one. However, it's interesting. You were talking about human values earlier. The groups that have a big advantage on this are faith groups because they are actually accustomed to, in an explicit way, describing what they believe and then enumerating that to as many possible cases in life as possible. And they have huge bodies. It's not all faith practices exist in text, but many of them do where it's like, well, yeah, we have this book from several thousand years ago. That's absolute truth. How does that apply to shopping carts? How does that apply to death? How does that apply to doing my hair? And they do have all this. So the ease of translating that to human values are very difficult to enumerate explicitly, but people do. The ones that have the most challenge is this very weird acronym, weird group that we are here, where we are secular and for some reason are not, for some reason we are not imposed upon to explicitly explain our ethics or beliefs on a day to day basis. And in companies we have like ethics.
Speaker A: But it really means corporate risk.
Speaker G: It's basically a list of no's that are like that. The lawyers kind of feel like won't upset the most people.
Speaker A: I just want to make sure people are on the same weird. I was going to ask educated, individualistic, what's the are rich developed?
Speaker G: Which I also think my point.
Speaker H: I.
Speaker G: Think I don't go too far on that because, no, I want, even if we use the normal word weird there, but that the biggest challenge is for. There's this weird thing where the people who are creating the models are this very small minority of the world who are not religious and whose ethics are not explicit and are kind of like a vague, pseudoprotistan, secular, scientific, materialist something. But then it's like, why, oh, how could an AI ever learn human values? And the answer is probably much easier for Mormons.
Speaker A: Yeah, you think Mormons would be like, we've spent 200 years explicating this. So an AI system would read this book and those weren't our revealed preferences. Those are our thoughtful preferences. Yeah.
Speaker G: And the church, a lot of day sense, is paying accenture a lot of money to figure this out for them right now. How do you make models?
Speaker B: They're paying accenture to build like a Mormon.
Speaker G: GPT I don't think it's gotten quite to that level of implementation. But all of these large, well funded faith groups, I'm sure like Chabad is on it and so is like the church of latter day Saints and so is the Catholic. We're just on a call with the head of AI for the catholic church. This is actually easier for faith.
Speaker C: God.
Speaker A: Yeah, you don't have to learn it. You just do the rag over. You just look up the thing and you're like, you don't do that.
Speaker B: The Vatican has a satellite, apparently now, like in space. I don't know what it's for, but.
Speaker A: God already gets enough. Ali, can I just ask, you brought up this interesting thing, which was clearly kind of inspiring, but are there in the range of. I'm sure there are other chinese words that reflect different aspects of what you've been talking about here. Besides majority knowledge, are there other senses of how could you have done that? Are there other terms that kind of tile the space that we don't have access to?
Speaker I: Can I share some thoughts? And then you get back to others.
Speaker A: Yeah, of course.
Speaker I: You immediately ask, what is the majority? That's not a question that gets asked because it's not an individualistic society. The majority is a great implicit that everyone understands what it is. The thought of questioning that is in of itself questionable. Why I wanted to address your point about Falcon and also like kids and capturing parental teachings and knowledge. I think that there is almost like evolutionary advantage to cultures that are highly implicit in context, that the persons who understand the implicit rules actually have like a social advantage over other people in immediate environments, wherever that's applied.
Speaker E: Absolutely.
Speaker I: Why would they capture that and write that down and then into a model so that others can be brought up to speed on how exactly to be the best, so on and so forth.
Speaker A: Right.
Speaker I: It's like if you know it, then you have the better sense of how to navigate your environment, crush your enemies.
Speaker G: There's a disincentive against sharing that, I.
Speaker H: Would say so most of the world is that it's implicit. They don't want you to share as much as you want to learn. They don't want you to know.
Speaker A: There are other people who host the workshops. They're like, I will show you the secret. Because they probably didn't. They're not the most successful of those people, but they're relatively. So they end up selling the pitchfork. No.
Speaker H: So what happens? I agree with that. A lot of people from the west, and it's not recent, like go to the, I don't know, eastern parts of the world, and they learn the practices, whatever. It's a very stimulated environment where they learn this kind of specific practice and they go to the real society. They realize none of those practices transform in any way how to navigate in the real world outside of those monasteries. Because the monasteries are created for them. They are not created for the local people there, because the implicit world which everyone is living, monasteries don't have that, but a foreigner might come there. They will assume that monastery teachings will translate in that kind of local world. Never true.
Speaker D: I think this points to the fact that for Judeo Christian or people in an abrahamic tradition, including secular people in the liberal west, for us, an ethical system is something that is actually coherent or cohesive entity, but that is very specific to a certain set of traditions.
Speaker H: Very new and recent phenomenon.
Speaker A: I'm going to bring in a good way. Yeah. A slightly different perspective, like a group that has done something, which is, here's a folk theory. I'm not going to put too much into it, but in the Bay area at least, there is a large king community. And the king community often seems to a kink. A kink like, not like vanilla sex. They pursue other kinds of. And these people have developed and forwarded a lot of knowledge around consent. And there also seems to be a bit of an overlap with people who are like a little socially awkward in some ways. And here's the folk theory that both the higher risk of the kinds of things that they want to engage in, and potentially the difficulty in engaging in normal social interactions, required or promoted the desire to create a structure, more explicit structure, around these engagements. And if we're going to cut each other, an extreme example, that consent conversation better be pretty explicit, and that discovery was pushed because of the constraints of that subculture, but the cultures outside of it actually benefit too. Right? Consent is actually a useful structure that other people didn't invent because there wasn't as much pressure for it. This is just one part history. I'm not a scholar of this area, but I like the idea of within the constraints lead to these social technologies being discovered, but that doesn't mean they're only useful for that society. They might be useful for everyone. We just didn't invent them. And this is one example.
Speaker B: So because of all of the conversations that are ongoing around consent, that could then inform, help an AI to be now quite knowledgeable, or at least be able to communicate human ideas around sexuality and consent.
Speaker A: Probably better today than like 50 years ago, right?
Speaker B: Because there's this corpus now of people who have been talking about these for a number of years now we can educate ais on these things that are now being entered into common sense culture. I don't know.
Speaker E: This is not exactly a counterargument, but it is a provocation, I suppose, which is people have studied persuasion a lot, and in particular, how do you behave as a waitress to get a better tip? And a couple of things consistently come up. Kneeling down by the table usually gets you a better tip. And it's kind of universal. Touching the customer, however, is either a positive or a very dire negative. And the difference that they've measured so far is whether or not the waitress is attractive and whether or not the subject is of these opposite sex. The attempt to define something which should be clear, like how do you get a better tip as a waitress? Is confounded by these other factors that we maybe, first the study exists it. And they were like, this is just not predictive. We have no idea what it is. And then they figured out that it has to do with the attractiveness. And then attractiveness is such a subjective thing that if we were to try to ask a model, how do you go get the best tip you can out of this person, assuming they were humanized or embodied somehow, how would we even program that in?
Speaker A: I feel like all of these different factors kind of going together are hard for us to keep in mind.
Speaker E: We feel it out.
Speaker A: I know early in the conversation you brought up, what about these different attitudes towards justice? Or like, use the marijuana example. I wonder if in even the corpus of text, if there are these explicit conflicts, or if you, with slightly more context. I'm going to use a cartoon example. You're like, I'm from San Francisco. I believe marijuana x. It's like, I'm from the Vatican. I believe that marijuana x, this conflict actually is completely, what seems like conflict is completely separated with enough context. And so right now, with an AI system, if you say, what should marijuana be? It has no knowledge of, like, I'm from San Diego, I'm from San Francisco, I'm from the Vatican. It doesn't have any of that. And so there is conflict. What should I do? I don't have enough knowledge, but it could have that knowledge. It could have more background. And the same, like here, with your point about consent, there's a lot of features for us to keep in mind, but clearly we do on some intuitive level. That's what common sense is.
Speaker E: But isn't that kind of depressing? Is the goal of this? And just for the very clear record, I don't think we're going to get to AgI in our lifetimes. But is the goal of this to have custom models that reflect back whatever you think they should say, or whatever the person wants you to say? Or are we aiming towards autonomy, intelligence, embodiment? Or are those things contradictory? I'm not entirely sure.
Speaker G: I think maybe. What kind of technology are these? Is an interesting question. Are AI, these generative models, communication technologies that I'm using to communicate as myself to another person, or are they mechanical technologies that I'm using to act on the world? I think it could be any one of those things depending on what we want them to do. I think that we're circling a lot around this subjectivity thing. And so I think that's why, for me, how we situate these things in groups of people that are collectively guiding them is ultimately the point. I think at this point it seems pretty clear that we can get them to do quite a number of things should we want them to. And so how that desire is expressed, I think, is kind of the interesting point. I don't have a personally have a clear definition of AGI, but.
Speaker A: Even if.
Speaker G: We just call what GPT four is AGI, it certainly is better than me at a lot of things. How do we situate that socially? And so someone asked earlier, how could you do that? I don't think it has to be a dow or something that exists at the level of a society. I think it could be like with low ranked fine tuning, you only need a few hundred examples of something to get pretty impressive results out of a model. Like you and your friends could spend an afternoon just literally writing by hand a few hundred things that you want the model to, and then get your friend who can understand a colab notebook to make it so that you have it. I don't want to trivialize the barriers to access that exist in that statement, but I think that for me the question comes back to if there's going to be a lot of these small models, if we want there to be society level models that do something. I think that's where it becomes complicated in terms of, do we need different forms of adjudication, do we need different forms of subjectivity limiting mechanisms that are separate from what we have today, separate from democratic systems or legal systems, or if you're in a different country, monarchical systems?
Speaker A: So just as an example, I think we're going to have more empirical evidence in this soon. In that right now, foundation models, at least in the US, there are a few large companies the EU is pushing in. They're attempting to push in a different direction where they have not national, whatever, EU level supercomputing cluster, they're trying to make force that data is interoperable in some way. And then they have their regulatory regime and their goal, their innovation goal, not just risk reduction goal, is a vibrant space of foundation startups like Mistral in France, that are both reflective, powerful and also reflective of the multiplicity of cultures that is the European Union. And kind of recognizing that, and also recognizing that you need capital and compute, and these other things are barrier to entry. And so if you want that multiplicity, you need to set up some infrastructure for that. And so this isn't a prediction or anything, I'm just saying we'll see how well that works. How well do you need separate foundation models? Can you have one foundation model that's fine tuned into different ways? How much does that distort? How much can you just turn the knob so that it reflects different cultures? How much do we want that to happen? Do we want the model to. If a flat earther comes and it's like, is the world flat? It's like, if you want it to be, maybe you want it to say that, but do you want it to know that that's all a different thing too? Maybe you want the model to know if you could read, you do, the mechanistic interpretability. It knows the world is round. And also, I'm going to play along with this guy right now. Maybe you want that.
Speaker E: Do we want them to lie?
Speaker A: Yeah. Do you want it to be like, we want you to deceive the person if they want that, but it's true.
Speaker C: Who knows?
Speaker A: Who knows? But we'll have more information.
Speaker E: Interesting.
Speaker A: So it is currently 335, and we actually have a bit of time left. But the way we like to close is to just have people go around. We don't literally have to go around, but like popcorn around and share a takeaway. A question that has been prompted either by this conversation before that hasn't been brought up yet, an idea like whatever you want, your final thing. And we'll have a little bit of time to follow up on each person, but let's try to not have this fall into conversation topics and instead be focused on closing out a little bit. So if anyone would like to share a closing thought, please.
Speaker H: I think my kind of takeaway is like listening to most of you people is that human judgment and taste is not going away sooner. I think it's going to stay for a while. It might take a while for it to go away. But I think, what are we kind of like running into this wall of this human judgment and taste? And I think it's going to be.
Speaker A: Here for a while with us as.
Speaker H: The AI progresses, and I hope it will, then I think humans will start giving away to AI. But I think it's going to take a long time.
Speaker F: On my side, one thing that it was very interesting for me to start thinking is this kind of dichotomy between human common sense and machine common sense. And I do feel like I need to think much more about it, because indeed, we might want machines to have a different type of sense or knowledge than us, and not necessarily converge towards our knowledge, because we are two very different beings with two very different objectives on this earth as well.
Speaker A: My side I would go next.
Speaker C: I think it looks like, based on the conversation, we want to move towards the world that we have more models that are more towards representative of every human being, rather than fine tuning big models like GPT that we have, for example. That's the take. It's more towards little. We go towards a world that each group of people at least have their own representative of their own llm system, rather than try to have one big LLM system and create the branches of that for the two people.
Speaker A: There was a conversation we had a while ago about digital twins, which in some sense is like, it's not the most extreme, but it's an aspect of like, each of us have our representative, our model, which we don't have to, through distribution of resources, need to have myself represented by the american model or San Francisco. But my model, your model and my agency doesn't have to be limited by being an individual.
Speaker C: That would be beautiful to see, honestly.
Speaker D: Fucking interesting for me, I guess my hypothesis about common sense coming out of this is that as we move into a world with more different kinds of potentially intelligent or agential beings, that the scope of what common sense includes is expanding because there are more sort of kinds of knowledge or behavior that becomes problematized because we're working with these beings that just don't navigate the world in.
Speaker A: The way that we do.
Speaker E: That's really fascinating. Let's write a paper about that.
Speaker A: Let's do it.
Speaker E: Okay, cool. Honestly, your comments about how to teach common sense machines by exposing them to kids training material was super fascinating to me. Very interesting. It's something I want to pursue a little bit and see if that would help. I feel like I'm a little bit more confident that it is a tractable task following this conversation. I still think it's going to take decades, but I'm pleasantly surprised and optimistic.
Speaker G: I think mine was similar to some points you brought up. Just like this difference between implicit and explicit and what's said and what's not said, and then just how much more important common sense becomes once we introduce action as opposed to communication. And I love the example you brought up of that. Felt very real of like, oh, if I ask you to order a pizza and it's closed, don't just wait because you don't care about waiting. But I do. So tell me. I just really appreciated that. But yeah, that agenda, gentle being in.
Speaker D: The world thing is, I think, important.
Speaker G: And something that was brought up.
Speaker A: Something that I've been thinking about actually just now, is how in some ways good a test bed, self driving cars are for a lot of the things we're talking about here. Like you are acting in the world, you're acting with other things. It's a context and it's an impactful context, and there's a lot that's unsaid and then there are some things that are said and you have to navigate that. And I was just. For those of you who have experienced cruise versus waymo, even before cruise and this whole thing, the experience in cruise has seemed to me to somehow be like you were more like I'm in a robot taxi. And when you're in a way mode, it's like, I can't believe there's not someone driving it. It feels human. And whatever that means in the way that it goes about things, they've done a good job in different aspects. Anyway, I wonder how much we're going to get from that that is generalizable. How much is that specific to this problem space versus them saying like, hey, we want agents in general to be interacting with other humans. Mohammed, who works on robotics, left. But what is the experience of when you're working with this next generation of at home helpers or whatever? Is this going to be where the rubber meets on that? Yeah.
Speaker B: If I were to give my sort of summary of the conversation today, I have the intuition that it is a tractable problem, hard, attractable, and I think after talking about it today, I feel more confident that it's possible, but it might take longer. I don't feel like decades, but I do feel like it will be tricky. In the same way that a lot of engineers thought autonomous vehicles are right around the corner.
Speaker E: Five years.
Speaker B: Five years will be easy. And then you run into this long tail of edge cases, edge cases upon edge cases. And how do you deal with every single one of these? That, for a human driver, go without saying. But to a machine driver, a robotic.
Speaker C: Car.
Speaker B: Doesn'T go without saying. And so the example of explicitly telling a child, like the left shoe goes on the left foot, or a religious system that is obsessively documenting how its doctrines apply in every circumstance, these are maybe helpful analogies to how we might be able to transmit our intuitions about common sense into a model. And even if we succeed at that, I believe there's going to be a multiplicity of models that will emerge from different groups as they all attempt this problem. And maybe we'll end up with ensembles later, or maybe there will be a competition between them to see who is the most effective in some way. It'll be interesting to see how this evolves.
Speaker A: I wonder what the release valve is. I don't know how the Talmud has been created over time, but I imagine at some point, people aren't just able to generalize out of jewish law to a new circumstance. They have to sort of like, this is a new thing. Can I go into an autonomous vehicle on the Sabbath? You go somewhere, and then this more effortful engagement happens, and then it becomes part of the law. Whatever, right? There are things that you're like, this is out of distribution in such a way that I can't confidently generalize any of my previous experiences to it, and I can recognize when that happens and then spin up this other process, and that seems pretty important. You're going to have these edge cases, and rather than be like, I have to cover them all. You need a graceful degradation. You need a graceful way to deal with those out of sample shit and be like, oh, for a lot of AI systems, I think we should, for a while, be like, bring in the human. That's, like, a very obvious one, right? I don't know what the fuck is going on. Bring in the human. We can expand out how much it can do.
Speaker E: You couldn't open the door, by the way, on the autonomous car because it turns the light on. On Sabbath, you can go in an elevator that's already running, so you can't press the button.
Speaker A: I assumed that getting into anatomical would definitely be not okay.
Speaker E: It would be fine if somebody else opens the door for you or if.
Speaker D: There were, like, a Sabbath mode, like elevator.
Speaker E: Exactly. That's what I was thinking.
Speaker D: Of they stop at every course.
Speaker A: You don't have to pick the button, but it's more about the process rather.
Speaker D: Than the end result. And that's what keeping in line with the law is in that tradition.
Speaker G: Sorry if we are going back into conversation instead of wrap ups. We did some thinking around this for magasha at AI. And it's like that epistemology is a process, not like a set of facts, and that there is like a process of analyzing Talmud that people specialize in, which also is not specifically characteristic. So that with med palm too, the base palm model was not better than a coin flip at outperforming doctors at diagnosis. But then after consulting with expert physicians, not in what was medically correct, but in the process of diagnosis, fine tuned it on a specialized chain of thought that was in that process of diagnosis, and it became better, was rated by physicians as better than physicians on eight out of ten categories.
Speaker A: Was this a Microsoft paper?
Speaker G: This was Google. Yeah, med palm, too.
Speaker E: Dr. Eilish. Adeline Eilish.
Speaker G: I don't know the.
Speaker E: It's anthropology, basically what we're talking about.
Speaker G: Yeah, but automated anthropology, automated epistemology. I was also thinking, regrettably, I have to go.
Speaker A: No problem.
Speaker F: Lovely meeting all of you.
Speaker A: Let me just say right now, the ways to engage in the future. First of all, there's a calendar of events, so if you see another event in the future, please sign up. As you saw, we actually get like many more sign ups than we have availability. So if you see one that you really like to go to, feel free to reach out to me. If I didn't accept you, and maybe I take that into account. We have a slack channel. I'll send out a follow up at some point from this. Hopefully now we'll just have stuff on the substac rather than me sending summaries. So you can see that from this. And that's pretty much it.
Speaker B: Yeah.
Speaker G: Thank you.
Speaker A: Thanks. Take care. That's cool. This also is related to putting some of your compute not at the training time, but at inference time. We don't do all of our thinking in our genes, we do it at inference time. And so, like, better prompt engineering strategies or like, essentially, which it sounds like this was a combination of that and also using those chain of thoughts as the fine tuning thing to bake it in.
Speaker G: Early on, I brought up the med palm example, not because we actually did fine tune midrash on a specialized chain of thought. That's from Tombuddick's study, but just to make the point that even in a situation where there is a right answer, using a specialized epistemological project process and fine tuning on that does get you very efficacious results. Although I think prompt engineering, usually when I'm working with clients, the initial conversation is talking them down from the ledge of training their own model and fine tuning before they exit prompt engineering. So you can get surprisingly far with it.
Speaker A: Yeah, totally. And it's like, actually people, their chain of thought isn't just let's think step by step. It's amazing how well that works. You can structure it even better and get better results.
Speaker G: Also, earlier he was talking about self play, and you were talking about the thing in the waymo thing. And there's kind of something I forgot to mention earlier was like these interesting limits to self play when we're talking about human interaction. This came up in the Cicero paper, which was, there's this game called diplomacy, which has partly strategic moves and partly people negotiating with each other. And when they trained it only on self play, where the model is playing itself, it just got really weird and wouldn't talk normally. So there's this element of how humans behave is not rational, but we expect other humans to act like that, even if a car is correctly moving. But it's like skittering around, we're going to freak out and all the humans will mess up. So that kind of extra layer outside of correctness of what socially that common sense comes back in.
Speaker D: Last week, I talked to an anthropologist who worked at Waymo, and her work specifically has Kia. Her main project was studying parking lots and the social norms of parking lots and the signaling communication and the habits, and then working with the engineers to try to operationalize that. But operative word was working there because she was laid off during their large round of layoffs under the idea that they had collected enough data about the parking lots to have a machine learning model be the anthropologist.
Speaker E: Of course.
Speaker A: I was like, to get into my garage, we have to turn and then take up the entire two way street for a moment. So normally we try to do that when there aren't people around. But if there's a gap, I sometimes say to my partner, who's a little more skittish about this, I'm like, they will stop. They will stop for us. It's okay. And I remember we were doing this and I saw a Waymo car coming in, and I, surprising, completely trusted it. But there was a moment when I was like, all of the things that I am applying, all what I'm generalizing here is to a different kind of agent.
Speaker G: Yeah.
Speaker A: Now, really, all I'm trusting is that who kind of burning man trust? Someone put a Ferris wheel here, and they probably didn't want to kill people. So it's probably. But it's, like, different kind of trust than the repeated knowledge or it's a misapplication. It's an anthropomorphization of the agent there that I'm like, should act like a person. And it did smash into me, and I'm lucky to know it stopped. It totally stopped. But anyway, it was just like, that's a weird. That's a slightly weird circumstance that it needs to be able to deal with. Okay, well, cool. Everyone, thank you so much.
Speaker E: Thanks for coming, everybody.
Speaker F: Thank you.
Speaker A: That was very fun.
Speaker I: Yeah.
Speaker A: Very fun conversation.